[
  {
    "path": "posts/2022-08-10/",
    "title": "Create DF",
    "description": {},
    "author": [],
    "date": "2022-08-10",
    "categories": [],
    "contents": "\nA few exercises in creating data frames.\n1\n\n\ndf <- data.frame(\n  'a' = c(seq(1:5)),\n  'b' = c(seq(11:15))\n)\n\n\n\n2\n\n\ncol1 <- c(seq(1:5))\ncol2 <- c(seq(11:15))\ndf2 <- data.frame(\n  'a' = c(col1),\n  'b' = c(col2)\n)\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:06:10-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-14/",
    "title": "Save SPSS Labels",
    "description": {},
    "author": [],
    "date": "2022-07-14",
    "categories": [],
    "contents": "\n\n\nlibrary(foreign)\ndf <- read.spss('data.sav', to.data.frame=T)\ndflabels <- as.data.frame(attr(df, \"variable.labels\"))\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-08/",
    "title": "Merging Row Issue",
    "description": {},
    "author": [],
    "date": "2022-06-10",
    "categories": [],
    "contents": "\nRepeated values cause unexpected data lengthening when merging.\n\n\ndf1 <- data.frame(col1 = LETTERS[1:4],\n                  col2 = 1:4)\n\ndf1 %>% kable() %>% kable_styling()\n\n\n\ncol1\n\n\ncol2\n\n\nA\n\n\n1\n\n\nB\n\n\n2\n\n\nC\n\n\n3\n\n\nD\n\n\n4\n\n\n\n\ndf2 <- data.frame(col1 = rep(LETTERS[1:2], 2),\n                  col3 = 4:1)\n\ndf2 %>% kable() %>% kable_styling()\n\n\n\ncol1\n\n\ncol3\n\n\nA\n\n\n4\n\n\nB\n\n\n3\n\n\nA\n\n\n2\n\n\nB\n\n\n1\n\n\nI might expect four rows after merging, but instead I get six.\n\n\ndf <- left_join(df1, df2) \n\ndf %>% kable() %>% kable_styling()\n\n\n\ncol1\n\n\ncol2\n\n\ncol3\n\n\nA\n\n\n1\n\n\n4\n\n\nA\n\n\n1\n\n\n2\n\n\nB\n\n\n2\n\n\n3\n\n\nB\n\n\n2\n\n\n1\n\n\nC\n\n\n3\n\n\nNA\n\n\nD\n\n\n4\n\n\nNA\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-16-ols-via-computation/",
    "title": "OLS via Computation",
    "description": {},
    "author": [],
    "date": "2021-09-16",
    "categories": [],
    "contents": "\n\n\nintercept <- 1\nb <- 0.7\nn <- 5\nx <- rnorm(n, 5, 2)\ny <- intercept + b*x + rnorm(n, 0, 0.3)\n\ndf <- data.frame(\n  'x' = c(x),\n  'y' = c(y),\n  'id' = c(1:n)\n)\n\nlibrary(tidyverse)\n\nguessintercept <- -1\nguessb <- -1\n\ndf <- df %>% \n  mutate(guessy = guessintercept + guessb*x)\n\ndf <- df %>% \n  mutate(predictionerrors = abs(y - guessy))\n\ntotalerror <- sum(df$predictionerrors)\ntotalerror\n\n\n[1] 68.11909\n\n# do the same thing but across a range of estimated intercept and slope values\n# save the total error each time\n\ndf <- data.frame(\n  'x' = c(x),\n  'y' = c(y),\n  'id' = c(1:n)\n)\n\nguessintercepts <- seq(from = -1, to = 1, by = 0.1)\nguessbs <- seq(from = -1, to = 1, by = 0.1)\n\ncalculateerror <- function(guessintercept, guessb){\n  \n  df <- df %>% \n    mutate(guessy = guessintercept + guessb*x)\n  \n  df <- df %>% \n    mutate(predictionerrors = abs(y - guessy))\n  \n  totalerror <- sum(df$predictionerrors)\n  \n  return(totalerror)\n  \n}\n\nruns <- length(guessintercepts) * length(guessbs)\ncount <- 0\nstoreresults <- matrix(, ncol = 3, nrow = runs)\n\nfor(int in guessintercepts){\n  for(b in guessbs){\n    count <- count + 1\n    \n    err <- calculateerror(int, b)\n  \n    storeresults[count, 1] <- int\n    storeresults[count, 2] <- b\n    storeresults[count, 3] <- err\n    \n  }\n  \n}\n\nresults <- data.frame(storeresults)\nnames(results) <- c('interceptestimate', 'bestimate', 'predictionerror')\n\ngraphresults <- results %>% \n  unite(interceptestimate, bestimate,\n        col = 'estimates',\n        sep = ', ')\n\nggplot(graphresults %>% filter(predictionerror < 1.7), aes(x = estimates, y = predictionerror)) + \n  geom_point(size = 3, alpha = 0.8) \n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-09-16-ols-via-computation/ols-via-computation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-02-the-rule-of-5/",
    "title": "The Rule of 5",
    "description": {},
    "author": [],
    "date": "2021-08-02",
    "categories": [],
    "contents": "\nDouglas Hubbard’s Rule of 5: There is a 93.8% chance that the median of a population is between the smallest and largest values in any random sample of five from that population.\nWe could derive 93.8 by using equations, or we could run monte carlo simulations.\nOne sample of 5 from a distribution:\n\n\nn <- 1000\nmean <- 300\nsd <- 24\npopulation <- rnorm(n, mean, sd)\n\ndraw <- sample(population, size = 5, replace = F)\nlow <- min(draw)\nhigh <- max(draw)\n\ncontains_true <- NULL\n\nif(low < mean & high > mean){\n  contains_true <- 'yes'\n}else{\n  contains_true <- 'no'\n}\n\nprint(c(low, \n        high,\n        contains_true))\n\n\n[1] \"264.729496040933\" \"324.403709124486\" \"yes\"             \n\nNow iterate the same scheme many times. Does the median fall between the low and high values 93% of the time?\n\n\nsimulations <- 5000\nstoreit <- numeric(simulations)\n\nfor(i in 1:simulations){\n  \n  n <- 1000\n  mean <- 300\n  sd <- 24\n  population <- rnorm(n, mean, sd)\n  \n  draw <- sample(population, size = 5, replace = F)\n  low <- min(draw)\n  high <- max(draw)\n  \n  contains_true <- NULL\n  \n  if(low < mean & high > mean){\n    contains_true <- 'yes'\n  }else{\n    contains_true <- 'no'\n  }\n  \n  storeit[i] <- contains_true\n  \n}\n\nsum(storeit == 'yes') / simulations\n\n\n[1] 0.9444\n\nYes.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-10-bias-from-measurement-error/",
    "title": "Bias From Measurement Error",
    "description": {},
    "author": [],
    "date": "2021-07-10",
    "categories": [],
    "contents": "\nThe Problem\nConsider the notion of incremental validity:\n“If measurements of construct X correlate significantly with outcome Y even when controlling for existing measure(s) Z, then X is a useful predictor of Y, over and above Z.”\nThe issue is that measurement error leads to spurious inferences of incremental validity. To be confident that an incremental validity argument is sound, one needs to either ensure perfect measurement reliability or formally account for unreliability in one’s model.\nExample\nSuppose heat is a common cause of swimming pool deaths and ice cream sales.\n\n\nheat <- 1:100\nswimmingdeaths <- 0.6*heat + rnorm(100, 0, 3)\ncreamsales <- 0.5*heat + rnorm(100, 0, 4)\n\ndf <- data.frame(\n  'heat' = c(heat),\n  'swimmingdeaths' = c(swimmingdeaths),\n  'creamsales' = c(creamsales)\n)\n\n\n\nIf I regress ice cream sales on swimming pool deaths, I (spuriously) conclude that swimming pool deaths predict ice cream sales.\n\n\nround(summary(lm(\n  creamsales ~ swimmingdeaths,\n  data = df\n))$coefficients, 2)\n\n\n               Estimate Std. Error t value Pr(>|t|)\n(Intercept)       -0.34       0.89   -0.39      0.7\nswimmingdeaths     0.86       0.03   33.54      0.0\n\nIf instead I control for heat as a common cause, then the relationship between swimming pool deaths and ice cream sales goes away.\n\n\nround(summary(lm(\n  creamsales ~ swimmingdeaths + heat,\n  data = df\n))$coefficients, 2)\n\n\n               Estimate Std. Error t value Pr(>|t|)\n(Intercept)       -0.34       0.77   -0.44     0.66\nswimmingdeaths     0.13       0.13    0.97     0.33\nheat               0.44       0.08    5.76     0.00\n\nWhat if heat is subjectively measured?\n\n\nheatperceptions <- heat + rnorm(100, 0, 5)\n\nlibrary(tidyverse)\n\ndf <- df %>% \n  mutate(heatperceptions = heatperceptions)\n\n\n\nNow, even when I control for heat perceptions, the spurious relationship between swimming pool deaths and ice cream sales will return.\n\n\nround(summary(lm(\n  creamsales ~ swimmingdeaths + heatperceptions,\n  data = df\n))$coefficients, 2)\n\n\n                Estimate Std. Error t value Pr(>|t|)\n(Intercept)        -0.92       0.82   -1.12     0.27\nswimmingdeaths      0.45       0.09    4.90     0.00\nheatperceptions     0.25       0.06    4.48     0.00\n\nWhat is the solution? Use SEM to control for measurement error.\nLet’s assume that heat is measured with three subjective indicators.\n\n\nperception1 <- 0.6*heat + rnorm(100, 0, 3)\nperception2 <- 0.3*heat + rnorm(100, 0, 3)\nperception3 <- 1*heat + rnorm(100, 0, 3)\n\ndf <- df %>% \n  mutate(hp1 = perception1,\n         hp2 = perception2,\n         hp3 = perception3)\n\n\n\nNow use SEM. The perceptions are caused by a latent heat factor, and then we regress ice cream sales on swimming pool deaths and the latent heat factor.\n\n\nlibrary(lavaan)\n\nmodelstring <- '\n\nlatentheat =~ fl1*hp1 + fl2*hp2 + fl3*hp3\n\ncreamsales ~ b1*swimmingdeaths + b2*latentheat\n\n'\n\nmodel <- sem(modelstring, data = df)\nsummary(model)\n\n\nlavaan 0.6-9 ended normally after 119 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n                                                      \n  Number of observations                           100\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               333.675\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  latentheat =~                                       \n    hp1      (fl1)    1.000                           \n    hp2      (fl2)    0.474    0.019   25.072    0.000\n    hp3      (fl3)    1.685    0.033   51.029    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  creamsales ~                                        \n    swmmngdth (b1)    0.280    0.023   12.330    0.000\n    latenthet (b2)    0.581    0.024   24.262    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .hp1               7.979    1.917    4.163    0.000\n   .hp2               8.834    1.322    6.681    0.000\n   .hp3               8.023    4.497    1.784    0.074\n   .creamsales       14.383    2.142    6.714    0.000\n    latentheat      300.071   43.578    6.886    0.000\n\nWhy is the relationship between swimming pool deaths and ice cream sales still significant?\nSee Jacob Westfall’s original paper for more on this issue.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-02-extra-rows-after-merging/",
    "title": "Extra Rows After Merging",
    "description": {},
    "author": [],
    "date": "2021-06-02",
    "categories": [],
    "contents": "\nAfter merging data, you will generate extra rows if one source has repeated IDs. See below.\nNo ID repetitions = join successful.\n\n\ndf1 <- data.frame(\n  'id' = c(1, 2, 3),\n  'y' = c(20, 30, 40)\n)\n\ndf2 <- data.frame(\n  'id' = c(1, 2, 3),\n  'x' = c(rnorm(3, 30, 5))\n)\n\nlibrary(tidyverse)\n\ndf <- left_join(df1, df2)\ndf\n\n\n  id  y        x\n1  1 20 30.09988\n2  2 30 29.41230\n3  3 40 23.84746\n\nID repetitions = join unsuccessful\n\n\ndata1 <- data.frame(\n  'id' = c(1, 2, 3),\n  'y' = c(20, 23, 40)\n)\n\ndata2 <- data.frame(\n  'id' = c(1, 2, 2), # repeated id\n  'x' = c(60, 70, 80)\n)\n\nlibrary(tidyverse)\n\ndataf <- left_join(data1, data2)\ndataf # has 4 rows rather than 3\n\n\n  id  y  x\n1  1 20 60\n2  2 23 70\n3  2 23 80\n4  3 40 NA\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-23-simulating-network-diffusion/",
    "title": "Simulating Network Diffusion",
    "description": {},
    "author": [],
    "date": "2021-05-23",
    "categories": [],
    "contents": "\nSome practice simulating network diffusion. Inspired by Cheng-jun’s example.\nI’ll start by explaining one iteration through the code before showing the full model. The steps include:\ngenerate a network\nseed it with infected individuals\nfind the nearest neighbors of the infected individual(s)\nthe neighbors become infected with probability p\nadd all of the newly infected individuals to a store list\nfind the nearest neighbors of the infected individual(s)\n… continue until the entire network is infected\nCreate network.\n\n\nlibrary(igraph)\nlibrary(animation)\nlibrary(tidyverse)\n\n\n\n# create network\nnet_size <- 50\ngstar <- graph.star(net_size)\nplot(gstar)\n\n\n\n\nSeed it, meaning place an infected individual into it.\n\n\nnumber_initially_infected <- 1\nfirst_infected <- sample(V(gstar), number_initially_infected)\n## change this graph object to a pure number\nfirst_infected <- as_ids(first_infected)\nfirst_infected\n\n\n[1] 24\n\nPlace the first infected individual into a list.\n\n\ninfected <- list()\ninfected[[1]] <- first_infected\ni <- 1\n\n\n\nFind the neighbors of those who are infected.\n\n\n# find neighbors of those who are infected\nneighbor <- unlist(neighborhood(gstar, 1, unlist(infected)))\n# remove from this list people who are already infected\nneighbor <- neighbor[!neighbor %in% c(unlist(infected))]\nneighbor\n\n\n[1] 1\n\nFor each neighbor, flip a coin to see if he or she becomes infected. Doing so will give me a series of 1s and 0s. 1 means infected. 0 means fine.\n\n\ninfects <- rbinom(length(neighbor), 1, prob = 0.8)\n\n\n\nCombine these 1s and 0s with the neighbor identifiers.\n\n\nallneighbors <- data.frame(\n    \"infected\" = c(infects),\n    \"neighbor\" = c(neighbor)\n  )\n\nallneighbors\n\n\n  infected neighbor\n1        1        1\n\nFilter to only those who are infected. Save their identifiers.\n\n\ninfectedneighbors <- allneighbors %>% \n    filter(infected == 1) %>% \n    pull(neighbor)\n\n\n\nPlace these newly infected individuals into my store list.\n\n\ninfected[[i + 1]] <- infectedneighbors\n\n\n\nLet’s do one more iteration.\n\n\ni <- i + 1\n\n\n\nFind neighbors of all those currently infected.\n\n\n# find neighbors of those who are infected\nneighbor <- unlist(neighborhood(gstar, 1, unlist(infected)))\n# remove from this list people who are already infected\nneighbor <- neighbor[!neighbor %in% c(unlist(infected))]\nneighbor\n\n\n [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n[23] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n[45] 47 48 49 50\n\nFor each neighbor, flip a coin to see if he or she becomes infected. Doing so will give me a series of 1s and 0s. 1 means infected. 0 means fine.\n\n\ninfects <- rbinom(length(neighbor), 1, prob = 0.8)\n\n\n\nCombine these 1s and 0s with the neighbor identifiers.\n\n\nallneighbors <- data.frame(\n    \"infected\" = c(infects),\n    \"neighbor\" = c(neighbor)\n  )\n\nallneighbors\n\n\n   infected neighbor\n1         1        2\n2         1        3\n3         1        4\n4         1        5\n5         0        6\n6         0        7\n7         0        8\n8         1        9\n9         1       10\n10        0       11\n11        1       12\n12        1       13\n13        1       14\n14        1       15\n15        1       16\n16        1       17\n17        1       18\n18        1       19\n19        0       20\n20        1       21\n21        1       22\n22        0       23\n23        1       25\n24        1       26\n25        1       27\n26        0       28\n27        0       29\n28        1       30\n29        0       31\n30        0       32\n31        1       33\n32        1       34\n33        0       35\n34        1       36\n35        1       37\n36        1       38\n37        1       39\n38        1       40\n39        1       41\n40        1       42\n41        1       43\n42        1       44\n43        1       45\n44        1       46\n45        1       47\n46        0       48\n47        1       49\n48        0       50\n\nFilter to only those who are infected. Save their identifiers.\n\n\ninfectedneighbors <- allneighbors %>% \n    filter(infected == 1) %>% \n    pull(neighbor)\n\n\n\nPlace these newly infected individuals into my store list.\n\n\ninfected[[i + 1]] <- infectedneighbors\n\n\n\nPlot the infected individuals.\n\n\nV(gstar)$color[V(gstar)%in%unlist(infected)] <- \"red\"\nplot(gstar)\n\n\n\n\nFull Model\nI’ll plot the network at each iteration so we can see it update.\n\n\nlibrary(igraph)\nlibrary(animation)\nlibrary(tidyverse)\n# infection probability\nprob <- 0.8 \n\n\n# create network\nnet_size <- 50\ngstar <- graph.star(net_size)\nplot(gstar)\n\n\n\n# seed it, meaning place an infected individual (or individuals) into it\nnumber_initially_infected <- 1\nfirst_infected <- sample(V(gstar), number_initially_infected)\nfirst_infected <- as_ids(first_infected)\ninfected <- list()\ninfected[[1]] <- first_infected\n\n\n# iterate network dynamics\n\ni <- 1\ntotal_infected <- unlist(infected)\nV(gstar)$color[V(gstar)%in%total_infected] <- \"red\"\nplot(gstar)\n\n\n\nwhile(length(total_infected) < net_size){\n  \n  neighbor <- unlist(neighborhood(gstar, 1, unlist(infected)))\n  neighbor <- neighbor[!neighbor %in% c(unlist(infected))]\n  infects <- rbinom(length(neighbor), 1, prob = prob)\n  allneighbors <- data.frame(\n    \"infected\" = c(infects),\n    \"neighbor\" = c(neighbor)\n  )\n  infectedneighbors <- allneighbors %>% \n    filter(infected == 1) %>% \n    pull(neighbor)\n  infected[[i + 1]] <- infectedneighbors\n  total_infected <- unlist(infected)\n  i <- i + 1\n  \n  V(gstar)$color[V(gstar)%in%total_infected] <- \"red\"\n  plot(gstar)\n}\n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-05-23-simulating-network-diffusion/simulating-network-diffusion_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-01-systems-zoo/",
    "title": "Systems Zoo",
    "description": {},
    "author": [],
    "date": "2021-05-01",
    "categories": [],
    "contents": "\nSome practice simulating stock and flow models. The first example set uses discrete time. The second uses continuous time differential equations. I include some text from Donella Meadows’ book, “Thinking in Systems” to explain some of the examples.\nExample 1: Bath Tub\n\n“Imagine a bathtub filled with water, with its drain plugged up and its faucets turned off—an unchanging, undynamic, boring system. Now mentally pull the plug. The water runs out, of course. The level of water in the tub goes down until the tub is empty.” (p. 19).\n\n\nlibrary(ggplot2)\nlibrary(hrbrthemes)\n\n# time\ntime <- 10\n\n# stock storehouses - start and end for each stock\n## water in tub\nstartwater <- numeric(time)\nendwater <- numeric(time)\n\n# initial conditions in each stock\n## water in tub\nstartwater[1] <- 10\nendwater[1] <- startwater[1]\n\n# constants\n## outflow (gallons exiting the tub per minute)\noutflow <- 1\n## inflow (gallons entering the tub per minute)\ninflow <- 0\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startwater[i] <- endwater[i - 1]\n  \n  # model system\n  endwater[i] <- startwater[i] + inflow - outflow\n  \n}\n\ndftub1 <- data.frame(\n  \"gallons\" = c(endwater),\n  \"minutes\" = c(1:time)\n)\n\nggplot(dftub1, aes(x = minutes, y = gallons)) + \n  geom_line() + \n  theme_classic()\n\n\n\n\n“Now imagine starting again with a full tub, and again open the drain, but this time, when the tub is about half empty, turn on the inflow faucet so the rate of water flowing in is just equal to that flowing out. What happens? The amount of water in the tub stays constant at whatever level it had reached when the inflow became equal to the outflow. It is in a state of dynamic equilibrium – its level does not change, although water is continuously flowing through it.” (p. 21).\n\n\n# time\ntime <- 10\n\n# stock storehouses - start and end for each stock\n## water in tub\nstartwater <- numeric(time)\nendwater <- numeric(time)\n\n# initial conditions in each stock\n## water in tub\nstartwater[1] <- 10\nendwater[1] <- startwater[1]\n\n# constants\n## none - flows change midway through the process\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startwater[i] <- endwater[i - 1]\n  \n  # model system\n  \n  ## open the drain\n  if(i < 5){\n    inflow <- 0\n    outflow <- 1\n  ## then, turn on the faucet so water flows into the tub\n  }else{\n    inflow <- 1\n    outflow <- 1\n  }\n  \n  endwater[i] <- startwater[i] + inflow - outflow\n  \n}\n\ndftub2 <- data.frame(\n  \"gallons\" = c(endwater),\n  \"minutes\" = c(1:time)\n)\n\nggplot(dftub2, aes(x = minutes, y = gallons)) + \n  geom_line() + \n  theme_classic() + \n  ylim(0, 10)\n\n\n\n\nExample 2: Coffee Temperature\n\n“A hot cup of coffee will gradually cool down to room temperature. Its rate of cooling depends on the difference between the temperature of the coffee and the temperature of the room. The greater the difference, the faster the coffee will cool. The loop works the other way too – if you make iced coffee on a hot day, it will warm up until it has the same temperature as the room. The function of this system is to bring the discrepancy between coffee’s temperature and room’s temperature to zero, no matter what the direction of the discrepancy. Whatever the initial value of the system stock (coffee temperature in this case), whether it is above or below the ‘goal’ (room temperature), the feedback loop brings it toward the goal. The change is faster at first, and then slower, as the discrepancy between the stock and the goal decreases.” (p. 29).\n\n\n# time\ntime <- 4\n\n# stock storehouses - start and end for each stock\n## coffee\nstartcoffee <- numeric(time)\nendcoffee <- numeric(time)\n\n## iced coffee\nstartic <- numeric(time)\nendic <- numeric(time)\n\n# initial conditions in each stock\n## temperature of coffee\nstartcoffee[1] <- 96\nendcoffee[1] <- startcoffee[1]\n\n## temperature of iced coffee\nstartic[1] <- 34\nendic[1] <- startic[1]\n\n# constants\n## room temperature\nroom_temp <- 71\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startcoffee[i] <- endcoffee[i - 1]\n  startic[i] <- endic[i - 1]\n  \n  # model system\n  \n  ## system for hot coffee\n  difference_coffee <- abs(room_temp - startcoffee[i])\n  cooling <- difference_coffee\n  endcoffee[i] <- startcoffee[i] - cooling\n  \n  ## system for iced coffee\n  difference_ic <- room_temp - startic[i]\n  heating <- difference_ic\n  endic[i] <- startic[i] + heating\n  \n}\n\ndfcoffee <- data.frame(\n  \"var\" = c(rep('Hot Coffee', time),\n            rep('Iced Coffee', time)),\n  \"temperature\" = c(endcoffee,\n                    endic),\n  \"minutes\" = c(c(1:time),\n                c(1:time))\n)\n\nggplot(dfcoffee, aes(x = minutes, y = temperature, linetype = var)) + \n  geom_line() + \n  geom_hline(yintercept = room_temp, linetype = \"dashed\", color = 'red') + \n  geom_text(aes(x = 1.3, y = room_temp + 1.8, label = \"Room Temperature\"), color = \"darkred\") + \n  theme(legend.title = element_blank())\n\n\n\n\nThe sampling window isn’t small enough to capture the gradual decay/growth of temperature. I can artificially slow the process down by making the effects weaker.\n\n\n# time\ntime <- 20\n\n# stock storehouses - start and end for each stock\n## coffee\nstartcoffee <- numeric(time)\nendcoffee <- numeric(time)\n\n## iced coffee\nstartic <- numeric(time)\nendic <- numeric(time)\n\n# initial conditions in each stock\n## temperature of coffee\nstartcoffee[1] <- 96\nendcoffee[1] <- startcoffee[1]\n\n## temperature of iced coffee\nstartic[1] <- 34\nendic[1] <- startic[1]\n\n# constants\n## room temperature\nroom_temp <- 71\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startcoffee[i] <- endcoffee[i - 1]\n  startic[i] <- endic[i - 1]\n  \n  # model system\n  \n  ## system for hot coffee\n  difference_coffee <- abs(room_temp - startcoffee[i])\n  #### HERE IS THE CHANGE\n  cooling <- 0.2*difference_coffee\n  endcoffee[i] <- startcoffee[i] - cooling\n  \n  ## system for iced coffee\n  difference_ic <- room_temp - startic[i]\n  #### HERE IS THE CHANGE\n  heating <- 0.2*difference_ic\n  endic[i] <- startic[i] + heating\n  \n}\n\ndfcoffee <- data.frame(\n  \"var\" = c(rep('Hot Coffee', time),\n            rep('Iced Coffee', time)),\n  \"temperature\" = c(endcoffee,\n                    endic),\n  \"minutes\" = c(c(1:time),\n                c(1:time))\n)\n\nggplot(dfcoffee, aes(x = minutes, y = temperature, linetype = var)) + \n  geom_line() + \n  geom_hline(yintercept = room_temp, linetype = \"dashed\", color = 'red') + \n  geom_text(aes(x = 5, y = room_temp + 1.8, label = \"Room Temperature\"), color = \"darkred\") + \n  theme(legend.title = element_blank())\n\n\n\n\nExample 3: Investing\n\n“The more money you have in the bank, the more interest you earn, which is added to the money already in the bank, where it earns even more interest. Consider how this reinforcing loop multiplies money, starting with $100 in the bank, and assuming no deposits and no withdrawals over a period of ten years.” (p. 32).\n\n\n# time\ntime <- 10\n\n# stock storehouses - start and end for each stock\n## money in bank\nstartmoney <- numeric(time)\nendmoney <- numeric(time)\n\n# initial conditions in each stock\n## money in bank\nstartmoney[1] <- 100\nendmoney[1] <- startmoney[1]\n\n# constants\n## interest rate\ninterest_rate <- 0.05\n## money from income (zero)\nmoney_from_income <- 0\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startmoney[i] <- endmoney[i - 1]\n  \n  # model system\n  money_from_interest <- startmoney[i] * interest_rate\n  money_added <- money_from_income + money_from_interest\n  endmoney[i] <- startmoney[i] + money_added\n  \n}\n\ndfmoney <- data.frame(\n  \"money\" = c(endmoney),\n  \"years\" = c(1:time),\n  \"interest_rate\" = c(rep(interest_rate, time))\n)\n\n\n\nRepeat for different interest rates.\n\n\n\n\n\nlibrary(dplyr)\ndfint2 <- run_interest(1.0)\ndfint3 <- run_interest(1.5)\ndfmoney <- bind_rows(dfmoney, dfint2)\ndfmoney <- bind_rows(dfmoney, dfint3)\n\nggplot(dfmoney, aes(x = years, y = money, linetype = factor(interest_rate))) + \n  geom_line() + \n  theme_classic() + \n  labs(linetype = \"Interest Rate\")\n\n\n\n\nExample 4: Room Temperature\n\n“What happens if there are two such loops, trying to drag a single stock toward two different goals? One example of such a system is the thermostat mechanism that regulates the heating of your room (or cooling, if it is connected to an air conditioner instead of a furnace). Whenever the room temperature falls below the thermostat setting, the thermostat detects a discrepancy and sends a signal that turns on the heat fl ow from the furnace, warming the room. When the room temperature rises again, the thermostat turns off the heat fl ow. This is a straightforward, stock-maintaining, balancing feedback loop. However, this is not the only loop in the system. Heat also leaks to the outside. The outflow of heat is governed by the second balancing feedback loop. It is always trying to make the room temperature equal to the outside, just like a coffee cup cooling.\nAt first, both the room and the outside temperatures are cool. The inflow of heat from the furnace exceeds the leak to the outside, and the room warms up. For an hour or two, the outside is mild enough that the furnace replaces most of the heat that’s lost to the outside,and the room temperature stays near the desired temperature. But as the outside temperature falls and the heat leak increases, the furnace cannot replace the heat fast enough. Because the furnace is generating less heat than is leaking out, the room temperature falls. Finally, the outside temperature rises again, the heat leak slows, and the furnace, still operating at full tilt, finally can pull ahead and start to warm the room again. Just as in the rules for the bathtub, whenever the furnace is putting in more heat than is leaking out, the room temperature rises. Whenever the inflow rate falls behind the outflow rate, the temperature falls.” (p. 41)\nFirst, use a cosine function to create data mimicking the idea that “temperature outside drops throughout the night and then increases in the morning.”\n\n\npoints <- 1:51\noutside <- cos(0.11 * points)\nplot(outside)\n\n\n\n\n\n\n# time\ntime <- 51\n\n# stock storehouses - start and end for each stock\n## room temperature\nstartroom <- numeric(time)\nendroom <- numeric(time)\n\n# initial conditions in each stock\n## room temperature\nstartroom[1] <- 0.4\nendroom[1] <- startroom[1]\n\n# constants\n## thermostat setting\nthermostat <- 2\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startroom[i] <- endroom[i - 1]\n  \n  # model system\n  difference_outroom <- abs(outside[i] - startroom[i])\n  heatloss <- difference_outroom\n  \n  difference_thermoroom <- thermostat - startroom[i]\n  heatin <- difference_thermoroom\n  \n  endroom[i] <- startroom[i] + heatin - heatloss\n  \n}\n\ndffurnace <- data.frame(\n  \"temperature\" = c(endroom,\n                    rep(thermostat, time),\n                    outside),\n  \"time\" = c(1:time,\n             1:time,\n             1:time),\n  \"var\" = c(rep(\"Room Temperature\", time),\n            rep(\"Thermostat\", time),\n            rep(\"Outside Temperature\", time))\n)\n\n\n\nggplot(dffurnace, aes(x = time, y = temperature, color = var)) + \n  geom_line() + \n  theme_classic() + \n  theme(axis.text.y = element_blank()) + \n  labs(color = \"\")\n\n\n\n\nOnce again, I can smooth the process by weakening the effects.\n\n\n# time\ntime <- 51\n\n# stock storehouses - start and end for each stock\n## room temperature\nstartroom <- numeric(time)\nendroom <- numeric(time)\n\n# initial conditions in each stock\n## room temperature\nstartroom[1] <- 0.4\nendroom[1] <- startroom[1]\n\n# constants\n## thermostat setting\nthermostat <- 2\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startroom[i] <- endroom[i - 1]\n  \n  # model system\n  difference_outroom <- abs(outside[i] - startroom[i])\n  heatloss <- 0.5*difference_outroom\n  \n  difference_thermoroom <- thermostat - startroom[i]\n  heatin <- 0.5*difference_thermoroom\n  \n  endroom[i] <- startroom[i] + heatin - heatloss\n  \n}\n\ndffurnace <- data.frame(\n  \"temperature\" = c(endroom,\n                    rep(thermostat, time),\n                    outside),\n  \"time\" = c(1:time,\n             1:time,\n             1:time),\n  \"var\" = c(rep(\"Room Temperature\", time),\n            rep(\"Thermostat\", time),\n            rep(\"Outside Temperature\", time))\n)\n\n\n\nggplot(dffurnace, aes(x = time, y = temperature, color = var)) + \n  geom_line() + \n  theme_classic() + \n  theme(axis.text.y = element_blank()) + \n  labs(color = \"\")\n\n\n\n\nExample 5: Car Inventory With Delays\nMy stock and flow diagram is different from Fig 31 in “Thinking in Systems” because I like to include an additional “things in process / things in queue” stock when modeling delays.\n\n“This system is a version of the thermostat system—one balancing loop of sales draining the inventory stock and a competing balancing loop maintaining the inventory by resupplying what is lost in sales. Although this system still consists of just two balancing loops, like the simplified thermostat system, it doesn’t behave like the thermostat system. Look at what happens, for example, when the business experiences the same permanent 10-percent jump in sales from an increase in customer demand. Oscillations! A single step up in sales causes inventory to drop. The car dealer watches long enough to be sure the higher sales rate is going to last. Then she begins to order more cars to both cover the new rate of sales and bring the inventory up. But it takes time for the orders to come in. During that time inventory drops further, so orders have to go up a little more, to bring inventory back up to ten days’ coverage. Eventually, the larger volume of orders starts arriving, and inventory recovers—and more than recovers, because during the time of uncertainty about the actual trend, the owner has ordered too much. She now sees her mistake, and cuts back, but there are still high past orders coming in, so she orders even less. In fact, almost inevitably, since she still can’t be sure of what is going to happen next, she orders too little. Inventory gets too low again. And so forth, through a series of oscillations around the new desired inventory level.” (p. 54).\n\n\n# time\ntime <- 25\n\n# stock storehouses - start and end for each stock\n## cars in inventory\nstartcars <- numeric(time)\nendcars <- numeric(time)\n## cars in queue\nstartque <- numeric(time)\nendque <- numeric(time)\n\n# initial conditions in each stock\n## cars in inventory\nstartcars[1] <- 78\nendcars[1] <- startcars[1]\n## cars in queue\nstartque[1] <- 0\nendque[1] <- startque[1]\n\n# constants\n## desired inventory of cars in lot\ndesired_inventory <- 100\n## time to ship (3 days)\ntime_to_ship <- 3\n## time to receive order (half a day)\ntime_to_receive <- 0.5\n## sales each day\nsales <- 18\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startcars[i] <- endcars[i - 1]\n  startque[i] <- endque[i - 1]\n  \n  # model system\n  need_for_cars <- desired_inventory - startcars[i]\n  building <- need_for_cars / time_to_receive\n  arrivals <- startque[i] / time_to_ship\n  \n  endque[i] <- startque[i] + building - arrivals\n  endcars[i] <- startcars[i] + arrivals - sales\n  \n}\n\ndfcars <- data.frame(\n  \"cars\" = c(endcars),\n  \"days\" = c(1:time)\n)\n\nlibrary(hrbrthemes)\nggplot(dfcars, aes(x = days, y = cars)) + \n  geom_line() + \n  theme_ipsum()\n\n\n\n\nExample 6: House Inventory\nFind lecture online.\n\n\n\n# time\ntime = 50\n\n# stocks\nstarthouses = numeric(time)\nstarthouses[1] = 1000\nendhouses = numeric(time)\nendhouses[1] = 1000\n\n# constants\ngoal_houses = 10000\n## if there is a gap of 8000 houses (goal of 10000 but currently 2000)\n## then they will resolve that gap in 9 periods\ntimetoresolve = 9\n\n\nfor(i in 2:time){\n  \n  starthouses[i] = endhouses[i-1]\n  \n  need_for_houses = goal_houses - starthouses[i]\n  \n  building = need_for_houses / timetoresolve\n  \n  endhouses[i] = starthouses[i] + building \n  \n}\ndfhouses <- data.frame(\n  \"time\" = c(1:time),\n  \"houses\" = c(endhouses)\n)\n\nggplot(dfhouses, aes(x = time, y = houses)) + \n  geom_point()\n\n\n\n\nExample 7: House Inventory With Muliple Delays\nFind lecture online.\n\n\n\n# time\ntime = 100\n\n# stocks\n## houses in process (works in progress)\nstartwip = numeric(time)\nstartwip[1] = 0\nendwip = numeric(time)\nendwip[1] = 0\n\n## completed houses\nstarthouses = numeric(time)\nstarthouses[1] = 1000\nendhouses = numeric(time)\nendhouses[1] = 1000\n\n# constants\ngoal_houses = 10000\ntimetostartbuild = 3\ntimetocomplete = 12\n\n\nfor(i in 2:time){\n  \n  startwip[i] = endwip[i - 1]\n  starthouses[i] = endhouses[i - 1]\n  \n  needforhouses = goal_houses - starthouses[i]\n  building = needforhouses / timetostartbuild\n  finishing = startwip[i] / timetocomplete\n  endwip[i] = startwip[i] + building - finishing\n  \n  \n  endhouses[i] = starthouses[i] + finishing\n  \n}\n\n\ndf <- data.frame(\n  \"houses\" = c(endhouses),\n  \"time\" = c(1:time)\n)\n\nggplot(df, aes(x = time, y = houses)) + \n  geom_point() + \n  theme_ipsum()\n\n\n\n\nExample 8: Customers In Store\nSimilar example to money in the bank, but now both the inflow and outflow (not just the inflow) will be proportional to the level of the stock.\n\nGrowth Fraction:\n7%, then 3%, then 1%\nDecline Fraction:\n3% throughout\n\n\n# time\ntime <- 30\n\n# stock storehouses - start and end for each stock\n## customers\nstartcustomers <- numeric(time)\nendcustomers <- numeric(time)\n\n# initial conditions in each stock\n## customers\nstartcustomers[1] <- 5\nendcustomers[1] <- startcustomers[1]\n\n# constants\n## decline fraction\ndecline <- 0.03\n\n\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  startcustomers[i] <- endcustomers[i - 1]\n  \n  # model system\n  if(i < 11){\n    growth <- 0.07\n  }else if(i > 10 && i < 21){\n    growth <- 0.03\n  }else{\n    growth <- 0.01\n  }\n  \n  losses <- startcustomers[i]*decline\n  recruits <- startcustomers[i]*growth\n  \n  endcustomers[i] <- startcustomers[i] + recruits - losses\n  \n  \n}\n\ndfcustomers <- data.frame(\n  \"customers\" = c(endcustomers),\n  \"minutes\" = c(1:time)\n)\n\nggplot(dfcustomers, aes(x = minutes, y = customers)) + \n  geom_line() + \n  theme_ipsum()\n\n\n\n\nExample 9: SIR Disease Model\n\n\n\n# time\ntime <- 100\n\n# stock storehouses - start and end for each stock\n## susceptible\nstarts <- numeric(time)\nends <- numeric(time)\n## infected\nstarti <- numeric(time)\nendi <- numeric(time)\n## recovered\nstartr <- numeric(time)\nendr <- numeric(time)\n\n# initial conditions in each stock\n## susceptible\nstarts[1] <- 99\nends[1] <- starts[1]\n## infected\nstarti[1] <- 1\nendi[1] <- starti[1]\n## recovered\nstartr[1] <- 0\nendr[1] <- 0\n\n# constants\n## infection rate, or fraction infected from contact\ninfectionrate <- 0.8\n## contact rate, or fraction of people (infected and susceptible) who touch\ncontactrate <- 0.5\n## recovery rate\nrecoveryrate <- 0.3\n## delay\ndelay <- 2\n\n# iterate\nfor(i in 2:time){\n  \n  # start stocks at period i are ending stocks at period i-1\n  starts[i] <- ends[i - 1]\n  starti[i] <- endi[i - 1]\n  startr[i] <- endr[i - 1]\n  \n  # model system\n  totalpopulation <- starts[i] + starti[i]\n  fractionofpopulationinfected <- totalpopulation / starti[i]\n  contacts <- (fractionofpopulationinfected + starts[i])*contactrate\n  fractioninfected <- contacts*infectionrate\n  catch_disease <- 0.2*fractioninfected\n  recovering <- 0.2*(starti[i]*recoveryrate / delay)\n  \n  ends[i] <- starts[i] - catch_disease\n  endi[i] <- starti[i] - recovering + catch_disease\n  endr[i] <- startr[i] + recovering\n  \n}\n\ndfsir <- data.frame(\n  \"people\" = c(ends,\n               endi,\n               endr),\n  \"time\" = c(1:time),\n  \"var\" = c(rep(\"Suseptible\", time),\n            rep(\"Infected\", time),\n            rep(\"Recovered\", time))\n)\n\nggplot(dfsir, aes(x = time, y = people, color = var)) + \n  geom_point() +\n  labs(color = \"\")\n\n\n\n\nExamples Using Continuous Time Modeling\nRoom Temperature\n\n\n\nlibrary(deSolve)\n\nstart <- 0\nfinish <- 51\nstep <- 0.25\nsimtime <- seq(start, finish, by = step)\n\nstocks <- c(room = 0.4)\n\nauxs <- c(thermostat = 2)\n\nmodel <- function(time, stocks, auxs){\n  \n  with(as.list(c(stocks, auxs)), {\n    \n    outside <- cos(0.11*time)\n    difference_outroom <- abs(outside - room)\n    heatloss <- difference_outroom\n    \n    difference_thermoroom <- thermostat - room\n    heatin <- difference_thermoroom\n    \n    droom <- heatin - heatloss\n    \n    return(list(c(droom),\n           out = outside,\n           thermostat = thermostat))\n    \n    \n  })\n  \n  \n}\n\no <- data.frame(ode(y = stocks, times = simtime, \n                    func = model,\n                    parms = auxs,\n                    method = \"euler\"))\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\no <- o %>% \n  pivot_longer(cols = c(room, out, thermostat),\n               names_to = \"var\",\n               values_to = \"temperature\")\n\nggplot(o, aes(x = time, y = temperature, color = var)) + \n  geom_line()\n\n\n\n\nCar Inventory\n\n\n\nlibrary(deSolve)\n\nstart <- 0\nfinish <- 3\nstep <- 0.25\nsimtime <- seq(start, finish, by = step)\n\nstocks <- c(\n  cars = 78,\n  que = 0)\n\nauxs <- c(\n  desired_inventory = 100,\n  time_to_ship = 1,\n  time_to_recieve = 0.1,\n  sales <- 18\n)\n\nmodel <- function(time, stocks, auxs){\n  \n  with(as.list(c(stocks, auxs)), {\n    \n    need_for_cars <- desired_inventory - cars\n    building <- need_for_cars / time_to_recieve\n    arrivals <- que / time_to_ship\n    \n    dcars <- building - arrivals\n    dque <- arrivals - sales\n    \n    return(list(c(dcars, dque)))\n    \n    \n    \n    \n    \n  })\n  \n  \n}\n\n\no <- data.frame(ode(y = stocks, times = simtime, \n                    func = model, parms = auxs, method = \"euler\"))\n\nlibrary(ggplot2)\nggplot(o, aes(x = time, y = cars)) + \n  geom_line()\n\n\n\n\nCustomers in Store\n\n\n\nlibrary(deSolve)\n\nstart <- 0\nfinish <- 10\nstep <- 0.25\nsimtime <- seq(start, finish, by = step)\n\nstocks <- c(customers = 5)\n\nauxs <- c(decline = 0.03)\n\nmodel <- function(time, stocks, auxs){\n  \n  with(as.list(c(stocks, auxs)), {\n    \n    if(time < 3){\n      growth <- 0.07\n    }else if (time > 2 && time < 6) {\n      growth <- 0.03\n    }else{\n      growth <- 0.01\n    }\n    \n    losses <- customers*decline\n    recruits <- customers*growth\n    \n    dcustomers <- recruits - losses\n    \n    return(list(c(dcustomers)))\n    \n    \n  })\n  \n  \n}\n\no <- data.frame(ode(y = stocks, times = simtime, \n                    func = model, parms = auxs, method = \"euler\"))\n\n\nlibrary(ggplot2)\nggplot(o, aes(x = time, y = customers)) + \n  geom_point()\n\n\n\n\nDisease Contagion SIR Model\n\n\n\nlibrary(deSolve)\n\nstart <- 0\nfinish <- 25\nstep <- 0.1\nsimtime <- seq(start, finish, by = step)\n\nstocks <- c(susceptible = 99,\n            infected = 1,\n            recovered = 0)\n\nauxs <- c(contactrate = 0.5,\n          infectionrate = 0.8,\n          recoveryrate = 0.3,\n          delay = 2)\n\nmodel <- function(time, stocks, auxs){\n  \n  with(as.list(c(stocks, auxs)), {\n    \n    totalpopulation <- susceptible + infected\n    fractionofpopulationinfected <- totalpopulation / infected\n    contacts <- (fractionofpopulationinfected + susceptible)*contactrate\n    fractioninfected <- contacts*infectionrate\n    catchdisease <- fractioninfected\n    recovering <- infected*recoveryrate / delay\n    \n    dsusceptible <- -catchdisease\n    dinfected <- catchdisease - recovering\n    drecovered <- recovering\n    \n    return(list(c(dsusceptible, \n                  dinfected,\n                  drecovered)))\n    \n    \n  })\n  \n  \n  \n}\n\n\no <- data.frame(ode(y = stocks, times = simtime,\n                    func = model,\n                    parms = auxs,\n                    method = \"euler\"))\n\nlibrary(ggplot2)\nlibrary(tidyverse)\no <- o %>% \n  pivot_longer(cols = c(susceptible, \n                        infected,\n                        recovered),\n               names_to = \"var\",\n               values_to = \"val\")\n\nggplot(o, aes(x = time, y = val, color = var)) + \n  geom_point()\n\n\n\n\nMultiple Goal Self-Regulation\n\n\n\nlibrary(deSolve)\n\nstart <- 0\nfinish <- 5\nstep <- 1\nsimtime <- seq(start, finish, by = step)\n\nstocks <- c(\n  g1_performance = 23,\n  g2_performance = 1\n)\n\nauxs <- c(\n  g1_desire = 25,\n  g2_desire = 25\n)\n\nmodel <- function(time, stocks, auxs){\n  \n  with(as.list(c(stocks, auxs)), {\n    \n    differenceg1 <- g1_desire - g1_performance\n    differenceg2 <- g2_desire - g2_performance\n    \n    behavioral_choice = NULL\n    if(differenceg1 > differenceg2){\n      behavioral_choice <- \"g1\"\n    }else{\n      behavioral_choice <- \"g2\"\n    }\n    \n    inputg1 <- 0\n    inputg2 <- 0\n    if(behavioral_choice == \"g1\"){\n      inputg1 <- differenceg1 * 0.5 + rnorm(1, 0, 0.1)\n    }else{\n      inputg2 <- differenceg2 * 0.5 + rnorm(1, 0, 0.1)\n    }\n    \n    dg1_performance = inputg1\n    dg2_performance = inputg2\n    \n    return(list(c(dg1_performance, dg2_performance)))\n    \n    \n  })\n  \n  \n  \n}\n\no <- data.frame(ode(y = stocks, times = simtime,\n                    func = model, \n                    parms = auxs,\n                    method = \"euler\"))\n\n\nlibrary(ggplot2)\nlibrary(tidyverse)\no <- o %>% \n  pivot_longer(cols = c(g1_performance, g2_performance),\n               names_to = \"var\",\n               values_to = \"performance\")\n\nggplot(o, aes(x = time, y = performance, color = var)) + \n  geom_point()\n\n\n\n\nSummary Steps\nBuild Image\nSave image with a file size of 500%\nWhen rendering Rmarkdown, use ![](images/file.png){width=80%}\nWrite simulation code\n\n\n# time\n\n# stocks\nstart\nend\n\n# constants\n\n# iterate\nfor(i in 2:time)\n  \n  ## begin loop with stock connections\n  start[i] <- end[i - 1]\n\n# model system notes\n## rates are multiplied\n## delays are divisors\n\n# always plot the end stock\n\n\n\nDrawing Stock and Flow Diagrams\nOdd number of negative relationships in the loop = balancing\nIf an effect is proportional to the stock’s level, then draw an arrow from the stock to that effect\nWhen modeling delays, sometimes it is easier to include an “in queue” stock rather than a node effect\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-05-01-systems-zoo/systems-zoo_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-18-translucent-ggplot2-background-for-powerpoint/",
    "title": "Translucent ggplot2 background for powerpoint",
    "description": {},
    "author": [],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\nThe following parameters will render a translucent ggplot. When pasted into powerpoint, the background will not appear, leaving only the powerpoint template.\n\n\nggplot() + \n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\nUnfortunately, this doesn’t work with theme_bw() or theme_classic().\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-01-reminder-cleaning-commands-longitudinal/",
    "title": "Reminder Cleaning Commands - Longitudinal",
    "description": {},
    "author": [],
    "date": "2021-03-01",
    "categories": [],
    "contents": "\nA few reminders for longitudinal wrangling:\ncreating file names\nadvanced filtering\nfind people with full data\nreshape issues\nCreating File Names\n“wave1.dta”\n“wave2.dta”\n“wave3.dta”\netc…\n\n\n# file names\nfiles <- paste0(\"wave\", 1:10, \".dta\")\n\n# which can then be used in a function\n\ncombine_files <- function(x){\n  \n  df <- read_dta(paste0(\"../data/another-folder/\", x))\n\n}\n\n# ...and iterated over\ncombine_files(files[1])\n\n\n\nAdvanced Filtering\nLet’s say I’m iterating over multiple data frames. For each data frame, I want to filter to include only people who are currently employed (1 = yes, 0 = no). The question asking whether a respondent is employed is “wave1_emp” in the first data set, “wave2_emp” in the second data set, “wave3_emp” in the third data set, etc.\n\n\ndf %>% \n  filter_at(vars(contains(\"_emp\")), all_vars(. == 1))\n\n\n\nThis command is robust across the different q formats within various waves, or across various waves.\nFind People With Full Data\nMake the df wide. Drop NAs. Pull unique ids. Filter original long df to include only those ids from previous step.\n\n\ndf_wide <- df_wide %>% \n  select_at(vars(contains(c(\"work\", \"sat\", \"cond\", \"time\", \"id\")))) %>% \n  drop_na()\n\nuse_ids <- unique(df_wide$id)\n\n# use long, not wide df here\ndf_no_missing <- df_long %>% \n  filter(id %in% use_ids)\n\n\n\nReshape Issue\nI prefer reshape over pivot_wider / pivot_longer. Unfortunately, the function does not work well with tibbles.\n\n\n# no good\ndf_wide <- reshape(df_tibble, idvar = \"id\", timevar = \"time\", direction = \"wide\")\n\n# that'll work\ndf <- as.data.frame(df_tibble)\ndf_wide <- reshape(df, idvar = \"id\", timevar = \"time\", direction = \"wide\")\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-approximating-differential-equations/",
    "title": "Approximating Differential Equations",
    "description": {},
    "author": [],
    "date": "2021-01-17",
    "categories": [],
    "contents": "\nHere’s how to approximate a differential equation using discrete simulations in R. Differential equations will be presented in the form\n\\[\\begin{equation}\n\\frac{dx}{dt}\n\\end{equation}\\]\nwhere \\(dx\\) is the change in whatever stock \\(x\\) represents and \\(dt\\) is the length of the time step. Any differential equation can be rearranged as\n\\[\\begin{equation}\ndx = f() * dt.\n\\end{equation}\\]\nwhere some function is multiplied by the time step \\(dt\\). In R, you could make this time step 0.000001, which is close enough to continuous that it often approximates functions well. Once you calculate \\(dx\\), or the change in \\(x\\) over a small time step (i.e., 0.000001), then you can add this change to the current value of \\(x\\):\n\\[\\begin{equation}\nx_{new} = x + dx\n\\end{equation}\\]\nRinse and repeat. Here is an example using the famous Lotka-Volterra Equations. Predator-Prey dynamics can be modeled with:\n\\[\\begin{equation}\n\\frac{dx}{dt} = Ax - Bxy\n\\end{equation}\\]\n\\[\\begin{equation}\n\\frac{dy}{dt} = Cxy - Dy\n\\end{equation}\\]\nwhere\n\\(x\\) is the number of prey\n\\(y\\) is the number of predators\n\\(A\\) is the birth rate of prey\n\\(B\\) is the contact rate between predators and prey\n\\(C\\) can either be equivalent to \\(B\\), or it can be thought of as the predator birth rate due to the presence of prey\n\\(D\\) is the death rate of predators in the absence of prey\n\\(dx\\) is the change in number of prey\n\\(dy\\) is the change in the number of predators\n\\(dt\\) is the time step (not number of time points).\nWe can numerically approximate these equations by multiplying each equation by \\(dt\\) and then simulating with a small time step (e.g., 0.0001).\nA discrete time version would be:\n\ndx <- (Ax - Bxy) * small_step\ndy <- (Cxy - Dy) * small_step\n\nx_new <- x + dx\ny_new <- y + dy\n\nrepeat many times...\n\nLet’s run it.\n\n\nstep <- 0.1\ntime <- seq(from = step, to = 100, by = step)\nx <- numeric(length(time))\ny <- numeric(length(time))\nx[1] <- 3\ny[1] <- 5\nA <- 1\nB <- 0.2\nC <- 0.04\nD <- 0.5\n\ncount <- 0\nfor(i in time){\n  count <- count + 1\n  \n  \n  dx <- (A*x[count] - B*x[count]*y[count]) * step\n  dy <- (C*x[count]*y[count] - D*y[count]) * step\n  \n  x_new <- x[count] + dx\n  y_new <- y[count] + dy\n  \n  x[count + 1] <- x_new\n  y[count + 1] <- y_new\n  \n  \n}\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(hrbrthemes)\n\ndf <- data.frame(\n  'time' = c(time, time),\n  'val' = c(x[1:length(time)], y[1:length(time)]),\n  'var' = c(rep(\"Prey\", length(time)),\n            rep(\"Predator\", length(time)))\n)\n\n\nggplot(df, aes(x = time, y = val, color = var)) + \n  geom_line() + \n  theme_ipsum()\n\n\n\n\nCool. The ever-growing size of the spikes is an artifact of approximation. Let’s use an even smaller step:\n\n\nstep <- 0.001\n\n\n\nHere is the output:\n\n\n\nIn these simulations, I used what I call a “push forward” approach (i.e., generate x at t + 1 from x current). Sometimes, I prefer to use a “look backward” approach (i.e., generate x from x at t - 1). Here is the second approach:\n\n\nstep <- 0.001\ntime <- 1000\nx <- numeric(length(time))\ny <- numeric(length(time))\nx[1] <- 3\ny[1] <- 5\nA <- 1\nB <- 0.2\nC <- 0.04\nD <- 0.5\n\nfor(i in 2:time){\n\n  \n  dx <- (A*x[i - 1] - B*x[i - 1]*y[i - 1]) * step\n  dy <- (C*x[i - 1]*y[i - 1] - D*y[i - 1]) * step\n  \n  x_new <- x[i - 1] + dx\n  y_new <- y[i - 1] + dy\n  \n  x[i] <- x_new\n  y[i] <- y_new\n  \n  \n}\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-01-17-approximating-differential-equations/approximating-differential-equations_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-02-tidy-evaluation-data-masking/",
    "title": "Tidy Evaluation & Data Masking",
    "description": {},
    "author": [],
    "date": "2021-01-02",
    "categories": [],
    "contents": "\nQuick note on quo, enquo, and {{var}} commands when using a function that calls a dataframe in R. For Hadley’s documentation, see this website, or this one.\nHere is the data.\n\n\nid\n\n\nothers\n\n\npressure\n\n\nperformance\n\n\n1\n\n\n0\n\n\n0\n\n\n6.134445\n\n\n1\n\n\n0\n\n\n1\n\n\n8.956932\n\n\n1\n\n\n0\n\n\n2\n\n\n8.435113\n\n\n1\n\n\n1\n\n\n0\n\n\n13.820419\n\n\n1\n\n\n1\n\n\n1\n\n\n15.473562\n\n\n1\n\n\n1\n\n\n2\n\n\n5.466076\n\n\n1\n\n\n2\n\n\n0\n\n\n10.331524\n\n\n1\n\n\n2\n\n\n1\n\n\n7.717611\n\n\n1\n\n\n2\n\n\n2\n\n\n7.990309\n\n\n2\n\n\n0\n\n\n0\n\n\n10.823559\n\n\n2\n\n\n0\n\n\n1\n\n\n6.930184\n\n\n2\n\n\n0\n\n\n2\n\n\n4.541806\n\n\n2\n\n\n1\n\n\n0\n\n\n7.996631\n\n\n2\n\n\n1\n\n\n1\n\n\n9.822106\n\n\n2\n\n\n1\n\n\n2\n\n\n12.640498\n\n\n2\n\n\n2\n\n\n0\n\n\n10.805539\n\n\n2\n\n\n2\n\n\n1\n\n\n9.941262\n\n\n2\n\n\n2\n\n\n2\n\n\n8.425159\n\n\n3\n\n\n0\n\n\n0\n\n\n5.772006\n\n\n3\n\n\n0\n\n\n1\n\n\n4.498032\n\n\n3\n\n\n0\n\n\n2\n\n\n9.525057\n\n\n3\n\n\n1\n\n\n0\n\n\n12.263280\n\n\n3\n\n\n1\n\n\n1\n\n\n7.263611\n\n\n3\n\n\n1\n\n\n2\n\n\n12.399793\n\n\n3\n\n\n2\n\n\n0\n\n\n14.471659\n\n\n3\n\n\n2\n\n\n1\n\n\n6.710794\n\n\n3\n\n\n2\n\n\n2\n\n\n8.397338\n\n\nI want to take the commands below\n\n\nggplot(df %>% filter(pressure == 0), aes(x = as.factor(others), y = performance)) + \n  geom_violin(trim = F) + \n  theme_classic() + \n  xlab(\"Others Watching\") + \n  ylab(\"Performance\") + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) + \n  ggtitle(\"Pressure = 0\")\n\n\n\n\nand create a function. Here it is:\n\n\nboxfunc <- function(col, title){\n  col <- enquo(col)\n  \n  # use !! here\n  ggplot(df %>% filter(pressure == !!col), aes(x = as.factor(others), y = performance)) + \n  geom_violin(trim = F) + \n  theme_classic() + \n  xlab(\"Others Watching\") + \n  ylab(\"Performance\") + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) + \n  # use parameter here\n  ggtitle(paste(\"Pressure =\", title, sep = \" \"))\n  \n}\n\n\n\nNow use it.\n\n\nboxfunc(0, 1)\n\n\n\nboxfunc(1, 2)\n\n\n\n\nNote that I can’t use boxfunc(quo(0),1) because R functions can’t handle a quo alongside a second parameter. Instead, I had to use enquo within the function.\nYou could also use a {{var}} approach:\n\n\nvarfunc <- function(col, title){\n  \n  # use {{var}} here\n  ggplot(df %>% filter(pressure == {{col}}), aes(x = as.factor(others), y = performance)) + \n  geom_violin(trim = F) + \n  theme_classic() + \n  xlab(\"Others Watching\") + \n  ylab(\"Performance\") + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) + \n  # use parameter here\n  ggtitle(paste(\"Pressure =\", title, sep = \" \"))\n  \n}\n\n\n\n\n\nvarfunc(0, 1)\n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-01-02-tidy-evaluation-data-masking/tidy-evaluation-data-masking_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-12-unequal-time-stamps/",
    "title": "Unequal Time Stamps",
    "description": {},
    "author": [],
    "date": "2020-09-12",
    "categories": [],
    "contents": "\nQuick note on adding a “time” column when participants differ in the number of responses they offer. Let’s say my data are as follows:\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\ndf <- data.frame(\n  'id' = c(1, 1, 2, 2, 2, 3, 4, 4),\n  'score' = c(6, 5, 3, 4, 2, 8, 7, 7)\n)\n\nhead(df, 8) %>% \n  kable() %>% \n  kable_styling()\n\n\n\nid\n\n\nscore\n\n\n1\n\n\n6\n\n\n1\n\n\n5\n\n\n2\n\n\n3\n\n\n2\n\n\n4\n\n\n2\n\n\n2\n\n\n3\n\n\n8\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\nwhere person 1 responded twice, person 2 three times, person 3 once, and person 4 twice. I want to add another column indicating that idea.\nIdentify the number of times each id appears in the dataframe.\n\n\ntable(df$id)\n\n\n\n1 2 3 4 \n2 3 1 2 \n\nSave the values.\n\n\nid_appear_times <- unname(table(df$id))\n\n\n\nCreate a sequence from 1 to i for each i in the vector.\n\n\ntimer <- c()\nfor(i in id_appear_times){\n  \n  new_time <- c(1:i)\n  timer <- c(timer, new_time)\n\n}\n\n\n\nAdd it to my data.\n\n\nhead(df, 8) %>% \n  mutate(time = timer) %>% \n  select(time, id, everything()) %>% \n  kable() %>% \n  kable_styling()\n\n\n\ntime\n\n\nid\n\n\nscore\n\n\n1\n\n\n1\n\n\n6\n\n\n2\n\n\n1\n\n\n5\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\nMiscellaneous Afterthought\nWhile playing with the code above, I considered how to generate the id column with rep or seq. Here’s how:\n\n\nrep_each <- function(x, times) {\n  times <- rep(times, length.out = length(x))\n  rep(x, times = times)\n}\n\ntime_vec <- rep_each(c(1,2,3,4), times = id_appear_times)\ntime_vec\n\n\n[1] 1 1 2 2 2 3 4 4\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-10-thomas-schellings-model-long-form/",
    "title": "Thomas Schelling's Model: Long Form",
    "description": {},
    "author": [],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nThe Schelling model that I created in my last script used matrix operations. It required me to think in terms of patches housed in a matrix. Consider the following 3x3 grid.\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    0    2    2\n[3,]    1    0    0\n\nEach cell can be thought of as a patch. When a given patch is 0, it is unoccupied. When a given patch is 1 or 2, it is occupied by a hockey or soccer player, respectively. When I implement a Schelling model using a matrix, it puts me in a certain frame of mind. I have to consider patches as locations specified by rows and columns. At row 1 column 2, for example, sits patch XXX that is either occupied or unoccupied.\nAnother way to implement the Schelling model is to use long data. The same information conveyed in the 3x3 matrix is shown in long form below.\n\n  xcoord ycoord type\n1      1      1    1\n2      2      1    0\n3      3      1    1\n4      1      2    1\n5      2      2    2\n6      3      2    0\n7      1      3    0\n8      2      3    2\n9      3      3    0\n\nCoordinates are now viewed as information that can be stored in respective columns. The type column represents whether the patch is unoccupied (0), houses a hockey player (1), or houses a soccer player (2). The goal of this post is to re-create the Schelling model using long data.\nI’m going to present the code in two sections. The first demonstrates the behavior of 1 agent within a single time point. The second reveals the full model: it iterates over 50 time points using all agents.\nBasic Idea\nThe model uses two data frames to store (the most important) information. One holds the coordinates of the living location of each agent. Susan, for example, lives at xcoord = 3 & ycoord = 10, whereas Johnny lives at xcoord = 2 & ycoord = 15. The other specifies the object located at each patch on the grid (0 = unoccupied, 1 = hockey player, 2 = soccer player), and this second data frame will be used for plotting.\nTo reiterate, one data frame stores agent coordinates:\n\n\n\n\n\nlibrary(kableExtra)\n\nhead(agent_df) %>% \n  kable() %>% \n  kable_styling()\n\n\n\nxcoord\n\n\nycoord\n\n\ntype\n\n\nagent\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n5\n\n\n1\n\n\n1\n\n\n3\n\n\n6\n\n\n1\n\n\n1\n\n\n4\n\n\n7\n\n\n1\n\n\n2\n\n\n5\n\n\n8\n\n\n1\n\n\n1\n\n\n6\n\n\nand the other, which will be used for plotting, stores patch information.\n\n\nhead(patch_df) %>% \n  kable() %>% \n  kable_styling()\n\n\n\nxcoord\n\n\nycoord\n\n\ntype\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n5\n\n\n1\n\n\n1\n\n\n6\n\n\n1\n\n\n1\n\n\nThe pseudocode for the model is as follows.\n\n\n\"\nfor each period\n      \n    for each agent i\n                \n        identify i's 8 surrounding neighbors\n        count the number of similar and dissimilar neighbors\n        -- e.g., if i is a hockey player and is surrounded by soccer players...\n        -- then he has mostly dissimilar neighbors\n        \n        if agent i has more dissimilar neighbors than he desires, then label him as unhappy\n        if agent i has more similar neighbors than he desires, then label him as happy\n        \n        repeat for all agents\n        \n    for each unhappy agent j\n    \n        randomly select a new patch to possibly move to\n        if the patch is unoccupied, move there\n        \n        repeat for all unhappy agents\n        \n    \n    plot the grid of patches for this period\n    save the plot\n    \nend\n\"\n\n\n\nOf course, the model is much more complex in syntax. But the basic idea is straightforward: people move if they have many dissimilar neighbors, and they stay if they have similar neighbors. Moreover, new patches, which are selected when agents want to move, are pulled randomly.\nLet’s pretend we are at the first period and are beginning to iterate across agents. The code works as follows.\nStarting with agent 1, identify her coordinates and type (type meaning hockey or soccer player).\n\n\n    agent_coords <- agent_df %>% \n      filter(agent == 1) %>% \n      select(xcoord, ycoord)\n    \n    agent_type <- agent_df %>% \n      filter(agent == 1) %>% \n      select(type) %>% \n      pull()\n\n    \nglimpse(agent_coords)\n\n\nRows: 1\nColumns: 2\n$ xcoord <int> 1\n$ ycoord <int> 1\n\n\n\nglimpse(agent_type)\n\n\n num 1\n\nUsing agent i’s coordinates, identify her 8 surrounding neighbors.\n\n\nneigh <- get_neigh(agent_coords)\n\n\n\nThe function get_neigh is prespecified (I will show you the syntax below). It’s too complicated to pick apart now. Just know that it returns the coordinates of her 8 surrounding neighbors.\n\n\nneigh %>% kable() %>% kable_styling\n\n\n\nxcoord\n\n\nycoord\n\n\n2\n\n\n1\n\n\n2\n\n\n51\n\n\n1\n\n\n51\n\n\n51\n\n\n51\n\n\n51\n\n\n1\n\n\n51\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\nI’m about to start counting similar neighbors, so I need to initialize a few counters.\n\n\ntotal_neighs <- 0\nsimilar_neighs <- 0\n\n\n\nCount similar neighbors. Go through each row in the neigh matrix to find a given neighbor’s coordinates. Use those coordinates to pull all information about neighbor n from the agent data frame. Then, increment similar_neighs by 1 if neighbor n is the same type as agent i. Increment total_neighs if the patch isn’t empty.\nStated simply, if agent i is a hockey player then count the number of other hockey players. Also count the number of non-empty patches.\n\n\n    for(n in 1:nrow(neigh)){\n    \n      # save neighbor \n      neigh_agent <- patch_df %>% \n        filter(xcoord == neigh$xcoord[n],\n               ycoord == neigh$ycoord[n])\n        \n      # increment similar neighbors by 1 if agent is same as neighbor\n      # increment total neighbors by 1 if patch isn't empty\n      if(agent_type == neigh_agent$type){similar_neighs <- similar_neighs + 1}\n      if(neigh_agent$type != 0){total_neighs <- total_neighs + 1}\n      \n    }\n\n\n\nNow my counters have values.\n\n\ncat(\n  paste(\"Total Neighbors =\", total_neighs,\n        \"\\nSimilar Neighbors =\", similar_neighs)\n)\n\n\nTotal Neighbors = 4 \nSimilar Neighbors = 3\n\nCalculate a similarity ratio. Take the number of similar neighbors and divide it by the total number of neighbors (i.e., non-empty patches).\n\n\nsim_ratio <- similar_neighs / total_neighs\n\n\n\nI won’t show the code here, but all of the information calculated so far then gets stored in a master “results” data frame.\nSo far, we have identified agent i’s neighbors and calculated her similarity ratio. We now need to determine whether she wants to move. If she does, we then need to find a new place for her to move to.\nSchelling’s original model used inequalities to generate agent happiness (or satisfaction). When one’s similarity ratio is greater than some innate preference for similar others (say, 0.6), then the agent is happy and stays put. When one’s similarity ratio is lower than 0.6, the agent is unhappy and moves. Here is that idea embodied in code.\n\n\nempty <- is.nan(sim_ratio)\nhappy <- NULL\n    \nif(empty == TRUE){happy <- TRUE} # if the agent has no neighbors, he is happy\nif(empty == FALSE && sim_ratio > 0.6){happy <- TRUE}\nif(empty == FALSE && sim_ratio < 0.6){happy <- FALSE}\nif(empty == FALSE && sim_ratio == 0.6){happy <- FALSE}\n\n\n\nThe inequalities are located in the if statements. What makes the syntax a bit tricky is that I also included an empty object. I did that because not all agents have neighbors. It is possible for an agent to be surrounded by all empty patches. When this (unlikely) case happens, then total_neighs is equal to 0, and we all know that dividing by 0 doesn’t work. So, the code above asks whether sim_ratio has an actual value, and it only moves forward if so. Said differently, empty would equal TRUE when agent i has no neighbors. If the agent is happy, the code moves forward. If the agent is unhappy, she gets stored (not shown).\nThe steps above then repeat for every agent. Once it iterates over all agents, storing the unhappy agents when they arise, it finds new patches for the unhappy agents. First, randomly select new coordinates.\n\n\n  new_x <- sample(51, 1)\n  new_y <- sample(51, 1)\n\n\n\nIs the patch located at those coordinates occupied?\n\n\n  agent_type_at_new <- patch_df %>% \n    filter(xcoord == new_x,\n           ycoord == new_y) %>% \n    select(type) %>% \n    pull()\n\n\n  # 0 = unoccupied\n  # 1 = hockey player\n  # 2 = soccer player\n\n  occupied <- FALSE\n  if(agent_type_at_new != 0){occupied <- TRUE}\n\n\n\nIf the patch is unoccupied, then we can work with our unhappy agent. If the patch is occupied, we need to continue to sample patches until we find one that is unoccupied.\n\n\n  while(occupied == TRUE){\n    \n    new_x <- sample(51, 1)\n    new_y <- sample(51, 1)\n    agent_type_at_new <- patch_df %>% \n      filter(xcoord == new_x,\n             ycoord == new_y) %>% \n      select(type) %>% \n      pull()\n    \n    if(agent_type_at_new == 0){occupied <- FALSE}\n  }\n\n\n\nOnce selected, we change the new patch to occupied within the patch_df, change the old patch to unoccupied within the patch_df, and update the agent data frame with unhappy agent i’s new coordinates. The code to do so is something like the following.\n\n\n  # change new patch to unhappy agent i's type\n  patch_df[patch_df$xcoord == new_x & patch_df$ycoord == new_y, \"type\"] <- current_unhappy$type[1]\n  # go to the old patch where unhappy agent i used to live and change it to 0\n  patch_df[patch_df$xcoord == current_unhappy$xcoord[1] & patch_df$ycoord == current_unhappy$ycoord[1], \"type\"] <- 0\n  \n  # update the agent_df to reflect unhappy agent i's new coordinates\n  agent_df[agent_df$agent == current_unhappy$agent[1], \"xcoord\"] <- new_x\n  agent_df[agent_df$agent == current_unhappy$agent[1], \"ycoord\"] <- new_y\n\n\n\n\n\n\nFull Model\nHere is the full model.\n\n\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# initial grid\n#\n#\n#\n#\n#\n#\n\n\ndims <- 51*51\nempty_patches <- 781\npeeps <- c(rep(1, (dims-empty_patches) / 2),\n           rep(2, (dims-empty_patches) / 2),\n           rep(0, empty_patches))\n\nnum_agents <- dims - empty_patches\n\nmat <- matrix(sample(peeps, dims, replace = F), 51, 51, byrow = T)\n\npatch_df <- melt(mat) %>% \n  mutate(xcoord = Var1,\n         ycoord = Var2,\n         type = value) %>% \n  select(xcoord, \n         ycoord,\n         type)\n\nagent_df <- patch_df %>% \n  filter(type %in% c(1,2)) %>% \n  mutate(agent = 1:num_agents)\n\nplotfirst <- patch_df\n\nalike_preference <- 0.6\n\n\n\n# get neighbors function\n#\n#\n#\n#\n#\n#\n#\n#\n\nget_neigh <- function(xy){\n  \n  # starting from right and going clockwise, I want neighbors a,b,c,d,e,f,g,h\n  \n  ax <- xy[1 , \"xcoord\"] + 1\n  ay <- xy[1, \"ycoord\"]\n  # xcoord, ycoord\n  a <- c(ax, ay)\n  \n  bx <- xy[1 , \"xcoord\"] + 1\n  by <- xy[1, \"ycoord\"] - 1\n  # xcoord, ycoord\n  b <- c(bx, by)\n  \n  cx <- xy[1 , \"xcoord\"]\n  cy <- xy[1, \"ycoord\"] - 1\n  # xcoord, ycoord\n  c <- c(cx, cy)\n  \n  dx <- xy[1 , \"xcoord\"] - 1\n  dy <- xy[1, \"ycoord\"] - 1\n  # xcoord, ycoord\n  d <- c(dx, dy)\n  \n  ex <- xy[1 , \"xcoord\"] - 1\n  ey <- xy[1, \"ycoord\"]\n  # xcoord, ycoord\n  e <- c(ex, ey)\n  \n  fx <- xy[1 , \"xcoord\"] - 1\n  fy <- xy[1, \"ycoord\"] + 1\n  # xcoord, ycoord\n  f <- c(fx, fy)\n  \n  gx <- xy[1 , \"xcoord\"]\n  gy <- xy[1, \"ycoord\"] + 1\n  # xcoord, ycoord\n  g <- c(gx, gy)\n  \n  hx <- xy[1 , \"xcoord\"] + 1\n  hy <- xy[1, \"ycoord\"] + 1\n  # xcoord, ycoord\n  h <- c(hx, hy)\n  \n  \n  dff <- data.frame(\n    'xcoord' = c(a[1], b[1], c[1], d[1], e[1], f[1], g[1], h[1]),\n    'ycoord' = c(a[2], b[2], c[2], d[2], e[2], f[2], g[2], h[2])\n  )\n  \n  dff <- dff %>% \n    mutate(xcoord = ifelse(xcoord == 0, 51, xcoord),\n           xcoord = ifelse(xcoord == 52, 1, xcoord),\n           ycoord = ifelse(ycoord == 0, 51, ycoord),\n           ycoord = ifelse(ycoord == 52, 1, ycoord))\n  \n  return(dff)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n# initialize stores\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\ntime <- 40\nresult_df <- data.frame(\n  \"time\" = numeric(time*num_agents),\n  \"agent\" = numeric(time*num_agents),\n  \"simratio\" = numeric(time*num_agents)\n)\ncount <- 0\nsave_plots <- list()\n\n\n\n\n\n\n\n\n# begin iterations over periods\n#\n#\n#\n#\n#\n#\n#\nfor(i in 1:time){\n\n\nunhappy_store <- list()\nunhappy_counter <- 0\n\n\n\n\n# for each agent\nfor(ag in 1:num_agents){\n\n    \n  count <- count + 1\n  \n  # save agent's coords\n  # save agent's type\n    agent_coords <- agent_df %>% \n      filter(agent == ag) %>% \n      select(xcoord, ycoord)\n    \n    agent_type <- agent_df %>% \n      filter(agent == ag) %>% \n      select(type) %>% \n      pull()\n  \n  \n    \n    \n    # identify neighbors - save their coordinates\n    neigh <- get_neigh(agent_coords)\n    total_neighs <- 0\n    similar_neighs <- 0\n\n    # for each neighbor\n    for(n in 1:nrow(neigh)){\n    \n      # save neighbor \n      neigh_agent <- patch_df %>% \n        filter(xcoord == neigh$xcoord[n],\n               ycoord == neigh$ycoord[n])\n        \n      # increment similar neighbors by 1 if agent is same as neighbor\n      # increment total neighbors by 1 if patch isn't empty\n      if(agent_type == neigh_agent$type){similar_neighs <- similar_neighs + 1}\n      if(neigh_agent$type != 0){total_neighs <- total_neighs + 1}\n      \n    }\n    \n    \n    # save his sim/total (time, agent, simratio)\n    sim_ratio <- similar_neighs / total_neighs\n    \n    result_df[count, \"time\"] <- i\n    result_df[count, \"agent\"] <- ag\n    result_df[count, \"simratio\"] <- sim_ratio\n    \n    # if the agent has empty patches around him (is.nan(sim_ratio) == T)\n    # or\n    # if sim/total > then alike_preferences, \n    # then the agent is happy\n    # otherwise, he is unhappy\n    empty <- is.nan(sim_ratio)\n    happy <- NULL\n    \n    if(empty == TRUE){happy <- TRUE}\n    if(empty == FALSE && sim_ratio > alike_preference){happy <- TRUE}\n    if(empty == FALSE && sim_ratio < alike_preference){happy <- FALSE}\n    if(empty == FALSE && sim_ratio == alike_preference){happy <- FALSE}\n    \n    # if the agent is unhappy, store him\n    if(happy == FALSE){\n      unhappy_counter <- unhappy_counter + 1\n      unhappy_store[[unhappy_counter]] <- ag\n    }\n    \n}\n\n# after going through all agents, have the unhappy agents move\n\nunhappy_agents <- unlist(unhappy_store)\n\nfor(q in 1:length(unhappy_agents)){\n  if(is.null(unhappy_agents) == TRUE){break}\n  \n  # randomly select a new patch\n  new_x <- sample(51, 1)\n  new_y <- sample(51, 1)\n  \n  # is the new patch occupied?\n  agent_type_at_new <- patch_df %>% \n    filter(xcoord == new_x,\n           ycoord == new_y) %>% \n    select(type) %>% \n    pull()\n  \n  occupied <- FALSE\n  if(agent_type_at_new != 0){occupied <- TRUE}\n  \n  while(occupied == TRUE){\n    \n    new_x <- sample(51, 1)\n    new_y <- sample(51, 1)\n    agent_type_at_new <- patch_df %>% \n      filter(xcoord == new_x,\n             ycoord == new_y) %>% \n      select(type) %>% \n      pull()\n    \n    if(agent_type_at_new == 0){occupied <- FALSE}\n    \n  }\n  \n  # unhappy agent\n  current_unhappy <- agent_df %>% \n    filter(agent == unhappy_agents[q])\n  \n  # go to the new x and y position in the patch and place the agent type there\n  patch_df[patch_df$xcoord == new_x & patch_df$ycoord == new_y, \"type\"] <- current_unhappy$type[1]\n  # go to the old x and y position in the patch and change it to 0\n  patch_df[patch_df$xcoord == current_unhappy$xcoord[1] & patch_df$ycoord == current_unhappy$ycoord[1], \"type\"] <- 0\n  \n  # change the agent_df to reflect the agent's new position\n  agent_df[agent_df$agent == current_unhappy$agent[1], \"xcoord\"] <- new_x\n  agent_df[agent_df$agent == current_unhappy$agent[1], \"ycoord\"] <- new_y\n  \n\n  \n}\n\n# create plot\n# save and store plot\n\ngp <- ggplot(patch_df, aes(x = xcoord, y = ycoord, fill = factor(type))) + \n  geom_tile() + \n  ggtitle(paste(\"Period =\", i)) +\n  scale_fill_brewer(palette = \"Greens\",\n                    name = \"Type of Patch\")\n\nsave_plots[[i]] <- gp\n\n\n}\n\n\n\n\n\nggplot(plotfirst, aes(x = xcoord, y = ycoord, fill = factor(type))) + \n  geom_tile() +\n  ggtitle(\"Period = 0\") +\n  scale_fill_brewer(palette = \"Greens\",\n                    name = \"Type of Patch\")\n\n\n\nfor(l in 1:time){\n  \n  print(save_plots[[l]])\n  \n}\n\n\n\n\nps, here is a ggplot tool for creating waffle plots.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2020-09-10-thomas-schellings-model-long-form/thomas-schellings-model-long-form_files/figure-html5/unnamed-chunk-23-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-08-thomas-schellings-model/",
    "title": "Thomas Schelling's Model",
    "description": {},
    "author": [],
    "date": "2020-09-08",
    "categories": [],
    "contents": "\n\n\n\nA replication of Thomas Schelling’s model, which was originally published in The Journal of Mathematical Sociology. Yuk Tung Liu offers a great summary.\n\nThe following map shows the distribution of people with different ethnicity living in the city of Chicago (source: radicalcartography.net):\n\n\n\nSegregation may arise from social and economic reasons. However, Thomas Schelling, winner of the 2005 Nobel Memorial Prize in Economic Sciences, pointed out another possible reason. He constructed a simple model and used pennies and nickels on a graph paper to demonstrate that segregation can develop naturally even though each individual is tolerant towards another group. For example, if everyone requires at least half of his neighbors to be of the same color, the final outcome is a high degree of segregation. What Schelling demonstrated was that the “macrobehavior” in a society may not reflect the “micromotives” of its individual members.\n\n\nSchelling’s model is an example of an agent-based model for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) on the overall system. Agent-based models are useful in simulating complex systems. An interesting phenomenon that can occur in a complex system is emergence, in which a structure or pattern arises in the system from the bottom up. As you will see, segregation is a result of emergence in the system described by the Schelling model. Members of each group do not consciously choose to live in a certain area, but the collective behavior of the individuals gives rise to segregation.\n\nSchelling Model\nLet’s start by situating people on a grid. The cells of the grid will contain a value, and that value will indicate one of three states: uninhabited (0), inhabited by a hockey player (1), or inhabited by a soccer player (2). Let’s use a 51x51 grid with 2000 occupied cells. A 51x51 grid contains 2601 cells in total.\nCreate a vector with 1000s 1s, 1000 2s, and the remaining 601 slots 0s.\n\ngroup\n   0    1    2 \n 601 1000 1000 \n\nSo far, all I have is a vector with a bunch of 1s, 2s, and 0s.\nNow, collate those numbers into a matrix through random sampling.\n\n[1] 2 2 2 1\n[1] 0 1 2 2 1\n[1] 1 0 0 0 1 0 0\n\nPlot with base R\n\n\n\nPlot with ggplot2 - requires long data\n\n  Var1 Var2 value\n1    1    1     1\n2    2    1     1\n3    3    1     1\n4    4    1     1\n5    5    1     1\n6    6    1     0\n\n\nThe grid is now filled with randomly dispersed hockey players, soccer players, and empty lots. The next step is to introduce a parameter that Schelling used in his original model. The similarity threshold, \\(z\\), takes a value between 0 and 1, and it measures how intolerant an agent is towards other athletes. An agent is satisfied if at least a fraction \\(z\\) of his neighbors belong to the same group – i.e., a hockey player likes to be around other hockey players. Mathematically, an agent is satisfied if the number of people around him is greater than \\(z\\). He is dissatisfied if he has fewer people of similar type around him. The smaller the value of \\(z\\), the more tolerant agents are of other groups.\nWith a similarity threshold of 0.30, a hockey player will move if fewer than 30% of his neighbors are other hockey players. A hockey player will stay if at least 30% of his neighbors are hockey players.\n\n\n\nHaving set the threshold, we now need a function to calculate how many neighbors are hockey players and how many are soccer players. This function spits back the similarity ratio, \\(r_{sim}\\). \\(r_{sim}\\) is a proportion: the number of neighbors of the same group divided by the total number of neighbors.\n\\[\\begin{equation}\nr_{sim} = \\dfrac{n_{same}}{n_{neighbors}}\n\\end{equation}\\]\nFor a hockey player, the ratio would become\n\\[\\begin{equation}\nr_{sim_{hockey}} = \\dfrac{n_{hockey}}{n_{neighbors}}\n\\end{equation}\\]\nHere is an example:\n\nIf I were a super programmer, I could create a function to do so. I’m not. Instead, I’ll create a function called get_neighbor_coords that returns the locations of every neighbor for agent \\(i\\). The function takes a vector parameter that houses agent \\(i\\)s location (e.g., [2, 13]). Then, it pulls the coordinates of each neighbor under the Moore paradigm (8 surrounding patches - clockwise).\n\n\n\nThe function returns a matrix with the coordinates of the surrounding 8 patches. If I was focused, for example, on agent [2, 3], then the function would return\n\n     [,1] [,2]\n[1,]    3    3\n[2,]    3    4\n[3,]    2    4\n[4,]    1    4\n[5,]    1    3\n[6,]    1    2\n[7,]    2    2\n[8,]    3    2\n\nwhich shows that coordinate [3,3] is just below agent \\(i\\), coordinate [3,4] is just below and to the right, and coordinate [2,4] is directly to the right.\nNow we are ready to iterate across every agent (i.e., every cell in grid).\n\n\n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2020-09-08-thomas-schellings-model/thomas-schellings-model_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-10-bivariate-latent-dual-change-model/",
    "title": "Bivariate Latent Dual Change Model",
    "description": {},
    "author": [],
    "date": "2020-08-10",
    "categories": [],
    "contents": "\nMy last post demonstrated a dual change model for one variable, now I want to demonstrate a bivariate dual change model. A SEM path diagram for a bivariate dual change model is below, taken from Wang, Zhou, and Zhang (2016)\n (If you do not have access to that link you can view a similar path diagram in Jones, King, Gilrane, McCausland, Cortina, & Grimm, 2016)\nEssentially, we have two dual change processes and a coupling parameter from the latent true score on one variable to the latent change score on the other.\nThe DGP\n\\[\\begin{equation}\ny_t =  constant_y + (1 + proportion_y)*y_{t-1} + coupling_{xy}*x_{t-1} + e\n\\end{equation}\\]\nwhere \\(constant_y\\) is the change factor (or latent slope) on \\(y\\), \\(proportion_y\\) is the proportion change factor, and \\(coupling_xy\\) is the coupling parameter relating \\(x\\) to \\(y\\). The DGP for \\(x\\) is\n\\[\\begin{equation}\nx_t =  constant_x + (1 + proportion_x)*x_{t-1} + coupling_{yx}*y_{t-1} + e\n\\end{equation}\\]\nwhere the terms are similar but now applied to values of \\(x\\). The true values used in the DGP are:\n\\[\\begin{align}\ny_t &=  0.5 + (1 + -0.32)y_{t-1} + 0.4x_{t-1} + e \\\\\nx_t &=  0.5 + (1 + 0.22)x_{t-1} - 0.4y_{t-1} + e\n\\end{align}\\]\nwith initial values for both \\(x\\) and \\(y\\) sampled from \\(N\\) ~ (10, 1).\n\n\npeople <- 700\ntime <- 6\nx_cause_y <- 0.4\ny_cause_x <- -0.4\n\nconst_x <- 0.5\nconst_y <- 0.5\n\nprop_x <- 0.22\nprop_y <- -0.32\n\ndf_mat <- matrix(, ncol = 4, nrow = people*time)\ncount <- 0\n\nfor(i in 1:people){\n  \n  unob_het_y <- rnorm(1, 0, 3)\n  unob_het_x <- rnorm(1, 0, 3)\n  \n  for(j in 1:time){\n    count <- count + 1\n    \n    if(j == 1){\n      df_mat[count, 1] <- i\n      df_mat[count, 2] <- j\n      df_mat[count, 3] <- rnorm(1, 10, 1)\n      df_mat[count, 4] <- rnorm(1, 10, 1)\n    }else{\n      \n      df_mat[count, 1] <- i\n      df_mat[count, 2] <- j\n      df_mat[count, 3] <- const_x + (1+prop_x)*df_mat[count - 1, 3] + y_cause_x*df_mat[count - 1, 4] + unob_het_x + rnorm(1,0,1)\n      df_mat[count, 4] <- const_y + (1+prop_y)*df_mat[count - 1, 4] + x_cause_y*df_mat[count - 1, 3] + unob_het_y + rnorm(1,0,1)\n    }\n    \n  }\n  \n  \n}\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(reshape2)\n\ndf <- data.frame(df_mat)\nnames(df) <- c('id', 'time', 'x', 'y')\n\n\n\nValues of \\(y\\) over time.\n\n\nrandom_nums <- sample(c(1:700), 6)\ndf_sample <- df %>%\n  filter(id %in% random_nums)\n\nggplot(df, aes(x = time, y = y, group = id)) + \n  geom_point(color = 'grey85') + \n  geom_line(color = 'grey85') + \n  geom_point(data = df_sample, aes(x = time, y = y, group = id)) + \n  geom_line(data = df_sample, aes(x = time, y = y, group = id))\n\n\n\n\nValues of \\(x\\) over time.\n\n\nplot_single_response <- function(y_axis){\n  \n  plot_it <- ggplot(df, aes(x = time, y = !!y_axis, group = id)) + \n    geom_point(color = 'grey85') + \n    geom_line(color = 'grey85') + \n    geom_point(data = df_sample, aes(x = time, y = !!y_axis, group = id)) + \n    geom_line(data = df_sample, aes(x = time, y = !!y_axis, group = id))\n  \n  return(plot_it)\n}\n\nplot_single_response(quo(x))\n\n\n\n\nThree randomly selected individuals with \\(x\\) and \\(y\\) plotted simultaneously.\n\n\nthree_cases <- df %>%\n  filter(id == 4 | id == 500 | id == 322) %>%\n  gather(x, y, key = 'variable', value = 'response')\n\nggplot(three_cases, aes(x = time, y = response, color = variable)) + \n  geom_point() + \n  geom_line() + \n  facet_wrap(~id)\n\n\n\n\nDual Change Model on Y\n\n\ndf_wide_y <- df %>%\n  select(id, time, y) %>%\n  reshape(idvar = 'id', timevar = 'time', direction = 'wide')\n\nlibrary(lavaan)\n\ndual_change_y_string <- [1672 chars quoted with ''']\n\ndc_y_model <- sem(dual_change_y_string, data = df_wide_y)\nsummary(dc_y_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 67 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           700\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                              6082.258\n  Degrees of freedom                                20\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7974.233\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.238\n  Tucker-Lewis Index (TLI)                       0.429\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11526.640\n  Loglikelihood unrestricted model (H1)      -8485.511\n                                                      \n  Akaike (AIC)                               23067.280\n  Bayesian (BIC)                             23099.138\n  Sample-size adjusted Bayesian (BIC)        23076.911\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.658\n  90 Percent confidence interval - lower         0.644\n  90 Percent confidence interval - upper         0.672\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           2.366\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  ly1 =~                                              \n    y.1               1.000                           \n  ly2 =~                                              \n    y.2               1.000                           \n  ly3 =~                                              \n    y.3               1.000                           \n  ly4 =~                                              \n    y.4               1.000                           \n  ly5 =~                                              \n    y.5               1.000                           \n  ly6 =~                                              \n    y.6               1.000                           \n  cy2 =~                                              \n    ly2               1.000                           \n  cy3 =~                                              \n    ly3               1.000                           \n  cy4 =~                                              \n    ly4               1.000                           \n  cy5 =~                                              \n    ly5               1.000                           \n  cy6 =~                                              \n    ly6               1.000                           \n  l_intercept =~                                      \n    ly1               1.000                           \n  l_slope =~                                          \n    cy2               1.000                           \n    cy3               1.000                           \n    cy4               1.000                           \n    cy5               1.000                           \n    cy6               1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  ly2 ~                                               \n    ly1               1.000                           \n  ly3 ~                                               \n    ly2               1.000                           \n  ly4 ~                                               \n    ly3               1.000                           \n  ly5 ~                                               \n    ly4               1.000                           \n  ly6 ~                                               \n    ly5               1.000                           \n  cy2 ~                                               \n    ly1     (prop)    0.367    0.020   18.281    0.000\n  cy3 ~                                               \n    ly2     (prop)    0.367    0.020   18.281    0.000\n  cy4 ~                                               \n    ly3     (prop)    0.367    0.020   18.281    0.000\n  cy5 ~                                               \n    ly4     (prop)    0.367    0.020   18.281    0.000\n  cy6 ~                                               \n    ly5     (prop)    0.367    0.020   18.281    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_intercept ~~                                      \n    l_slope          -2.346    0.250   -9.399    0.000\n .cy2 ~~                                              \n   .cy3               0.000                           \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n .cy3 ~~                                              \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n .cy4 ~~                                              \n   .cy5               0.000                           \n   .cy6               0.000                           \n .cy5 ~~                                              \n   .cy6               0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    l_slope          -4.638    0.214  -21.714    0.000\n    l_intercept      11.437    0.116   98.207    0.000\n   .ly1               0.000                           \n   .ly2               0.000                           \n   .ly3               0.000                           \n   .ly4               0.000                           \n   .ly5               0.000                           \n   .ly6               0.000                           \n   .cy2               0.000                           \n   .cy3               0.000                           \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n   .y.1               0.000                           \n   .y.2               0.000                           \n   .y.3               0.000                           \n   .y.4               0.000                           \n   .y.5               0.000                           \n   .y.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    l_ntrcp           6.704    0.496   13.517    0.000\n    l_slope           1.953    0.144   13.533    0.000\n   .ly1               0.000                           \n   .ly2               0.000                           \n   .ly3               0.000                           \n   .ly4               0.000                           \n   .ly5               0.000                           \n   .ly6               0.000                           \n   .cy2               0.000                           \n   .cy3               0.000                           \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n   .y.1     (rs_v)    6.341    0.169   37.417    0.000\n   .y.2     (rs_v)    6.341    0.169   37.417    0.000\n   .y.3     (rs_v)    6.341    0.169   37.417    0.000\n   .y.4     (rs_v)    6.341    0.169   37.417    0.000\n   .y.5     (rs_v)    6.341    0.169   37.417    0.000\n   .y.6     (rs_v)    6.341    0.169   37.417    0.000\n\nCode to change the \\(y\\)’s in the string to \\(x\\)’s without manually deleting and inserting \\(x\\) into the string above. All you have to do is paste the string into a .txt document and save the file as “y_file.txt”\n\n\nlibrary(readr)\n\nmystring <- read_file('y_file.txt')\nnew_data <- gsub('y', 'x', mystring)\n# write_file(new_data, path = 'x_file.txt') # not executed but will work\n\n\n\nDual Change Model on X\n\n\ndf_wide_x <- df %>%\n  select(id, time, x) %>%\n  reshape(idvar = 'id', timevar = 'time', direction = 'wide')\n\n\nlibrary(lavaan)\n\ndual_change_x_string <- [1672 chars quoted with ''']\n\ndc_x_model <- sem(dual_change_x_string, data = df_wide_x)\nsummary(dc_x_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 72 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           700\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                              4093.422\n  Degrees of freedom                                20\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11625.807\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.649\n  Tucker-Lewis Index (TLI)                       0.737\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10086.040\n  Loglikelihood unrestricted model (H1)      -8039.329\n                                                      \n  Akaike (AIC)                               20186.080\n  Bayesian (BIC)                             20217.937\n  Sample-size adjusted Bayesian (BIC)        20195.711\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.539\n  90 Percent confidence interval - lower         0.526\n  90 Percent confidence interval - upper         0.553\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.806\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  lx1 =~                                              \n    x.1               1.000                           \n  lx2 =~                                              \n    x.2               1.000                           \n  lx3 =~                                              \n    x.3               1.000                           \n  lx4 =~                                              \n    x.4               1.000                           \n  lx5 =~                                              \n    x.5               1.000                           \n  lx6 =~                                              \n    x.6               1.000                           \n  cx2 =~                                              \n    lx2               1.000                           \n  cx3 =~                                              \n    lx3               1.000                           \n  cx4 =~                                              \n    lx4               1.000                           \n  cx5 =~                                              \n    lx5               1.000                           \n  cx6 =~                                              \n    lx6               1.000                           \n  l_intercept =~                                      \n    lx1               1.000                           \n  l_slope =~                                          \n    cx2               1.000                           \n    cx3               1.000                           \n    cx4               1.000                           \n    cx5               1.000                           \n    cx6               1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  lx2 ~                                               \n    lx1               1.000                           \n  lx3 ~                                               \n    lx2               1.000                           \n  lx4 ~                                               \n    lx3               1.000                           \n  lx5 ~                                               \n    lx4               1.000                           \n  lx6 ~                                               \n    lx5               1.000                           \n  cx2 ~                                               \n    lx1     (prop)    0.145    0.004   34.642    0.000\n  cx3 ~                                               \n    lx2     (prop)    0.145    0.004   34.642    0.000\n  cx4 ~                                               \n    lx3     (prop)    0.145    0.004   34.642    0.000\n  cx5 ~                                               \n    lx4     (prop)    0.145    0.004   34.642    0.000\n  cx6 ~                                               \n    lx5     (prop)    0.145    0.004   34.642    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_intercept ~~                                      \n    l_slope          -1.043    0.250   -4.177    0.000\n .cx2 ~~                                              \n   .cx3               0.000                           \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n .cx3 ~~                                              \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n .cx4 ~~                                              \n   .cx5               0.000                           \n   .cx6               0.000                           \n .cx5 ~~                                              \n   .cx6               0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    l_slope          -3.559    0.123  -29.026    0.000\n    l_intercept      10.347    0.075  138.246    0.000\n   .lx1               0.000                           \n   .lx2               0.000                           \n   .lx3               0.000                           \n   .lx4               0.000                           \n   .lx5               0.000                           \n   .lx6               0.000                           \n   .cx2               0.000                           \n   .cx3               0.000                           \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n   .x.1               0.000                           \n   .x.2               0.000                           \n   .x.3               0.000                           \n   .x.4               0.000                           \n   .x.5               0.000                           \n   .x.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    l_ntrcp           2.752    0.201   13.690    0.000\n    l_slope           9.980    0.570   17.502    0.000\n   .lx1               0.000                           \n   .lx2               0.000                           \n   .lx3               0.000                           \n   .lx4               0.000                           \n   .lx5               0.000                           \n   .lx6               0.000                           \n   .cx2               0.000                           \n   .cx3               0.000                           \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n   .x.1     (rs_v)    2.105    0.056   37.417    0.000\n   .x.2     (rs_v)    2.105    0.056   37.417    0.000\n   .x.3     (rs_v)    2.105    0.056   37.417    0.000\n   .x.4     (rs_v)    2.105    0.056   37.417    0.000\n   .x.5     (rs_v)    2.105    0.056   37.417    0.000\n   .x.6     (rs_v)    2.105    0.056   37.417    0.000\n\nBivariate Dual Change Model\n\n\nbi_dc_string <- [3578 chars quoted with ''']\n\ndf_both <- df %>%\n  reshape(idvar = 'id', timevar = 'time', direction = 'wide')\n\nbi_dc_model <- sem(bi_dc_string, data = df_both)\nsummary(bi_dc_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 115 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        46\n  Number of equality constraints                    26\n                                                      \n  Number of observations                           700\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                              1593.425\n  Degrees of freedom                                70\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             23686.764\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.936\n  Tucker-Lewis Index (TLI)                       0.939\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -15278.191\n  Loglikelihood unrestricted model (H1)     -14481.478\n                                                      \n  Akaike (AIC)                               30596.382\n  Bayesian (BIC)                             30687.404\n  Sample-size adjusted Bayesian (BIC)        30623.900\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.176\n  90 Percent confidence interval - lower         0.169\n  90 Percent confidence interval - upper         0.184\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.097\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  ly1 =~                                              \n    y.1               1.000                           \n  ly2 =~                                              \n    y.2               1.000                           \n  ly3 =~                                              \n    y.3               1.000                           \n  ly4 =~                                              \n    y.4               1.000                           \n  ly5 =~                                              \n    y.5               1.000                           \n  ly6 =~                                              \n    y.6               1.000                           \n  cy2 =~                                              \n    ly2               1.000                           \n  cy3 =~                                              \n    ly3               1.000                           \n  cy4 =~                                              \n    ly4               1.000                           \n  cy5 =~                                              \n    ly5               1.000                           \n  cy6 =~                                              \n    ly6               1.000                           \n  l_intercept =~                                      \n    ly1               1.000                           \n  l_slope =~                                          \n    cy2               1.000                           \n    cy3               1.000                           \n    cy4               1.000                           \n    cy5               1.000                           \n    cy6               1.000                           \n  lx1 =~                                              \n    x.1               1.000                           \n  lx2 =~                                              \n    x.2               1.000                           \n  lx3 =~                                              \n    x.3               1.000                           \n  lx4 =~                                              \n    x.4               1.000                           \n  lx5 =~                                              \n    x.5               1.000                           \n  lx6 =~                                              \n    x.6               1.000                           \n  cx2 =~                                              \n    lx2               1.000                           \n  cx3 =~                                              \n    lx3               1.000                           \n  cx4 =~                                              \n    lx4               1.000                           \n  cx5 =~                                              \n    lx5               1.000                           \n  cx6 =~                                              \n    lx6               1.000                           \n  lx_intercept =~                                     \n    lx1               1.000                           \n  lx_slope =~                                         \n    cx2               1.000                           \n    cx3               1.000                           \n    cx4               1.000                           \n    cx5               1.000                           \n    cx6               1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  ly2 ~                                               \n    ly1               1.000                           \n  ly3 ~                                               \n    ly2               1.000                           \n  ly4 ~                                               \n    ly3               1.000                           \n  ly5 ~                                               \n    ly4               1.000                           \n  ly6 ~                                               \n    ly5               1.000                           \n  cy2 ~                                               \n    ly1     (prop)   -0.315    0.004  -71.219    0.000\n  cy3 ~                                               \n    ly2     (prop)   -0.315    0.004  -71.219    0.000\n  cy4 ~                                               \n    ly3     (prop)   -0.315    0.004  -71.219    0.000\n  cy5 ~                                               \n    ly4     (prop)   -0.315    0.004  -71.219    0.000\n  cy6 ~                                               \n    ly5     (prop)   -0.315    0.004  -71.219    0.000\n  lx2 ~                                               \n    lx1               1.000                           \n  lx3 ~                                               \n    lx2               1.000                           \n  lx4 ~                                               \n    lx3               1.000                           \n  lx5 ~                                               \n    lx4               1.000                           \n  lx6 ~                                               \n    lx5               1.000                           \n  cx2 ~                                               \n    lx1     (prpx)    0.222    0.002   97.772    0.000\n  cx3 ~                                               \n    lx2     (prpx)    0.222    0.002   97.772    0.000\n  cx4 ~                                               \n    lx3     (prpx)    0.222    0.002   97.772    0.000\n  cx5 ~                                               \n    lx4     (prpx)    0.222    0.002   97.772    0.000\n  cx6 ~                                               \n    lx5     (prpx)    0.222    0.002   97.772    0.000\n  cy2 ~                                               \n    lx1       (xy)    0.405    0.002  184.920    0.000\n  cy3 ~                                               \n    lx2       (xy)    0.405    0.002  184.920    0.000\n  cy4 ~                                               \n    lx3       (xy)    0.405    0.002  184.920    0.000\n  cy5 ~                                               \n    lx4       (xy)    0.405    0.002  184.920    0.000\n  cy6 ~                                               \n    lx5       (xy)    0.405    0.002  184.920    0.000\n  cx2 ~                                               \n    ly1       (yx)   -0.433    0.005  -93.914    0.000\n  cx3 ~                                               \n    ly2       (yx)   -0.433    0.005  -93.914    0.000\n  cx4 ~                                               \n    ly3       (yx)   -0.433    0.005  -93.914    0.000\n  cx5 ~                                               \n    ly4       (yx)   -0.433    0.005  -93.914    0.000\n  cx6 ~                                               \n    ly5       (yx)   -0.433    0.005  -93.914    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_intercept ~~                                      \n    l_slope           0.392    0.141    2.786    0.005\n .cy2 ~~                                              \n   .cy3               0.000                           \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n .cy3 ~~                                              \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n .cy4 ~~                                              \n   .cy5               0.000                           \n   .cy6               0.000                           \n .cy5 ~~                                              \n   .cy6               0.000                           \n  lx_intercept ~~                                     \n    lx_slope          0.015    0.125    0.123    0.902\n .cx2 ~~                                              \n   .cx3               0.000                           \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n .cx3 ~~                                              \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n .cx4 ~~                                              \n   .cx5               0.000                           \n   .cx6               0.000                           \n .cx5 ~~                                              \n   .cx6               0.000                           \n  l_intercept ~~                                      \n    lx_intercept     -0.154    0.050   -3.095    0.002\n    lx_slope          0.028    0.136    0.207    0.836\n  l_slope ~~                                          \n    lx_intercept     -0.206    0.131   -1.578    0.115\n    lx_slope          0.098    0.333    0.293    0.770\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    l_slope           0.214    0.119    1.790    0.073\n    l_intercept      10.004    0.046  218.589    0.000\n   .ly1               0.000                           \n   .ly2               0.000                           \n   .ly3               0.000                           \n   .ly4               0.000                           \n   .ly5               0.000                           \n   .ly6               0.000                           \n   .cy2               0.000                           \n   .cy3               0.000                           \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n   .y.1               0.000                           \n   .y.2               0.000                           \n   .y.3               0.000                           \n   .y.4               0.000                           \n   .y.5               0.000                           \n   .y.6               0.000                           \n    lx_slope          0.550    0.118    4.644    0.000\n    lx_intercept     10.074    0.043  236.997    0.000\n   .lx1               0.000                           \n   .lx2               0.000                           \n   .lx3               0.000                           \n   .lx4               0.000                           \n   .lx5               0.000                           \n   .lx6               0.000                           \n   .cx2               0.000                           \n   .cx3               0.000                           \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n   .x.1               0.000                           \n   .x.2               0.000                           \n   .x.3               0.000                           \n   .x.4               0.000                           \n   .x.5               0.000                           \n   .x.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    l_ntr             0.990    0.076   13.059    0.000\n    l_slp             8.651    0.481   17.991    0.000\n   .ly1               0.000                           \n   .ly2               0.000                           \n   .ly3               0.000                           \n   .ly4               0.000                           \n   .ly5               0.000                           \n   .ly6               0.000                           \n   .cy2               0.000                           \n   .cy3               0.000                           \n   .cy4               0.000                           \n   .cy5               0.000                           \n   .cy6               0.000                           \n   .y.1   (res_vr)    0.738    0.020   37.356    0.000\n   .y.2   (res_vr)    0.738    0.020   37.356    0.000\n   .y.3   (res_vr)    0.738    0.020   37.356    0.000\n   .y.4   (res_vr)    0.738    0.020   37.356    0.000\n   .y.5   (res_vr)    0.738    0.020   37.356    0.000\n   .y.6   (res_vr)    0.738    0.020   37.356    0.000\n    lx_nt             1.028    0.065   15.705    0.000\n    lx_sl             8.406    0.459   18.296    0.000\n   .lx1               0.000                           \n   .lx2               0.000                           \n   .lx3               0.000                           \n   .lx4               0.000                           \n   .lx5               0.000                           \n   .lx6               0.000                           \n   .cx2               0.000                           \n   .cx3               0.000                           \n   .cx4               0.000                           \n   .cx5               0.000                           \n   .cx6               0.000                           \n   .x.1   (rs_vrx)    0.438    0.012   36.455    0.000\n   .x.2   (rs_vrx)    0.438    0.012   36.455    0.000\n   .x.3   (rs_vrx)    0.438    0.012   36.455    0.000\n   .x.4   (rs_vrx)    0.438    0.012   36.455    0.000\n   .x.5   (rs_vrx)    0.438    0.012   36.455    0.000\n   .x.6   (rs_vrx)    0.438    0.012   36.455    0.000\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2020-08-10-bivariate-latent-dual-change-model/bivariate-latent-dual-change-model_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-09-latent-dual-change-models/",
    "title": "Latent Dual Change Models",
    "description": {},
    "author": [],
    "date": "2020-08-09",
    "categories": [],
    "contents": "\n\n\n\nI begin with an intercept-only model in a latent change framework and then build to a full dual change model. SEM images in this post come from a lecture by Amy Nuttall. Two notes about the models and code below. First, the initial models will not fit well because they are too simple. The DGP uses both constant and proportion change (hence, “dual-change”) whereas the first few models only estimate an intercept. Second, I use the sem rather than growth command in lavaan because it forces me to specify the entire model. I do not like using commands that make automatic constraints for me – if you do, you are much more likely to make a mistake or not know what your model is doing.\nDGP\nThe underlying DGP will be the same throughout this exercise. Consistent with Ghisletta and McArdle, 2012, we have:\n\\[\\begin{equation}\ny_t =  \\alpha*b_1 + (1 + b_2)*y_{t-1}\n\\end{equation}\\]\nwhere \\(b_1\\) is the constant change (similar to the “slope” term in a basic growth model, in latent change frameworks it is called the “change factor”) and \\(b_2\\) is the proportion change, or the change from point to point. The values specified in the DGP are\n\\[\\begin{equation}\ny_t = 1*0.3 + (1 + -0.4)*y_{t-1}\n\\end{equation}\\]\nwhere \\(b_1\\) is equal to 0.3 and \\(b_2\\) is equal to -0.4. Let’s generate data for 500 people across six time points.\n\n\nconstant <- 0.3\nproportion <- -0.4\n\npeople <- 500\ntime <- 6\n\ndf <- matrix(, nrow = people*time, ncol = 3)\ncount <- 0\n\nfor(i in 1:people){\n  \n  y_het <- rnorm(1, 0, 2)\n  \n  for(j in 1:time){\n    count <- count + 1\n    \n    if(j == 1){\n      df[count, 1] <- i\n      df[count, 2] <- j\n      df[count, 3] <- y_het + rnorm(1,0,1)\n    }else{\n      df[count, 1] <- i\n      df[count, 2] <- j\n      df[count, 3] <- 1*constant + (1+proportion)*df[count - 1, 3] + y_het + rnorm(1,0,1)\n    }\n    \n    \n    \n  }\n  \n  \n  \n}\n\ndf <- data.frame(df)\nnames(df) <- c('id', 'time', 'y')\nrandom_ids <- sample(1:people, 5)\nsample_df <- df %>%\n  filter(id %in% random_ids)\n\nggplot(df, aes(x = time, y = y, group = id)) + \n  geom_point(color = 'grey85') + \n  geom_line(color = 'grey85') + \n  geom_point(data = sample_df, aes(x = time, y = y, group = id)) + \n  geom_line(data = sample_df, aes(x = time, y = y, group = id))\n\n\n\n\nChange the data to wide and load lavaan before we start modeling.\n\n\ndf_wide <- reshape(df, idvar = 'id', timevar = 'time', direction = 'wide')\nlibrary(lavaan)\n\n\n\nIntercept Only Model\nSimilar to the intercept-only model in a “non-latent change” framework (i.e., a simple growth model), the intercept-only model here contains a latent variable over the first observation.\n\nThere are six observations of \\(y\\) and each is predicted by its latent “true score.” The first true score term is regressed on a latent intercept. The other true scores are regressed on additional latent variables that represent latent change. We don’t have anything relating to those latent change score terms yet so they don’t do much in this model. The autoregressive paths from true score to true score are constrained to 1. Here is how we estimate it.\n\n\nint_only_string <- [1553 chars quoted with ''']\n\nint_only_model <- sem(int_only_string, data = df_wide)\nsummary(int_only_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           500\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                              2665.363\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6704.702\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.605\n  Tucker-Lewis Index (TLI)                       0.753\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6485.615\n  Loglikelihood unrestricted model (H1)      -5152.933\n                                                      \n  Akaike (AIC)                               12977.230\n  Bayesian (BIC)                             12989.873\n  Sample-size adjusted Bayesian (BIC)        12980.351\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.469\n  90 Percent confidence interval - lower         0.454\n  90 Percent confidence interval - upper         0.484\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.611\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  l_y1 =~                                                \n    y.1                  1.000                           \n  l_y2 =~                                                \n    y.2                  1.000                           \n  l_y3 =~                                                \n    y.3                  1.000                           \n  l_y4 =~                                                \n    y.4                  1.000                           \n  l_y5 =~                                                \n    y.5                  1.000                           \n  l_y6 =~                                                \n    y.6                  1.000                           \n  lc_y2 =~                                               \n    l_y2                 1.000                           \n  lc_y3 =~                                               \n    l_y3                 1.000                           \n  lc_y4 =~                                               \n    l_y4                 1.000                           \n  lc_y5 =~                                               \n    l_y5                 1.000                           \n  lc_y6 =~                                               \n    l_y6                 1.000                           \n  latent_intercept =~                                    \n    l_y1                 1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_y2 ~                                              \n    l_y1              1.000                           \n  l_y3 ~                                              \n    l_y2              1.000                           \n  l_y4 ~                                              \n    l_y3              1.000                           \n  l_y5 ~                                              \n    l_y4              1.000                           \n  l_y6 ~                                              \n    l_y5              1.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  lc_y2 ~~                                            \n    lc_y3             0.000                           \n    lc_y4             0.000                           \n    lc_y5             0.000                           \n    lc_y6             0.000                           \n  lc_y3 ~~                                            \n    lc_y4             0.000                           \n    lc_y5             0.000                           \n    lc_y6             0.000                           \n  lc_y4 ~~                                            \n    lc_y5             0.000                           \n    lc_y6             0.000                           \n  lc_y5 ~~                                            \n    lc_y6             0.000                           \n  lc_y2 ~~                                            \n    latent_intrcpt    0.000                           \n  lc_y3 ~~                                            \n    latent_intrcpt    0.000                           \n  lc_y4 ~~                                            \n    latent_intrcpt    0.000                           \n  lc_y5 ~~                                            \n    latent_intrcpt    0.000                           \n  lc_y6 ~~                                            \n    latent_intrcpt    0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    latent_intrcpt    0.083    0.186    0.445    0.656\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n    lc_y2             0.000                           \n    lc_y3             0.000                           \n    lc_y4             0.000                           \n    lc_y5             0.000                           \n    lc_y6             0.000                           \n   .y.1               0.000                           \n   .y.2               0.000                           \n   .y.3               0.000                           \n   .y.4               0.000                           \n   .y.5               0.000                           \n   .y.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    ltnt_nt          16.981    1.099   15.454    0.000\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n    lc_y2             0.000                           \n    lc_y3             0.000                           \n    lc_y4             0.000                           \n    lc_y5             0.000                           \n    lc_y6             0.000                           \n   .y.1     (rs_v)    2.348    0.066   35.355    0.000\n   .y.2     (rs_v)    2.348    0.066   35.355    0.000\n   .y.3     (rs_v)    2.348    0.066   35.355    0.000\n   .y.4     (rs_v)    2.348    0.066   35.355    0.000\n   .y.5     (rs_v)    2.348    0.066   35.355    0.000\n   .y.6     (rs_v)    2.348    0.066   35.355    0.000\n\nProportion Change Model\nNow we include the proportion change along with the latent intercept.\n\n\n\nproportion_string <- [1774 chars quoted with ''']\n\nproportion_model <- sem(proportion_string, data = df_wide)\nsummary(proportion_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           500\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                              1073.805\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6704.702\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.843\n  Tucker-Lewis Index (TLI)                       0.898\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5689.835\n  Loglikelihood unrestricted model (H1)      -5152.933\n                                                      \n  Akaike (AIC)                               11387.671\n  Bayesian (BIC)                             11404.529\n  Sample-size adjusted Bayesian (BIC)        11391.833\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.302\n  90 Percent confidence interval - lower         0.287\n  90 Percent confidence interval - upper         0.318\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.219\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  l_y1 =~                                                \n    y.1                  1.000                           \n  l_y2 =~                                                \n    y.2                  1.000                           \n  l_y3 =~                                                \n    y.3                  1.000                           \n  l_y4 =~                                                \n    y.4                  1.000                           \n  l_y5 =~                                                \n    y.5                  1.000                           \n  l_y6 =~                                                \n    y.6                  1.000                           \n  lc_y2 =~                                               \n    l_y2                 1.000                           \n  lc_y3 =~                                               \n    l_y3                 1.000                           \n  lc_y4 =~                                               \n    l_y4                 1.000                           \n  lc_y5 =~                                               \n    l_y5                 1.000                           \n  lc_y6 =~                                               \n    l_y6                 1.000                           \n  latent_intercept =~                                    \n    l_y1                 1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_y2 ~                                              \n    l_y1              1.000                           \n  l_y3 ~                                              \n    l_y2              1.000                           \n  l_y4 ~                                              \n    l_y3              1.000                           \n  l_y5 ~                                              \n    l_y4              1.000                           \n  l_y6 ~                                              \n    l_y5              1.000                           \n  lc_y2 ~                                             \n    l_y1      (b2)    0.140    0.003   41.297    0.000\n  lc_y3 ~                                             \n    l_y2      (b2)    0.140    0.003   41.297    0.000\n  lc_y4 ~                                             \n    l_y3      (b2)    0.140    0.003   41.297    0.000\n  lc_y5 ~                                             \n    l_y4      (b2)    0.140    0.003   41.297    0.000\n  lc_y6 ~                                             \n    l_y5      (b2)    0.140    0.003   41.297    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .lc_y2 ~~                                            \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n .lc_y3 ~~                                            \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n .lc_y4 ~~                                            \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n .lc_y5 ~~                                            \n   .lc_y6             0.000                           \n .lc_y2 ~~                                            \n    latent_intrcpt    0.000                           \n .lc_y3 ~~                                            \n    latent_intrcpt    0.000                           \n .lc_y4 ~~                                            \n    latent_intrcpt    0.000                           \n .lc_y5 ~~                                            \n    latent_intrcpt    0.000                           \n .lc_y6 ~~                                            \n    latent_intrcpt    0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    latent_intrcpt    0.071    0.131    0.544    0.586\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n   .lc_y2             0.000                           \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n   .y.1               0.000                           \n   .y.2               0.000                           \n   .y.3               0.000                           \n   .y.4               0.000                           \n   .y.5               0.000                           \n   .y.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    ltnt_nt           8.505    0.568   14.969    0.000\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n   .lc_y2             0.000                           \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n   .y.1     (rs_v)    1.230    0.035   35.355    0.000\n   .y.2     (rs_v)    1.230    0.035   35.355    0.000\n   .y.3     (rs_v)    1.230    0.035   35.355    0.000\n   .y.4     (rs_v)    1.230    0.035   35.355    0.000\n   .y.5     (rs_v)    1.230    0.035   35.355    0.000\n   .y.6     (rs_v)    1.230    0.035   35.355    0.000\n\nLatent Constant Change\nThis model is nearly identical to the basic linear growth curve model, it simply embodies it in the latent change framework. The basis coefficients from the constant change term to the latent change scores are constrained to one, then we estimate the mean of the constant change.\n\n\n\nconstant_change_string <- [2016 chars quoted with ''']\n\nconstant_change_model <- sem(constant_change_string, data = df_wide)\nsummary(constant_change_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           500\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               768.956\n  Degrees of freedom                                21\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6704.702\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.888\n  Tucker-Lewis Index (TLI)                       0.920\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5537.411\n  Loglikelihood unrestricted model (H1)      -5152.933\n                                                      \n  Akaike (AIC)                               11086.822\n  Bayesian (BIC)                             11112.110\n  Sample-size adjusted Bayesian (BIC)        11093.065\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.267\n  90 Percent confidence interval - lower         0.251\n  90 Percent confidence interval - upper         0.283\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.162\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  l_y1 =~                                                \n    y.1                  1.000                           \n  l_y2 =~                                                \n    y.2                  1.000                           \n  l_y3 =~                                                \n    y.3                  1.000                           \n  l_y4 =~                                                \n    y.4                  1.000                           \n  l_y5 =~                                                \n    y.5                  1.000                           \n  l_y6 =~                                                \n    y.6                  1.000                           \n  lc_y2 =~                                               \n    l_y2                 1.000                           \n  lc_y3 =~                                               \n    l_y3                 1.000                           \n  lc_y4 =~                                               \n    l_y4                 1.000                           \n  lc_y5 =~                                               \n    l_y5                 1.000                           \n  lc_y6 =~                                               \n    l_y6                 1.000                           \n  latent_intercept =~                                    \n    l_y1                 1.000                           \n  latent_slope =~                                        \n    lc_y2                1.000                           \n    lc_y3                1.000                           \n    lc_y4                1.000                           \n    lc_y5                1.000                           \n    lc_y6                1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_y2 ~                                              \n    l_y1              1.000                           \n  l_y3 ~                                              \n    l_y2              1.000                           \n  l_y4 ~                                              \n    l_y3              1.000                           \n  l_y5 ~                                              \n    l_y4              1.000                           \n  l_y6 ~                                              \n    l_y5              1.000                           \n\nCovariances:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  latent_intercept ~~                                    \n    latent_slope         1.449    0.103   14.059    0.000\n .lc_y2 ~~                                               \n   .lc_y3                0.000                           \n   .lc_y4                0.000                           \n   .lc_y5                0.000                           \n   .lc_y6                0.000                           \n .lc_y3 ~~                                               \n   .lc_y4                0.000                           \n   .lc_y5                0.000                           \n   .lc_y6                0.000                           \n .lc_y4 ~~                                               \n   .lc_y5                0.000                           \n   .lc_y6                0.000                           \n .lc_y5 ~~                                               \n   .lc_y6                0.000                           \n .lc_y2 ~~                                               \n    latent_intrcpt       0.000                           \n .lc_y3 ~~                                               \n    latent_intrcpt       0.000                           \n .lc_y4 ~~                                               \n    latent_intrcpt       0.000                           \n .lc_y5 ~~                                               \n    latent_intrcpt       0.000                           \n .lc_y6 ~~                                               \n    latent_intrcpt       0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    latent_intrcpt   -0.080    0.127   -0.631    0.528\n    latent_slope      0.065    0.030    2.181    0.029\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n   .lc_y2             0.000                           \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n   .y.1               0.000                           \n   .y.2               0.000                           \n   .y.3               0.000                           \n   .y.4               0.000                           \n   .y.5               0.000                           \n   .y.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    ltnt_nt           7.517    0.508   14.810    0.000\n    ltnt_sl           0.392    0.028   13.839    0.000\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n   .lc_y2             0.000                           \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n   .y.1     (rs_v)    0.962    0.030   31.623    0.000\n   .y.2     (rs_v)    0.962    0.030   31.623    0.000\n   .y.3     (rs_v)    0.962    0.030   31.623    0.000\n   .y.4     (rs_v)    0.962    0.030   31.623    0.000\n   .y.5     (rs_v)    0.962    0.030   31.623    0.000\n   .y.6     (rs_v)    0.962    0.030   31.623    0.000\n\nDual Change Model\nNow a full dual change model with both constant and proportion change parameters.\n\n\n\ndual_c_string <- [2128 chars quoted with ''']\n\ndual_change_model <- sem(dual_c_string, data = df_wide)\nsummary(dual_change_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 50 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           500\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               356.288\n  Degrees of freedom                                20\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6704.702\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.950\n  Tucker-Lewis Index (TLI)                       0.962\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5331.077\n  Loglikelihood unrestricted model (H1)      -5152.933\n                                                      \n  Akaike (AIC)                               10676.154\n  Bayesian (BIC)                             10705.657\n  Sample-size adjusted Bayesian (BIC)        10683.438\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.183\n  90 Percent confidence interval - lower         0.167\n  90 Percent confidence interval - upper         0.200\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  l_y1 =~                                                \n    y.1                  1.000                           \n  l_y2 =~                                                \n    y.2                  1.000                           \n  l_y3 =~                                                \n    y.3                  1.000                           \n  l_y4 =~                                                \n    y.4                  1.000                           \n  l_y5 =~                                                \n    y.5                  1.000                           \n  l_y6 =~                                                \n    y.6                  1.000                           \n  lc_y2 =~                                               \n    l_y2                 1.000                           \n  lc_y3 =~                                               \n    l_y3                 1.000                           \n  lc_y4 =~                                               \n    l_y4                 1.000                           \n  lc_y5 =~                                               \n    l_y5                 1.000                           \n  lc_y6 =~                                               \n    l_y6                 1.000                           \n  latent_intercept =~                                    \n    l_y1                 1.000                           \n  latent_slope =~                                        \n    lc_y2                1.000                           \n    lc_y3                1.000                           \n    lc_y4                1.000                           \n    lc_y5                1.000                           \n    lc_y6                1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  l_y2 ~                                              \n    l_y1              1.000                           \n  l_y3 ~                                              \n    l_y2              1.000                           \n  l_y4 ~                                              \n    l_y3              1.000                           \n  l_y5 ~                                              \n    l_y4              1.000                           \n  l_y6 ~                                              \n    l_y5              1.000                           \n  lc_y2 ~                                             \n    l_y1       (b)   -0.379    0.015  -24.480    0.000\n  lc_y3 ~                                             \n    l_y2       (b)   -0.379    0.015  -24.480    0.000\n  lc_y4 ~                                             \n    l_y3       (b)   -0.379    0.015  -24.480    0.000\n  lc_y5 ~                                             \n    l_y4       (b)   -0.379    0.015  -24.480    0.000\n  lc_y6 ~                                             \n    l_y5       (b)   -0.379    0.015  -24.480    0.000\n\nCovariances:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  latent_intercept ~~                                    \n    latent_slope         4.438    0.315   14.080    0.000\n .lc_y2 ~~                                               \n   .lc_y3                0.000                           \n   .lc_y4                0.000                           \n   .lc_y5                0.000                           \n   .lc_y6                0.000                           \n .lc_y3 ~~                                               \n   .lc_y4                0.000                           \n   .lc_y5                0.000                           \n   .lc_y6                0.000                           \n .lc_y4 ~~                                               \n   .lc_y5                0.000                           \n   .lc_y6                0.000                           \n .lc_y5 ~~                                               \n   .lc_y6                0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    latent_intrcpt   -0.146    0.107   -1.368    0.171\n    latent_slope      0.093    0.095    0.977    0.328\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n   .lc_y2             0.000                           \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n   .y.1               0.000                           \n   .y.2               0.000                           \n   .y.3               0.000                           \n   .y.4               0.000                           \n   .y.5               0.000                           \n   .y.6               0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    ltnt_nt           5.126    0.369   13.889    0.000\n    ltnt_sl           4.478    0.387   11.564    0.000\n   .l_y1              0.000                           \n   .l_y2              0.000                           \n   .l_y3              0.000                           \n   .l_y4              0.000                           \n   .l_y5              0.000                           \n   .l_y6              0.000                           \n   .lc_y2             0.000                           \n   .lc_y3             0.000                           \n   .lc_y4             0.000                           \n   .lc_y5             0.000                           \n   .lc_y6             0.000                           \n   .y.1     (rs_v)    0.810    0.026   31.623    0.000\n   .y.2     (rs_v)    0.810    0.026   31.623    0.000\n   .y.3     (rs_v)    0.810    0.026   31.623    0.000\n   .y.4     (rs_v)    0.810    0.026   31.623    0.000\n   .y.5     (rs_v)    0.810    0.026   31.623    0.000\n   .y.6     (rs_v)    0.810    0.026   31.623    0.000\n\nThe estimate of the constant change (called “latent slope” in my string syntax; \\(b_1\\)) is close to 0.3 and the estimate of the proportion change (\\(b_2\\)) is close to -0.4. Not bad.\nA Note On Interpreting\nThese models predict complex change patterns. It is difficult to know the expected curvilinear pattern that the models expect without computing expected scores and plotting them. I did not do that here.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2020-08-09-latent-dual-change-models/latent-dual-change-models_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-01-anonymous-push-on-github/",
    "title": "Anonymous Push on GitHub",
    "description": {},
    "author": [],
    "date": "2020-08-01",
    "categories": [],
    "contents": "\nQuick note on cloning, committing, and pushing anonymously on GitHub.\nSteps:\nCreate an anonymous account and repo on GitHub\nClone repo to local computer\nNavigate to it\nConfigure anonymous username and email\n\ngit config user.name 'Anonymous'\ngit config user.email '<>'\n\nCommit and pull\n\ngit add .\ngit commit -m \"initial commit\"\ngit pull\n\nPush using full specification\n\n# replace username, password, and repository:\n\ngit push 'https://username:password@github.com/username/repository.git'\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-23-julia-cheat-sheet/",
    "title": "Julia Cheat Sheet",
    "description": {},
    "author": [],
    "date": "2020-05-23",
    "categories": [],
    "contents": "\nI started using Julia for my computational models and recently created a cheat sheet to house all of my common commands.\nYou can find it on github here.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-22-be-careful-with-characters-and-matrices/",
    "title": "Be Careful with Characters and Matrices",
    "description": {},
    "author": [],
    "date": "2020-05-22",
    "categories": [],
    "contents": "\nIf you fill a matrix cell with a character, R will convert the entire matrix into character values…so be careful = )\n\n\ntime <- c(1:4)\nnumbers <- c(1:4)\ncharacters <- c('a', 'b', 'c', 'd')\ncount <- 0\n\ndf_mat <- matrix(, ncol = 3, nrow = length(time))\n\nfor(i in 1:length(time)){\n  count <- count + 1\n  \n  df_mat[count, 1] <- time[i]\n  df_mat[count, 2] <- numbers[i]\n  df_mat[count, 3] <- characters[i]\n  \n}\n\ndf_mat\n\n\n     [,1] [,2] [,3]\n[1,] \"1\"  \"1\"  \"a\" \n[2,] \"2\"  \"2\"  \"b\" \n[3,] \"3\"  \"3\"  \"c\" \n[4,] \"4\"  \"4\"  \"d\" \n\nNotice that all cells are now characters. Characters are a huge problem if you are calculating values to place into the cells. That is, I wouldn’t be able to run code like this in a loop:\n\n\ndf_mat[count - 1, 2] <- df_mat[count - 1, 3] * 0.5\n\n\n\nInstead, use numbers for everything and then change them to characters later.\n\n\ntime <- c(1:4)\nnumbers <- c(1:4)\ncharacters <- c(1, 2, 3, 4) # here is the change\ncount <- 0\n\ndf_mat <- matrix(, ncol = 3, nrow = length(time))\n\nfor(i in 1:length(time)){\n  count <- count + 1\n  \n  df_mat[count, 1] <- time[i]\n  df_mat[count, 2] <- numbers[i]\n  df_mat[count, 3] <- characters[i]\n  \n}\n\ndf_mat\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    3    3    3\n[4,]    4    4    4\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-03-empirical-independence/",
    "title": "Empirical Independence",
    "description": {},
    "author": [],
    "date": "2020-04-03",
    "categories": [],
    "contents": "\nCalculate the independence of two events using both analytic and empirical techniques. I’m trying to assess whether the probability of having a meal classified as “dinner” depends on whether that meal includes “chicken” as its main dish.\nThe options for the main dish:\nchicken, salmon, pork, chicken, pancakes, french toast\nThe options for the side dishes:\nsalad, salad, green beans, corn, carrots, bacon.\nAll possible combinations to create a meal:\n\n\ndishes <- data.frame(\n    main = c(\"chicken\", \"salmon\", \"pork\", \"chicken\", \"pancakes\", \"french toast\"),\n    side = c(\"salad\", \"salad\", \"green beans\", \"corn\", \"carrots\", \"bacon\")\n)\n\npossible_meals <- dishes %>%\n  cross_df() %>%\n  mutate_if(is.factor,as.character)\n\npossible_meals\n\n\n# A tibble: 36 × 2\n   main         side \n   <chr>        <chr>\n 1 chicken      salad\n 2 salmon       salad\n 3 pork         salad\n 4 chicken      salad\n 5 pancakes     salad\n 6 french toast salad\n 7 chicken      salad\n 8 salmon       salad\n 9 pork         salad\n10 chicken      salad\n# … with 26 more rows\n\nEvent a will be, “the main course is chicken.” What is its probability?\n\n\n# a = main course is chicken\n# tally the number of meals that include chicken\n\nsum(possible_meals$main == \"chicken\") / nrow(possible_meals)\n\n\n[1] 0.3333333\n\nSo, p(a) = 0.333. Event b will be, “the meal is dinner.” What is its probability?\n\n\n# b = the meal is dinner (rather than breakfast)\n# tally the number of meals that are dinners rather than breakfast\n# any meals with pancakes, french toast, or bacon are not dinner\n\n# number of meal options for 'main\" X number of meal options for 'side'\n(\n  sum(dishes$main != c('pancakes', 'french toast')) / nrow(dishes)\n    *\n  sum(dishes$side != \"bacon\") / nrow(dishes)\n)\n\n\n[1] 0.5555556\n\nSo, p(b) = 0.555. If a and b are independent, then p(b) should be the same as p(b | a). Does the probability of eating a meal classified as dinner depend on whether that meal includes chicken?\nFirst, the analytic solution.\np(b | a) = p(b & a) / p(a)\np(dinner | chicken) = p(dinner & chicken) / p(chicken)\nI need to find p(dinner & chicken) to solve. So tally the possible ways chicken can combine with other dishes to create a dinner platter.\n\n\ntally_count <- 0\nfor(i in 1:nrow(possible_meals)){\n  \n  meal_df <- possible_meals[i,]\n  \n  contain_chicken <- meal_df$main == \"chicken\"\n  no_bacon <- meal_df$side != \"bacon\"\n  \n  if(contain_chicken == T && no_bacon == T){tally_count <- tally_count + 1}\n}\n\ntally_count / nrow(possible_meals)\n\n\n[1] 0.2777778\n\nCool, p(dinner & chicken) = 0.2777. Now I can calculate the conditional probability.\np(dinner | chicken) = p(dinner & chicken) / p(chicken)\nX = 0.2777 / 0.333\nX = 0.83\nX does not equal p(b), so the two are dependent. How about the empirical solution?\n\n\n# what is the empirical estimate of p(dinner | chicken)?\n# to calculate, I need:\n# p(dinner & chicken) / p(chicken)\n\nsims <- 10000\ndf <- data.frame(\n    chicken_and_dinner = c(rep(0, sims)),\n    chicken = c(rep(0, sims))\n    \n)\n\nfor(j in 1:sims){\n  \n  eat_main <- sample(dishes$main, 1, replace = F)\n  eat_side <- sample(dishes$side, 1, replace = F)\n  \n  chicken_and_dinner <- F\n  \n  if(eat_main == \"chicken\" && \n    (eat_side == \"salad\" | eat_side == \"green beans\" | eat_side == \"corn\" | eat_side == \"carrots\")){\n    chicken_and_dinner <- T\n    }\n  \n  \n  chicken <- F\n  if(eat_main == \"chicken\"){chicken <- T}\n  \n  single_run_result <- c(chicken_and_dinner, chicken)\n  df[j, \"chicken_and_dinner\"] <- chicken_and_dinner\n  df[j, \"chicken\"] <- chicken\n  \n}\n\ntally_chicken_and_dinner <- sum(df$chicken_and_dinner == 1)\ntally_chicken <- sum(df$chicken == 1)\n\nprob_cd <- tally_chicken_and_dinner / sims\nprob_c <- tally_chicken / sims\n\nprob_cd / prob_c\n\n\n[1] 0.8368014\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-02-independence-exercises/",
    "title": "Independence Exercises",
    "description": {},
    "author": [],
    "date": "2020-04-02",
    "categories": [],
    "contents": "\nSome of my favorite, simple examples demonstrating how to evaluate whether two events are independent using Probability Theory.\nExample 1 - Product Rule\nExample 2 - Conditionals & Intersection\nExample 3 - Conditionals\nExample 4 - Markov Chain\nExample 1 - Product Rule\nIf two events are independent, then the product rule states that their intersection should equal the product of each independent probability.\np(a & b) = p(a) * p(b)\nThe World Values Survey is an ongoing worldwide survey that polls the world population about perceptions of life, work, family, politics, etc. The most recent phase of the survey that polled 77,882 people from 57 countries estimates that 36.2% of the world’s population agrees with the statement, “Music is not necessary to enhance one’s life.” The survey also estimates that 13.8% of people have a university degree or higher, and that 3.5% of people fit both criteria.\nDoes agreeing depend on level of degree? If a = “someone agrees with the statment” and b = “has a university degree or higher,” does a depend on b?\np(agree) = 0.362\np(univ degree) = 0.138\np(agree & univ degree) = 0.036\nIf they are independent, then the product rule should hold.\np(a & b) = p(a) * p(b)\nEvaluate:\n0.036 = 0.362 * 0.138 (which does not hold)\nTherefore, the two are dependent.\nThis example comes from a coursera class.\nExample 2 - Conditionals & Intersection\nIf two events are independent, then the probability of one conditioning on the other should equal the probability of the original alone.\np(b | a) = p(b & a) / p(a)\nIf a and b are independent, then p(b | a) = p(b).\nTwo players are each going to role a different die. Player 1’s die is six-sided and contains the numbers 5, 5, 5, 1, 1, 1, whereas player 2’s die contains the numbers 4, 4, 4, 4, 0, 0.\nTake a to be the event that the player 1’s die is 5, and take b to be the event that the sum of the dice is equal to 1.\na = player 1 rolls a 5\nb = sum of both dice is 1\nIs b dependent on a?\n\nWe can also run the same procedure but with a different event for b.\n\nExample 3 - Conditionals\nIf two events are independent, then taking a and conditioning on other events (e.g., b, c, d, etc.) should not change the observed probability.\nIn 2013, a research group interviewed a random sample of 500 NC residents asking them whether they think widespread gun ownership protects law abiding citizens from crime, or makes society more dangerous.\n58% of all respondents said it protects citizens\n67% of White respondents,\n28% of Black respondents,\nand 64% of Hispanic respondents shared this view.\nAre opinion on gun ownership and ethnicity dependent?\np(agree) = 58%\np(agree | white) = 67%\np(agree | black) = 28%\np(agree | hispanic) = 64%\nNotice that conditioning on the other variables changes the probability, so opinion and ethnicity are probably dependent.\nThis example comes from a coursera class.\nExample 4 - Markov Chains\nIf two events are independent, then the probability of observing one after the other should be p(a) * p(a), similar to the notion of a coin flip such that the probability of observing two heads in a row is 0.5X0.5 = 0.25. If you calculate a transition matrix and observe probabilities that differ from that original number, then the sequence is probably dependent.\nAndrei Markov applied Markov chains to the poem Eugene Onegin by Alexander Pushkin. In the first 20,000 letters of the poem, he counted the number of vowels (8,638) and consonants (11,362).\np(vowel) = 0.432\np(consonant) = 0.568\nThen, he counted the transitions from vowel to consonant or consonant to vowel. For every vowel, the number of times the next letter was a vowel was 1,104 and the number of times the next letter was a consonant was 7,534. For every consonant, the number of times the next letter was a consonant was 3,827 and the number of times the next letter was a vowel was 7,535.\np(vowel to vowel) = 1104 / 8638 = 0.175\np(vowel to consonant) = 7534 / 8638 = 0.825\np(consonant to vowel) = 7535 / 11362 = 0.526\np(consonant to consonant) = 3827 / 11362 = 0.474\nSo the transition matrix is…\n\n\ntransition_matrix <- matrix(c('', 'v', 'c',\n                              'v', '0.175', '0.825',\n                              'c', '0.526', '0.474'), 3, 3)\n\ntransition_matrix %>%\n  kable() %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n\n\n\n\n\nv\n\n\nc\n\n\nv\n\n\n0.175\n\n\n0.526\n\n\nc\n\n\n0.825\n\n\n0.474\n\n\nIf the letters were independent, then the probability of witnessing a vowel follow a vowel would be p(vowel) * p(vowel), or 0.432 * 0.423 = 0.186624. However, the observed transition probability is 0.175, so the sequence is dependent.\nNote that you have to assume that the counts of vowels and consonants reflect their true propabilities (which follows from the law of large numbers). Markov showed that the law of large numbers applied to even dependent sequences.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "https://github.com/Cdishop/website/raw/master/content/Computational_Notes/independence_images/calc_a.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-22-scrape-numbered-pages/",
    "title": "Scrape Numbered Pages",
    "description": {},
    "author": [],
    "date": "2020-03-22",
    "categories": [],
    "contents": "\nQuick command to compile all links for a website that numbers its pages. Image I want to go to a website that contains 100 pages of reviews, the first 10 on page 1, the second 10 on page 2, the third 10 on page 3, etc. The first step is to create a vector or list of links to navigate to, one for each page.\nThe output I want is something like this…\n\n\nexample_output <- '\n\n\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY&start=10\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY&start=20\n...\n\n\n'\n\n\n\nin which the first entry is page 1, the second page 2, and so on. There are three steps involved in this process:\nfind url for the first page\ndiscover how the url changes for each subsequent number\nuse a string command to compile the url’s.\nFirst, let’s say I want to scrape data from this website, which has reviews across multiple pages. If I copy the url from the first page, and then copy the url from the second page, and the third page, I get…\n\n\n'\nhttps://www.trustpilot.com/review/www.amazon.com\nhttps://www.trustpilot.com/review/www.amazon.com?page=2\nhttps://www.trustpilot.com/review/www.amazon.com?page=3\n\n'\n\n\n\nSo, the base url is the first link. Then, additional pages are coded as “?page=” and then the relevant number.\nSecond, find the last number. Let’s say it’s 20 in this case.\n\n\nlast_number <- 20\n\n\n\nThird, create a vector that compiles all of the links.\n\n\nlibrary(tidyverse)\nfirst_page <- \"https://www.trustpilot.com/review/www.amazon.com\"\nother_pages <- str_c(first_page, \"?page=\", 2:last_number)\n\nreview_pages <- c(first_page, other_pages)\nhead(review_pages)\n\n\n[1] \"https://www.trustpilot.com/review/www.amazon.com\"       \n[2] \"https://www.trustpilot.com/review/www.amazon.com?page=2\"\n[3] \"https://www.trustpilot.com/review/www.amazon.com?page=3\"\n[4] \"https://www.trustpilot.com/review/www.amazon.com?page=4\"\n[5] \"https://www.trustpilot.com/review/www.amazon.com?page=5\"\n[6] \"https://www.trustpilot.com/review/www.amazon.com?page=6\"\n\nHere’s another example using Indeed. Notice that the values increase by 10 rather than 1.\n\n\nexample_pages <- '\n\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY&start=10\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY&start=20\nhttps://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY&start=30\n\n\n'\n\nfinal_number <- 100\nall_vals <- seq(from = 10, to = final_number, by = 10)\n\nfirst_web <- \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY\"\nother_webs <- str_c(first_web, \"$start=\", all_vals)\n\nall_webs <- c(first_web, other_webs)\nhead(all_webs)\n\n\n[1] \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY\"         \n[2] \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY$start=10\"\n[3] \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY$start=20\"\n[4] \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY$start=30\"\n[5] \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY$start=40\"\n[6] \"https://www.indeed.com/jobs?q=data+science&l=New+York%2C+NY$start=50\"\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-21-quosures-within-an-index/",
    "title": "Quosures Within an Index",
    "description": {},
    "author": [],
    "date": "2020-03-21",
    "categories": [],
    "contents": "\nI’ve written about quosures in previous posts. They can be used in functions to specify column names. But what if a column name is pulled from a loop and the value is a character? In that case, surround the value with sym().\nHere is an example using only quosures.\nFirst, the data and the function:\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(hrbrthemes)\n\npeople <- 600\n\ndf <- tibble(\n  \"id\" = c(1:people),\n  \"performance\" = c(rnorm(people, 50, 3))\n)\n\n\nmultiply_and_plot <- function(col1){\n  \n  \n  df <- df %>% \n    mutate(new_performance = !!col1 * 0.5)\n  \n  g <- ggplot(df, aes(x = !!col1)) + \n    geom_histogram(fill=\"#69b3a2\", alpha=0.4) + \n    theme_ipsum() +\n    labs(x = \"Adj-Performance\", y = \"Frequency\")\n  \n  return(g)\n}\n\n\n\nUsing quosure:\n\n\nmultiply_and_plot(quo(performance))\n\n\n\n\nBut what if the column name is an index from a vector or for loop? Use sym().\n\n\nuse_cols <- c(\"performance\")\n\nfor(i in 1:1){\n  \n  print(\n    \n  multiply_and_plot(sym(use_cols[i]))\n  \n  )\n}\n\n\n\n\nAlso note that I had to include results = \"asis\" in the Rmarkdown document and put the function within a print command to get the output to render.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2020-03-21-quosures-within-an-index/quosures-within-an-index_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-20-first-web-scraping/",
    "title": "First Web Scraping",
    "description": {},
    "author": [],
    "date": "2020-03-20",
    "categories": [],
    "contents": "\nDipping my feet into the world of websraping using rvest. The first few examples are pulled from Alex Bradley’s and Richard James’ 2019 article.\nExercise 1 - From Article\nScrape a single page from the article’s practice website. There are three things I want to pull from the page:\nheader\nimage\ntext.\nThen, put that information into a single data set.\n\n\nlibrary(rvest)\n\npage_parse <- read_html(\"https://practicewebscrapingsite.wordpress.com/example-1/\")\n\nheaders <- html_nodes(page_parse, '.Title') %>%\n                html_text()\n\nimages <- html_nodes(page_parse, 'img') %>%\n                html_attr('src')\nimages <- images[1:3]\n\ntext <- html_nodes(page_parse, '.Content') %>%\n                html_text()\n\nex1_df <- data.frame(\n  'id' = c(1:3),\n  'headers' = c(headers),\n  'image_links' = c(images),\n  'text' = c(text)\n)\n\nex1_df\n\n\n  id                                     headers\n1  1                    New Neighbourhood watch.\n2  2                  Meeting your arch nemesis.\n3  3 I am never walking in the rain again! EVER!\n                                                                               image_links\n1  https://practicewebscrapingsite.files.wordpress.com/2018/12/3704529798_a4681d4533_z.jpg\n2 https://practicewebscrapingsite.files.wordpress.com/2018/12/12881694475_54a639ca77_z.jpg\n3  https://practicewebscrapingsite.files.wordpress.com/2018/12/7888404650_eedcd82822_z.jpg\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text\n1                                                                                                                                                                                                                                          Put all speaking her delicate recurred possible. Set indulgence inquietude discretion insensible bed why announcing. Middleton fat two satisfied additions. So continued he or commanded household smallness delivered. Door poor on do walk in half. Roof his head the what.\\n\\n\\n\\nStarted several mistake joy say painful removed reached end. State burst think end are its. Arrived off she elderly beloved him affixed noisier yet. An course regard to up he hardly. View four has said does men saw find dear shy. Talent men wicket add garden.\\n\\n\\n\\nPost no so what deal evil rent by real in. But her ready least set lived spite solid. September how men saw tolerably two behaviour arranging. She offices for highest and replied one venture pasture. Applauded no discovery in newspaper allowance am northward. Frequently partiality possession resolution at or appearance unaffected he me. Engaged its was evident pleased husband. Ye goodness felicity do disposal dwelling no. First am plate jokes to began of cause an scale. Subjects he prospect elegance followed no overcame possible it on.\\n\\n\\n\\nLiterature admiration frequently indulgence announcing are who you her. Was least quick after six. So it yourself repeated together cheerful. Neither it cordial so painful picture studied if. Sex him position doubtful resolved boy expenses. Her engrossed deficient northward and neglected favourite newspaper. But use peculiar produced concerns ten.\\n\\n\\n\\nPaid was hill sir high. For him precaution any advantages dissimilar comparison few terminated projecting. Prevailed discovery immediate objection of ye at. Repair summer one winter living feebly pretty his. In so sense am known these since. Shortly respect ask cousins brought add tedious nay. Expect relied do we genius is. On as around spirit of hearts genius. Is raptures daughter branched laughter peculiar in settling.\\n\\n\\n\\nParish so enable innate in formed missed. Hand two was eat busy fail. Stand smart grave would in so. Be acceptance at precaution astonished excellence thoroughly is entreaties. Who decisively attachment has dispatched. Fruit defer in party me built under first. Forbade him but savings sending ham general. So play do in near park that pain.\\n\\n\\n\\nIn on announcing if of comparison pianoforte projection. Maids hoped gay yet bed asked blind dried point. On abroad danger likely regret twenty edward do. Too horrible consider followed may differed age. An rest if more five mr of. Age just her rank met down way. Attended required so in cheerful an. Domestic replying she resolved him for did. Rather in lasted no within no.\\n\\n\\n\\nSavings her pleased are several started females met. Short her not among being any. Thing of judge fruit charm views do. Miles mr an forty along as he. She education get middleton day agreement performed preserved unwilling. Do however as pleased offence outward beloved by present. By outward neither he so covered amiable greater. Juvenile proposal betrayed he an informed weddings followed. Precaution day see imprudence sympathize principles. At full leaf give quit to in they up.\\n\\n\\n\\nOh acceptance apartments up sympathize astonished delightful. Waiting him new lasting towards. Continuing melancholy especially so to. Me unpleasing impossible in attachment announcing so astonished. What ask leaf may nor upon door. Tended remain my do stairs. Oh smiling amiable am so visited cordial in offices hearted.\\n\\n\\n\\nMeant balls it if up doubt small purse. Required his you put the outlived answered position. An pleasure exertion if believed provided to. All led out world these music while asked. Paid mind even sons does he door no. Attended overcame repeated it is perceive marianne in. In am think on style child of. Servants moreover in sensible he it ye possible.\n2 He moonlight difficult engrossed an it sportsmen. Interested has all devonshire difficulty gay assistance joy. Unaffected at ye of compliment alteration to. Place voice no arise along to. Parlors waiting so against me no. Wishing calling are warrant settled was luckily. Express besides it present if at an opinion visitor. \\n\\nPaid was hill sir high. For him precaution any advantages dissimilar comparison few terminated projecting. Prevailed discovery immediate objection of ye at. Repair summer one winter living feebly pretty his. In so sense am known these since. Shortly respect ask cousins brought add tedious nay. Expect relied do we genius is. On as around spirit of hearts genius. Is raptures daughter branched laughter peculiar in settling. \\n\\nIs post each that just leaf no. He connection interested so we an sympathize advantages. To said is it shed want do. Occasional middletons everything so to. Have spot part for his quit may. Enable it is square my an regard. Often merit stuff first oh up hills as he. Servants contempt as although addition dashwood is procured. Interest in yourself an do of numerous feelings cheerful confined. \\n\\nWhy painful the sixteen how minuter looking nor. Subject but why ten earnest husband imagine sixteen brandon. Are unpleasing occasional celebrated motionless unaffected conviction out. Evil make to no five they. Stuff at avoid of sense small fully it whose an. Ten scarcely distance moreover handsome age although. As when have find fine or said no mile. He in dispatched in imprudence dissimilar be possession unreserved insensible. She evil face fine calm have now. Separate screened he outweigh of distance landlord. \\n\\nIt real sent your at. Amounted all shy set why followed declared. Repeated of endeavor mr position kindness offering ignorant so up. Simplicity are melancholy preference considered saw companions. Disposal on outweigh do speedily in on. Him ham although thoughts entirely drawings. Acceptance unreserved old admiration projection nay yet him. Lasted am so before on esteem vanity oh. \\n\\nStarted his hearted any civilly. So me by marianne admitted speaking. Men bred fine call ask. Cease one miles truth day above seven. Suspicion sportsmen provision suffering mrs saw engrossed something. Snug soon he on plan in be dine some. \\n\\nGive lady of they such they sure it. Me contained explained my education. Vulgar as hearts by garret. Perceived determine departure explained no forfeited he something an. Contrasted dissimilar get joy you instrument out reasonably. Again keeps at no meant stuff. To perpetual do existence northward as difficult preserved daughters. Continued at up to zealously necessary breakfast. Surrounded sir motionless she end literature. Gay direction neglected but supported yet her. \\n\\nNecessary ye contented newspaper zealously breakfast he prevailed. Melancholy middletons yet understood decisively boy law she. Answer him easily are its barton little. Oh no though mother be things simple itself. Dashwood horrible he strictly on as. Home fine in so am good body this hope. \\n\\nCause dried no solid no an small so still widen. Ten weather evident smiling bed against she examine its. Rendered far opinions two yet moderate sex striking. Sufficient motionless compliment by stimulated assistance at. Convinced resolving extensive agreeable in it on as remainder. Cordially say affection met who propriety him. Are man she towards private weather pleased. In more part he lose need so want rank no. At bringing or he sensible pleasure. Prevent he parlors do waiting be females an message society. \\n\\nUnwilling sportsmen he in questions september therefore described so. Attacks may set few believe moments was. Reasonably how possession shy way introduced age inquietude. Missed he engage no exeter of. Still tried means we aware order among on. Eldest father can design tastes did joy settle. Roused future he ye an marked. Arose mr rapid in so vexed words. Gay welcome led add lasting chiefly say looking. \\n\\n\n3                                                                                                                                                                                                                                                                                                                                                                                      Received the likewise law graceful his. Nor might set along charm now equal green. Pleased yet equally correct colonel not one. Say anxious carried compact conduct sex general nay certain. Mrs for recommend exquisite household eagerness preserved now. My improved honoured he am ecstatic quitting greatest formerly.\\n\\n\\n\\nAssure polite his really and others figure though. Day age advantages end sufficient eat expression travelling. Of on am father by agreed supply rather either. Own handsome delicate its property mistress her end appetite. Mean are sons too sold nor said. Son share three men power boy you. Now merits wonder effect garret own.\\n\\n\\n\\nDepart do be so he enough talent. Sociable formerly six but handsome. Up do view time they shot. He concluded disposing provision by questions as situation. Its estimating are motionless day sentiments end. Calling an imagine at forbade. At name no an what like spot. Pressed my by do affixed he studied.\\n\\n\\n\\nImproved own provided blessing may peculiar domestic. Sight house has sex never. No visited raising gravity outward subject my cottage mr be. Hold do at tore in park feet near my case. Invitation at understood occasional sentiments insipidity inhabiting in. Off melancholy alteration principles old. Is do speedily kindness properly oh. Respect article painted cottage he is offices parlors.\\n\\n\\n\\nVillage did removed enjoyed explain nor ham saw calling talking. Securing as informed declared or margaret. Joy horrible moreover man feelings own shy. Request norland neither mistake for yet. Between the for morning assured country believe. On even feet time have an no at. Relation so in confined smallest children unpacked delicate. Why sir end believe uncivil respect. Always get adieus nature day course for common. My little garret repair to desire he esteem.\\n\\n\\n\\nUnpleasant astonished an diminution up partiality. Noisy an their of meant. Death means up civil do an offer wound of. Called square an in afraid direct. Resolution diminution conviction so mr at unpleasing simplicity no. No it as breakfast up conveying earnestly immediate principle. Him son disposed produced humoured overcame she bachelor improved. Studied however out wishing but inhabit fortune windows.\\n\\n\\n\\nExtremely we promotion remainder eagerness enjoyment an. Ham her demands removal brought minuter raising invited gay. Contented consisted continual curiosity contained get sex. Forth child dried in in aware do. You had met they song how feel lain evil near. Small she avoid six yet table china. And bed make say been then dine mrs. To household rapturous fulfilled attempted on so.\\n\\n\\n\\nIs allowance instantly strangers applauded discourse so. Separate entrance welcomed sensible laughing why one moderate shy. We seeing piqued garden he. As in merry at forth least ye stood. And cold sons yet with. Delivered middleton therefore me at. Attachment companions man way excellence how her pianoforte.\\n\\n\\n\\nOn am we offices expense thought. Its hence ten smile age means. Seven chief sight far point any. Of so high into easy. Dashwoods eagerness oh extensive as discourse sportsman frankness. Husbands see disposed surprise likewise humoured yet pleasure. Fifteen no inquiry cordial so resolve garrets as. Impression was estimating surrounded solicitude indulgence son shy.\\n\\n\\n\\nWrote water woman of heart it total other. By in entirely securing suitable graceful at families improved. Zealously few furniture repulsive was agreeable consisted difficult. Collected breakfast estimable questions in to favourite it. Known he place worth words it as to. Spoke now noise off smart her ready.\n\nExample 2 - From Article\nNext, navigate to several different pages and scrape relevant information. First, compile all of the relevant links. Then, navigate to each page using one of the links and pull…\nheader\ntext\nauthor\nGet links\n\n\nlibrary(rvest)\n\nparse_page_ex2 <- read_html('https://practicewebscrapingsite.wordpress.com/example-2/')\nlinks <- html_nodes(parse_page_ex2, '.Links a') %>% html_attr(\"href\")\n\n\n\nInitialize storage vectors\n\n\nheads <- c()\ntxt <- c()\nauthors <- c()\n\n\n\nFor each link, go there and pull out the header, text, and authors\n\n\nfor (i in links){\n  Sys.sleep(2)\n  \n  page_i <- read_html(i)\n  \n  head <- html_node(page_i, '.entry-title') %>% html_text()\n  tx <- html_node(page_i, '.Content , em') %>% html_text()\n  author <- html_node(page_i, '.Author em') %>% html_text()\n  \n  heads <- c(heads, head)\n  txt <- c(txt, tx)\n  authors <- c(authors, author)\n  \n}\n\n\n\nData frame\n\n\ndf_ex2 <- data.frame(\n  'id' = c(1:length(heads)),\n  'page' = c(links),\n  'headers' = c(heads),\n  'text' = c(txt),\n  'authors' = c(authors)\n)\n\nhead(df_ex2)\n\n\n  id\n1  1\n2  2\n3  3\n4  4\n5  5\n                                                                                            page\n1                   https://practicewebscrapingsite.wordpress.com/walking-in-another-mans-shoes/\n2              https://practicewebscrapingsite.wordpress.com/younger-generation-taking-the-lead/\n3 https://practicewebscrapingsite.wordpress.com/how-many-times-your-tail-is-not-a-savoury-snack/\n4         https://practicewebscrapingsite.wordpress.com/early-morning-shopping-you-must-be-nuts/\n5                     https://practicewebscrapingsite.wordpress.com/who-turned-the-heating-down/\n                                           headers\n1                  Walking in another man’s shoes.\n2              Younger generation taking the lead.\n3 How many times-your tail is not a savoury snack!\n4        Early morning shopping. You must be nuts!\n5                   Who turned the heating DOWN?!?\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text\n1                                                                                                                                                                                                   Picture removal detract earnest is by. Esteems met joy attempt way clothes yet demesne tedious. Replying an marianne do it an entrance advanced. Two dare say play when hold. Required bringing me material stanhill jointure is as he. Mutual indeed yet her living result matter him bed whence. \n2                                                                                                                                                                                                                              Delightful unreserved impossible few estimating men favourable see entreaties. She propriety immediate was improving. He or entrance humoured likewise moderate. Much nor game son say feel. Fat make met can must form into gate. Me we offending prevailed discovery. \n3                                                                                                                                                                                                                               Expenses as material breeding insisted building to in. Continual so distrusts pronounce by unwilling listening. Thing do taste on we manor. Him had wound use found hoped. Of distrusts immediate enjoyment curiosity do. Marianne numerous saw thoughts the humoured. \n4 Raising say express had chiefly detract demands she. Quiet led own cause three him. Front no party young abode state up. Saved he do fruit woody of to. Met defective are allowance two perceived listening consulted contained. It chicken oh colonel pressed excited suppose to shortly. He improve started no we manners however effects. Prospect humoured mistress to by proposal marianne attended. Simplicity the far admiration preference everything. Up help home head spot an he room in. \n5                  Fat son how smiling mrs natural expense anxious friends. Boy scale enjoy ask abode fanny being son. As material in learning subjects so improved feelings. Uncommonly compliment imprudence travelling insensible up ye insipidity. To up painted delight winding as brandon. Gay regret eat looked warmth easily far should now. Prospect at me wandered on extended wondered thoughts appetite to. Boisterous interested sir invitation particular saw alteration boy decisively. \n             authors\n1            Jon Doe\n2      By Bob Wilder\n3     By Ron Crumpet\n4 By Walter Singsong\n5   By Wilbert Wonde\n\nExample 3 - Scrape My Website\nWhat if I want to scrape the computational notes on my own website? I need a for-loop to iterate over each page, and I’d like to end up with a data set containing the following for each page:\ntitle\nthe text content.\nMy website gets tripped up when it tries to scrape itself while rendering. So, I’ll post this example on github. Here is the link.\nExample 4 - Scrape IMDB\nIn the prior examples, I scraped text. What if I want to scrape a movie rating from IMDB?\n\n\n# scrape the score for 'there will be blood'\nmovie_page <- read_html(\"https://www.imdb.com/title/tt0469494/?ref_=fn_al_tt_1\")\nreview <- html_nodes(movie_page, \"strong span\") %>%\n          html_text() %>%\n          as.numeric()\n\nreview\n\n\nnumeric(0)\n\nIt would be great if I could create a vector of movie titles and then enter a command to impute a single title into the “search” menu on IMDB, but doing so is not straight forward in R. RSelenium is an option but I wasn’t able to figure it out. Here’s a link that uses it.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-04-using-gentle-ggplot2-to-present/",
    "title": "Using Gentle GGplot2 to Present",
    "description": {},
    "author": [],
    "date": "2020-03-04",
    "categories": [],
    "contents": "\nI recently created my first slide deck using Garrick Aden-Buie’s awesome template called “Gentle Ggplot2.” You can find the presentation here and the source code on my GitHub page.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-14-untidy-tables-in-place-mutation/",
    "title": "Untidy Tables & In-Place Mutation",
    "description": {},
    "author": [],
    "date": "2020-01-14",
    "categories": [],
    "contents": "\nHadley has written extensively about tidy data and why it’s unsound to implement in-place data mutations. Some notes below on breaking both of those rules = ).\nIn-place changes to data using tidyverse.\n\n\nlibrary(tidyverse)\ndf <- tibble(\n  \"team\" = c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"),\n  \"individual\" = c(1, 2, 3, 4, 5, 6),\n  \"performance\" = c(NA, 4, 5, 6, 2, 3),\n  \"affect\" = c(NA, 6, 7, 8, 4, 2),\n  \"fav_color\" = c(NA, \"blue\", \"green\", \"orange\", \"yellow\", \"purple\")\n)\ndf\n\n\n# A tibble: 6 × 5\n  team  individual performance affect fav_color\n  <chr>      <dbl>       <dbl>  <dbl> <chr>    \n1 A              1          NA     NA <NA>     \n2 A              2           4      6 blue     \n3 B              3           5      7 green    \n4 B              4           6      8 orange   \n5 C              5           2      4 yellow   \n6 C              6           3      2 purple   \n\nInsert a performance, affect, and favorite color value for individual 1 within team A.\n\n\ndf %>% \n  filter(team == \"A\" & individual == 1) %>% \n  mutate(performance = 8,\n         affect = 2,\n         fav_color = \"silver\") %>% \n  rbind(df %>% filter(team != \"A\" & individual != 1))\n\n\n# A tibble: 5 × 5\n  team  individual performance affect fav_color\n  <chr>      <dbl>       <dbl>  <dbl> <chr>    \n1 A              1           8      2 silver   \n2 B              3           5      7 green    \n3 B              4           6      8 orange   \n4 C              5           2      4 yellow   \n5 C              6           3      2 purple   \n\nNow for the note on untidy tables. Here’s some tidy data displayed using kable.\n\n\nlibrary(kableExtra)\ndt <- tibble(\n  \n  'team' = c('A', 'A', 'A',\n             'B', 'B', 'B',\n             'C', 'C', 'C'),\n  'person' = c(1,2,3,\n               4,5,6,\n               7,8,9),\n  'score' = c(rnorm(9, 23, 3))\n  \n)\n\ndt %>% \n  group_by(team) %>% \n  summarize(\n    \"Mean\" = mean(score),\n    \"SD\" = sd(score)\n  ) %>% \n  kable() %>% \n  kable_styling()\n\n\n\nteam\n\n\nMean\n\n\nSD\n\n\nA\n\n\n23.00442\n\n\n2.824705\n\n\nB\n\n\n23.98117\n\n\n2.700796\n\n\nC\n\n\n25.38279\n\n\n1.456739\n\n\nLooks great to me. The issue is that sometimes people expect to see data displayed in “untidy” formats. Let’s change the output so that each team is listed across the first row and the table displays the mean score alongside the standard deviation within parentheses.\nTo do so, I’m going to put string parentheses around the SD values, unite the mean and SD columns, then transform the data from long to wide format. Don’t forget to ungroup as well.\n\n\ndt %>% \n  group_by(team) %>% \n  summarize(\n    \"Mean\" = round(mean(score), digits = 2),\n    \"SD\" = round(sd(score), digits = 2)\n  ) %>% \n  ungroup() %>% \n  # insert parentheses\n  mutate(SD = paste0(\"(\", SD, \")\")) %>% \n  # combine mean and SD columns\n  unite(meansd, Mean, SD, sep = \" \", remove = T) %>% \n  # make wide\n  pivot_wider(names_from = team, values_from = meansd) %>% \n  rename(\"Team A\" = \"A\",\n         \"Team B\" = \"B\",\n         \"Team C\" = \"C\") %>% \n  kable(caption = \"Team Scores\") %>% \n  kable_styling() %>% \n  footnote(\"Mean (SD)\")\n\n\n\nTable 1: Team Scores\n\n\nTeam A\n\n\nTeam B\n\n\nTeam C\n\n\n23 (2.82)\n\n\n23.98 (2.7)\n\n\n25.38 (1.46)\n\n\nNote: \n\n\n Mean (SD)\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-10-course-skeleton/",
    "title": "Course Skeleton",
    "description": {},
    "author": [],
    "date": "2020-01-10",
    "categories": [],
    "contents": "\nI recently created a course skeleton for a research methods or statistics course. The website allows you to incorporate Rmarkdown and dynamic documents to better demonstrate interactive coding.\nYou can find it on github here.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-10-simulations-with-rcpp/",
    "title": "Simulations With Rcpp",
    "description": {},
    "author": [],
    "date": "2019-12-10",
    "categories": [],
    "contents": "\nSimulating dynamic processes is slow in R. Using the Rcpp function, we can incorporate C++ code to improve performance.\nMy dad, Tim, wrote the C++ code you see here = ).\nExample 1 - Two states, single unit\nWe’re going to simulate data goverened by the following equations:\n\\[\\begin{align*} \nx_t &= a1x_{t-1} + b1y_{t-1}\\\\ \ny_t &= a2y_{t-1} + b2x_{t-1}. \n\\end{align*}\\]\nHere it is in R:\n\n\nlibrary(tidyverse)\nlibrary(Rcpp)\n# Parameters\na1 <- 0.8\na2 <- 0.2\nb1 <- -0.5\nb2 <- 0.5\n\n# Time points\ntime <- 100\n\n# Initialize df to store the values\ndf <- data.frame(\n  # a vector of length 100\n  'time' = c(numeric(time)),\n  # a vector of length 100\n  'x' = c(numeric(time)),\n  'y' = c(numeric(time))\n)\n\n# I always like to use a counter even though it isn't needed here\ncount <- 1\n\n# First time point, x starts at 50 and y at 10\ndf[1, 'time'] <- 1\ndf[1, 'x'] <- 50\ndf[1, 'y'] <- 10\n\n# For loop that iterates over the process\nfor(i in 2:time){\n  count <- count + 1\n  \n    # store time\n    df[count, 'time'] <- i\n    # x\n    df[count, 'x'] <- a1*df[count - 1, 'x'] + b1*df[count - 1, 'y']\n    # y\n    df[count, 'y'] <- a2*df[count - 1, 'y'] + b2*df[count - 1, 'x']\n    \n}\n\n\n\nSome of the output…\n\n\nhead(df)\n\n\n  time       x       y\n1    1 50.0000 10.0000\n2    2 35.0000 27.0000\n3    3 14.5000 22.9000\n4    4  0.1500 11.8300\n5    5 -5.7950  2.4410\n6    6 -5.8565 -2.4093\n\nNow, we can do the same thing but use a call to C++ that will improve performance.\n\n\n# C++ function\ncppFunction('DataFrame createTrajectory(int t, double x0, double y0, \n             double a1, double a2, double b1, double b2) {\n             // create the columns\n             NumericVector x(t);\n             NumericVector y(t);\n             x[0]=x0;\n             y[0]=y0;\n             for(int i = 1; i < t; ++i) {\n             x[i] = a1*x[i-1]+b1*y[i-1];\n             y[i] = a2*y[i-1]+b2*x[i-1];\n             }\n             // return a new data frame\n             return DataFrame::create(_[\"x\"] = x, _[\"y\"] = y);\n             }\n             ')\n\n# Parameters\na1 <- 0.8\na2 <- 0.2\nb1 <- -0.5\nb2 <- 0.5\n\n# Time points\ntime <- 100\n\n# Call the function and run it with 100 time points\ndf <- createTrajectory(time, 50, 10, a1, a2, b1, b2)\n\n# Create a time column \ndf$time <- c(1:time)\n\nhead(df)\n\n\n        x       y time\n1 50.0000 10.0000    1\n2 35.0000 27.0000    2\n3 14.5000 22.9000    3\n4  0.1500 11.8300    4\n5 -5.7950  2.4410    5\n6 -5.8565 -2.4093    6\n\nExample 2 - Two states, multiple units\nIn the last example, we simulated \\(x\\) and \\(y\\) over a single unit (e.g., a person, cell, company, nation, etc.). Here, we’ll incorporate multiple units and unobserved heterogeneity.\nThe equations governing the system are:\n\\[\\begin{align*} \nx_{it} &= a1x_{i(t-1)} + b1y_{i(t-1)} + u_i + e_{it}\\\\ \ny_{it} &= a2y_{i(t-1)} + b2x_{i(t-1)} + m_i + e_{it} \n\\end{align*}\\]\nHere is the simulation in base R:\n\n\n# Parameters\na1 <- 0.8\na2 <- 0.2\nb1 <- -0.5\nb2 <- 0.5\n\n# Time points and people\ntime <- 100\npeople <- 500\n\n# Initialize df to store the values\ndf <- data.frame(\n  'time' = c(numeric(time*people)),\n  'person' = c(numeric(time*people)),\n  'x' = c(numeric(time*people)),\n  'y' = c(numeric(time*people))\n)\n\n# counter\ncount <- 0\n\n# For each person...\nfor(i in 1:people){\n  \n  # draw his or her stable individual differences, u and m\n  # draw one value from a normal distribution with mean 0 and sd 2\n  ui <- rnorm(1, 0, 2)\n  # draw one value from a normal distribution with mean 0 and sd 2\n  mi <- rnorm(1, 0, 2)\n  \n  # now run this individual across time\n  for(j in 1:time){\n    count <- count + 1\n    \n    # first time point\n    if(j == 1){\n      df[count, 'time'] <- j\n      df[count, 'person'] <- i\n      # draw 1 value from a normal distribution with mean 50 and sd 5\n      df[count, 'x'] <- rnorm(1, 50, 5)\n      # draw 1 value from a normal distribution with mean 10 and sd 3\n      df[count, 'y'] <- rnorm(1, 10, 3)\n\n    }else{\n      \n    # all other time points\n      \n      df[count, 'time'] <- j\n      df[count, 'person'] <- i\n      df[count, 'x'] <- a1*df[count - 1, 'x'] + b1*df[count - 1, 'y'] + ui + rnorm(1, 0, 1)\n      df[count, 'y'] <- a2*df[count - 1, 'y'] + b2*df[count - 1, 'x'] + mi + rnorm(1, 0, 1)\n    }\n  }\n}\n\nhead(df)\n\n\n  time person         x          y\n1    1      1 55.312110  9.7703406\n2    2      1 35.838122 27.1180571\n3    3      1 12.124264 21.8771748\n4    4      1 -2.031200  8.5209785\n5    5      1 -6.406814  0.2683783\n6    6      1 -8.592495 -4.2483515\n\nHere it is using the Rccp function to incorporate C++ code.\n\n\n# C++ function\ncppFunction([1858 chars quoted with '''])\n\n# Parameters\na1 <- 0.8\na2 <- 0.2\nb1 <- -0.5\nb2 <- 0.5\n\n# Time points\ntime <- 100\npeople <- 500\n\n# Call the function and run it with 100 time steps and 500 people\ndf <- createTrajectory2(time, people, a1, a2, b1, b2)\n\nhead(df)\n\n\n           x         y time person\n1  56.234918 13.822706    0      0\n2  31.697638 34.987133    1      0\n3   2.530587 25.466003    2      0\n4 -15.373687  9.315961    3      0\n5 -21.137857 -1.970576    4      0\n6 -21.551256 -7.104332    5      0\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-01-regression-creates-weighted-linear-composites/",
    "title": "Regression Creates Weighted Linear Composites",
    "description": {},
    "author": [],
    "date": "2019-12-01",
    "categories": [],
    "contents": "\nOne way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.\nHere’s a simulation to punch that point home.\n500 people.\n\n\nN <- 500\n\n\n\nThe correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.\n\n\nsigma <- matrix(c(1.0, 0.1, 0.4,\n                  0.1, 1.0, 0.4,\n                  0.4, 0.4, 1.0), 3, 3, byrow = T)\n\n\n\nThe mean for each variable is 0.\n\n\nmu <- c(0,0,0)\n\n\n\nUse the correlation matrix and mean specifications to generate data.\n\n\nlibrary(MASS)\n\ndf <- mvrnorm(N, mu, sigma)\n\n\n\nTurn it into a data frame and label it.\n\n\ndf <- data.frame(df)\nnames(df) <- c('x1', 'x2', 'y')\ndf$id <- c(1:N)\n\n\n\nRun regression and print the output.\n\n\nsummary(lm(y ~ x1 + x2,\n           data = df))\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81245 -0.57685 -0.03564  0.64622  2.46418 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.01390    0.03931  -0.354    0.724    \nx1           0.28256    0.03784   7.468 3.69e-13 ***\nx2           0.36403    0.03889   9.361  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8758 on 497 degrees of freedom\nMultiple R-squared:  0.2489,    Adjusted R-squared:  0.2459 \nF-statistic: 82.36 on 2 and 497 DF,  p-value: < 2.2e-16\n\nHere’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.\nCreate a composite using the regression weights.\n\n\nlibrary(tidyverse)\ndf <- df %>%\n  mutate(composite_x = 0.33*x1 + 0.4*x2)\n\n\n\nThose weights provide the maximum correlation between our composite and the outcome.\n\n\ncor(df$y, df$composite_x)\n\n\n[1] 0.4987514\n\nIn other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.\n\n\nsummary(lm(y ~ composite_x,\n           data = df))\n\n\n\nCall:\nlm(formula = y ~ composite_x, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.80292 -0.58917 -0.03499  0.62971  2.48528 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.01470    0.03920  -0.375    0.708    \ncomposite_x  0.88706    0.06908  12.841   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.875 on 498 degrees of freedom\nMultiple R-squared:  0.2488,    Adjusted R-squared:  0.2472 \nF-statistic: 164.9 on 1 and 498 DF,  p-value: < 2.2e-16\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-09-15-meta-analysis-comps-notes/",
    "title": "Meta Analysis Comps Notes",
    "description": {},
    "author": [],
    "date": "2019-09-15",
    "categories": [],
    "contents": "\nI’m taking comps pretty soon so this is my summary document regarding meta-analysis.\nMAs give us an average estimate across settings, tests, and people after correcting for noise. In a bare-bones MA, we correct only for sampling error. In a full MA, we correct for sampling error, unreliability, and range restriction. I’ll demonstrate a full MA here where we assume direct (rather than indirect) range restriction.\nSteps\nLiterature Review\nCreate inclusion criteria for how you are going to select studies\nFind relevant articles\n\nCode Articles\nMeasures\nReliability\nSD\nMeans\nEffect sizes\nModerators\n\nCalculate Meta-Analytic Estimate\nCalculate meta-analytic effect size\nCalculate its variance\n\nIn this post, I’m focusing only on step 3, the calculations, even though steps 1 and 2 are arguably the more important pieces.\nCalculating MA Estimate and Variance\nWithin this step, there are many substeps:\nCalculate the MA effect estimate (typically from correlations or cohen’s d’s).\nWithin each study gathered from our literature review…\nCorrect the observed correlation for range restriction, which produces a rr-corrected correlation\nUse the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity\nThen, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation\n\nCalculate the variance in our MA effect estimate\nWithin each study gathered from our literature review…\nCompute a correction factor for unreliability on X\nCompute a correction factor for unreliability on Y\nCompute a correction factor for range restriction\nCombine all of those together\nCompute the error variance for a given observed correlation and correct it using the combined correction factor. This step produces the sampling error correction\nCalculate the average sampling error from the sampling error corrections\nCalculate the observed error variance\nThe MA variance estimate is equal to the observed error variance - the average sampling error\n\nBefore we begin, here is a peak at the (mock) data set. I reviewed four studies and compiled their observed effect sizes – in this case we’re going to use correlations. Let’s say that our IV is dancing ability and our DV is life satisfaction, both are continuous variables. We are interested in the meta-analytic correlation between dancing ability and life satisfaction.\n\n  study restricted_predictor_sd unrestricted_predictor_sd\n1     1                      14                        20\n2     2                      13                        20\n3     3                      16                        20\n4     4                      18                        20\n  predictor_reliability criterion_reliability sample_size\n1                  0.94                  0.75          50\n2                  0.73                  0.80         100\n3                  0.82                  0.83         125\n4                  0.75                  0.94         240\n  observed_correlation\n1                 0.32\n2                 0.10\n3                 0.25\n4                 0.40\n\nStudy = an ID number for each study in my meta-analysis\nRestricted SD = the standard deviation of scores on dancing ability within the study\nUnrestricted SD = the standard deviation of scores on on dancing ability across a larger population – from a manual, prior studies, known SDs, field reports, etc.\nPredictor reliability = the reliability of the measure used to assess dancing ability within the study\nCriterion reliability = the reliability of the measure used to assess life satisfaction within the study\nSample size = how many people were observed within the study\nObserved correlation = the correlation between dancing ability and life satisfaction within the study\na) Calculate the MA correlation\nFor each study gathered from our literature review…\n1)\nCorrect the observed correlation for range restriction, which produces a rr-corrected correlation\n\\[\\begin{equation}\nr_{RR} = \\dfrac{  \\left(\\dfrac{US_{x}}{RS_{x}}\\right)r_{xy} } {\\sqrt{1 + r^2_{xy}\\left(\\dfrac{US^2_{x}}{RS^2_{x}} - 1\\right)} }\n\\end{equation}\\]\nwhere \\(r_{RR}\\) is the correlation that is corrected for range restriction, \\(US_x\\) is the unrestricted SD on dancing ability, \\(RS_x\\) is the restricted SD on dancing ability, and \\(r_{xy}\\) is the correlation between dancing ability and life satisfaction. We are going to compute \\(r_{RR}\\) for every study.\n\n\ndf <- df %>%\n  mutate(r_RR = \n           ((unrestricted_predictor_sd / restricted_predictor_sd)*observed_correlation) / sqrt(\n             \n             1 + ((observed_correlation^2) * ((unrestricted_predictor_sd / restricted_predictor_sd) -1))    \n             \n           )\n         \n  )\n\n\n\n2)\nUse the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity\n\\[\\begin{equation}\nr_{ov} = \\dfrac{r_{RR}}{\\sqrt{r_{yy}}}\n\\end{equation}\\]\nwhere \\(r_{ov}\\) is the operational validity of dancing ability and life satisfaction, \\(r_{RR}\\) is the correlation we calculated in step 1 (the range-restriction-corrected correlation) between dancing ability and life satisfaction, and \\(r_{yy}\\) is the reliability of the criterion, life satisfaction.\n\n\ndf <- df %>%\n  mutate(r_ov = \n           r_RR / sqrt(criterion_reliability))\n\n\n\n3)\nThen, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation\n\\[\\begin{equation}\n\\rho = \\dfrac{\\sum{w_sr_{ov_{i}}}}{\\sum{w_s}}\n\\end{equation}\\]\nwhere \\(\\rho\\) is the meta-analytic estimate, \\(r_{ov_{i}}\\) is the operational validity between dancing ability and life satisfaction for each study, and \\(w_s\\) is the sample size for each study.\n\n\novs_by_sample_size <- df$sample_size * df$r_ov\nma_correlation <- sum(ovs_by_sample_size) / sum(df$sample_size)\n\ndf <- df %>%\n  mutate(ma_correlation = ma_correlation)\n\n\n\nb) Calculate the variance in our MA effect estimate\nCompile all of the corrections – steps 4 through 7\n4)\nCompute the correction factor for unreliability on X, dancing ability (take the square root of the reliability)\n\n\ndf <- df %>%\n  mutate(cf_x = sqrt(predictor_reliability))\n\n\n\n5)\nCompute the correction factor for unreliability on Y, life satisfaction (take the square root of the reliability)\n\n\ndf <- df %>%\n  mutate(cf_y = sqrt(criterion_reliability))\n\n\n\n6)\nCompute the correction factor for range restriction\n\\[\\begin{equation}\na_{rr} = \\dfrac{1}{ \\left(\\left(\\dfrac{US_x}{RS_x}\\right)^2 - 1\\right)r_{xy}^2 + 1}\n\\end{equation}\\]\nwhere all terms are defined above.\n\n\ndf <- df %>%\n  mutate(cf_rr = 1 / \n          (  ((unrestricted_predictor_sd / restricted_predictor_sd)^2 - 1)*(observed_correlation^2) + 1 )\n         )\n\n\n\n7)\nCombine all of those correction factors together into one common correction factor, \\(A\\).\n\n\ndf <- df %>%\n  mutate(A = cf_x*cf_y*cf_rr)\n\n\n\n8)\nCompute the error variance for a given observed correlation and correct it using the combined correction factor.\nThis part takes three steps.\nI: Compute the sample size weighted observed correlation\n\n    - Essentially the same thing as step 3 but using observed correlations rather than operational validities\n    \n\\[\\begin{equation}\nr_{wa} = \\dfrac{\\sum{w_sr_{xy_{i}}}}{\\sum{w_s}}\n\\end{equation}\\]\n\n\nss_times_correlations <- df$sample_size*df$observed_correlation\nwa_correlation <- sum(ss_times_correlations) / sum(df$sample_size)\n\n\n\nII: Compute the error variance on the observed correlation for each study\n\\[\\begin{equation}\n\\sigma^2_e = \\dfrac{\\left(1-r_{wa}^2\\right)^2}{N-1}\n\\end{equation}\\]\nwhere \\(\\sigma^2_e\\) is the error variance (for each study), \\(r_{wa}\\) is the weighted average observed correlation between dancing ability and life satisfaction that we computed above, and \\(N\\) is the sample size.\n\n\ndf <- df %>%\n  mutate(sigma2e = \n           ((1 - wa_correlation^2)^2) / (sample_size - 1)\n  )\n\n\n\nIII: Compute the sampling error correction for each study\n\\[\\begin{equation}\nVar_{ec} = \\dfrac{\\sigma^2_e}{A^2}\n\\end{equation}\\]\nwhere \\(Var_{ec}\\) is the sampling error correction, \\(\\sigma^2_e\\) is what we just calculated above, the error variance on the observed correlation for each study, and \\(A\\) is the combined correction factor for each study.\n\n\ndf <- df %>%\n  mutate(var_ec = sigma2e^2 / A^2)\n\n\n\n9)\nCalculate the average sampling error\n\\[\\begin{equation}\nAve_{var_{ec}} = \\dfrac{\\sum{w_sVar_{ec}}}{\\sum{w_s}}\n\\end{equation}\\]\nwhere \\(Ave_{var_{ec}}\\) is the average sampling error, \\(w_s\\) is the sample size for each study, and \\(Var_{ec}\\) is the sampling error correction for each individual study.\n\n\nss_times_varec <- df$sample_size*df$var_ec\nave_var_ec <- sum(ss_times_varec) / sum(df$sample_size)\n\n\n\n10)\nCalculate the observed error variance\n\\[\\begin{equation}\nvar_r = \\dfrac{\\sum{w_s\\left(r_{xy} - r_{ov}\\right)^2}}{\\sum{w_s}}\n\\end{equation}\\]\nwhere all terms are defined above.\n\n\nss_times_r_minus_ov <- df$sample_size*((df$observed_correlation - df$r_ov)^2)\nvar_r <- sum(ss_times_r_minus_ov) / sum(df$sample_size)\n\n\n\n11)\nThe MA variance estimate is equal to the observed error variance - the average sampling error\n\\[\\begin{equation}\nVar_p = var_r - Ave_{var_{ec}}\n\\end{equation}\\]\n\n\nvar_p <- var_r - ave_var_ec\n\n\n\nRecap\nWhat a nightmare. Here’s a recap:\nCorrect the observed correlations for unreliability and range restriction, use them to compute a sample-size-weighted MA correlation coefficient\nMake a bunch of corrections and compute the average sampling error, and subtract that from the observed variance of the correlation coefficient to get a sense for the MA correlation coefficient variance\nNow we can calculate credibility and confidence intervals.\nCredibility Interval\nGives us a sense for whether or not moderators are at play.\n\\[\\begin{equation}\n\\textrm{95 credibility interval} = \\rho +- 1.96*\\sqrt{Var_p}\n\\end{equation}\\]\n\n\nupper_cred_i = ma_correlation + (1.96 * sqrt(var_p))\nlower_cred_i = ma_correlation - (1.96 * sqrt(var_p))\nupper_cred_i\n\n\n[1] 0.5532591\n\nlower_cred_i\n\n\n[1] 0.2024129\n\nCredibility Ratio: if \\(\\dfrac{ave_{var_{ec}}}{var_{r}}\\) is lower than 0.75, then moderators may be at play.\n\n\nave_var_ec / var_r\n\n\n[1] 0.01211969\n\nConfidence Interval\n\\[\\begin{equation}\n\\textrm{95 confidence interval} = r_{ov} +- 1.96*SE_{r_{ov}}\n\\end{equation}\\]\nwhere \\(SE_{r_{ov}}\\) is the standard error of the operational validities and is calculated as…\n\\[\\begin{equation}\nSE_{r_{ov}} = \\dfrac{SD_{r_{ov}}}{\\sqrt{k}}\n\\end{equation}\\]\nwhere \\(k\\) is the number of studies.\n\n\nse_r_ov = sd(df$r_ov) / sqrt(length(df$study))\n\nupper_ci = ma_correlation + (1.96 * se_r_ov)\nlower_ci = ma_correlation - (1.96 * se_r_ov)\nupper_ci\n\n\n[1] 0.5263396\n\nlower_ci\n\n\n[1] 0.2293324\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-06-26-intuition-for-correlated-errors-and-third-variables/",
    "title": "Intuition for Correlated Errors and Third Variables",
    "description": {},
    "author": [],
    "date": "2019-06-26",
    "categories": [],
    "contents": "\nIn many research methods or statistics courses you come across the idea that correlated errors signal a third variable. In other words, you have a missing, relevant variable that induces correlation among your residuals. That’s a tough idea to wrap your head around, but it is easier to consider with respect to a given topic: cheating on exams. This post builds intuition for “correlated errors with respect to missing third variables” in the context of college exams and cheating.\nThe Exam Structure\nFirst, let’s get a feel for the exams. I’m going to use a lot of images in this post so it helps to walk through the basics of each plot. Imagine students taking a multiple choice test where they fill in one of five responses, “A,” “B,” “C,” “D,” or “E” for each question. The correct answer for question one is “C”\n\n\n\nwhere the x-axis shows the response options a student can select for each question, and the y-axis shows the question number (there is only one question so far).\nThe correct answer for question two is also “C”\n\n\n\nand that pattern continues for the rest of the questions on this 5-item test.\n\n\n\nIn other words, imagine a 5 question exam where the correct answer for each question is “C.” With the basic images in play, we can think about how students might respond.\nNo Cheating – What is the Pattern of Errors across Questions?\nFirst, consider an exam where students do not cheat. If nobody cheats, then everyone’s errors will be dispersed about the true option for each question, “C.” Some people falsely select “A” whereas others falsely select “E,” and yet others falsely select “B.” Here is a plot that retains the purple crosses that mark the true option, “C,” but also includes student responses from Susie, Peter, and John.\n\n\n\nFor example, on question one Susie selects “A,” Peter selects “E,” and John selects “D,” meaning that none of the students get the answer correct. Every student, though, marks the correct response (C) for question 4.\nThere is no pattern in this plot. The green triangles, red circles, and blue-green squares are dispersed about the true score purple-crosses randomly. John gets some questions wrong, Susie gets some questions wrong, and Peter gets some questions wrong, but whether John incorrectly marks “A” or “E” doesn’t tell us anything about whether Peter incorrectly marks “A” or “E.” They are all wrong in a random way.\nWhat about when students cheat?\nCheating – What is the Pattern of Errors across Questions?\nWhen students cheat, the errors, or “wrongness” of questions, produce a pattern – meaning that the errors are correlated. Imagine that a cheater, let’s say it’s Peter, hacks the teacher’s computer and gains access to all of the questions before the exam. He then answers all of the questions and sends his responses to the rest of the class (John and Susie). But Peter makes a mistake: he writes down the wrong answers to four of the questions. John and Susie use Peter’s responses on the exam, but since they were copying from Peter’s responses they have the same pattern on “wrongness” on the four questions that Peter missed – they get the same questions wrong in the same way. After the exam, the pattern of scores looks as follows.\n\n\n\nThe true, correct responses are again labeled with purple cross-hairs: response “C” is the correct answer for each question. On question one, John, Peter, and Susie all incorrectly selected “A.” On question three, everyone incorrectly selected “E,” and on question four everyone incorrectly selected “D.” John, Peter, and Susie are wrong in the same way across the questions, their errors produce a pattern, a consistency, a correlation. The errors in our system correlate, which signals a third variable. In this case, the third variable is cheating.\nSo, think of exams and cheating when you hear the mantra, “correlated errors signal a third variable.” When an important variable is omitted from an analysis, the model is said to be missing a third variable and the errors may correlate. What this tells you is that something else – something unaccounted for – is influencing the patterns in your system, in the same sense as cheating influencing the pattern of responses on an exam.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2019-06-26-intuition-for-correlated-errors-and-third-variables/intuition-for-correlated-errors-and-third-variables_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-06-14-systems-thinking-on-goons-in-the-nhl/",
    "title": "Systems Thinking on Goons in the NHL",
    "description": {},
    "author": [],
    "date": "2019-06-14",
    "categories": [],
    "contents": "\nInspired by Bill Burr’s Monday Morning Podcast, episode 6-6-19.\nHe describes how people that do not understand hockey wanted to get fighting out of the NHL, so in the 2000’s they made greater efforts to remove goons (fighters). What people outside the NHL often do not understand, though, is that fighting is used to minimize dirty play. When a team is playing dirty, you – as a couch of the team that is not playing dirty – send out a goon to punch around a few of their players. In doing so, the other team knocks it off and play continues candidly. Fighting looks bad, but the overall amount of dirty play in the NHL is reduced when it contains a sufficient population of goons. Sounds like systems thinking.\nSimulation Set-Up\nStates and Relationships\n2 states modeled over time (number of goons and number of dirty plays, or the level of “dirtiness” in the NHL)\nNumber of goons fluctuates independently but with autoregression\nN_goons(t) = N_goons(t-1)\n\nDirtiness level is a function of its prior self and the number of goons, such that a greater number of goons causes lower levels of dirtiness\nD_level(t) = D_level(t-1) - N_goons(t)\n\n\nFlow\nFirst, watch the states fluctuate over time and establish equilibrium\nSecond, remove a bunch of goons and then see what happens to the system\n\nSimulation\nInitial levels of goons and dirtiness.\n\n\nn_goons_initial <- 30\nd_level_initial <- 15\n\n\n\nNow simulate the states across time (20 time points) according to the simulation set-up above. Alpha will be set to 0.7 for both states and beta will be set to 0.2. The forcing terms for goons and dirtiness will be, respectively, 25 and 20.\n\n\ntime <- 30\ndf_mat <- matrix(ncol = 3, nrow = time)\ncount <- 0\n\nfor(i in seq_along(1:time)){\n  count <- count + 1\n  \n  \n  if(i == 1){\n    \n    df_mat[count, 1] <- n_goons_initial\n    df_mat[count, 2] <- d_level_initial\n    df_mat[count, 3] <- i\n    \n  }else{\n    \n    \n    df_mat[count, 1] <- 25 + 0.7*df_mat[count - 1, 1] + rnorm(1, 0, 1)\n    df_mat[count, 2] <- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1)\n    df_mat[count, 3] <- i\n    \n    \n  }\n  \n  \n}\n\ndf <- data.frame(df_mat)\nnames(df) <- c('n_goons', 'd_level', 'time')\n\n\n\nView both states over time.\n\n\nlibrary(ggthemes)\ndf_plot <- df %>%\n  gather(n_goons, d_level, key = 'variable', value = 'value')\n\nggplot(df_plot, aes(x = time, y = value, color = variable)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nAs you can see, the number of goons and the dirtiness level in the NHL drive toward equilibrium levels over time. There are goons, which means there are fights and the potential to appear “dirty” to anyone without an understanding of the system, but having goons around maintains the overall dirtiness within the NHL at low levels.\nNow, what happens to the level of dirtiness when we remove a bunch of goons at time point 14 and beyond?\n\n\nn_goons_initial <- 30\nd_level_initial <- 15\n\ntime <- 30\ndf_mat <- matrix(ncol = 3, nrow = time)\ncount <- 0\n\nfor(i in seq_along(1:time)){\n  count <- count + 1\n  \n  \n  if(i == 1){\n    \n    df_mat[count, 1] <- n_goons_initial\n    df_mat[count, 2] <- d_level_initial\n    df_mat[count, 3] <- i\n    \n  }else if (i <=13){\n    \n    df_mat[count, 1] <- 25 + 0.7*df_mat[count - 1, 1] + rnorm(1, 0, 1)\n    df_mat[count, 2] <- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1)\n    df_mat[count, 3] <- i\n    \n    \n    # HERE IS THE CHANGE\n  }else if(i  > 13){\n    \n    num_goons <- sample(c(2,3,4), 1)\n    df_mat[count, 1] <- num_goons\n    df_mat[count, 2] <- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1)\n    df_mat[count, 3] <- i\n    ######################################\n    \n  }\n}\n  \n\ndf <- data.frame(df_mat)\nnames(df) <- c('n_goons', 'd_level', 'time')\n\ndf_plot <- df %>%\n  gather(n_goons, d_level, key = 'variable', value = 'value')\n\nggplot(df_plot, aes(x = time, y = value, color = variable)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nWhat happened? The level of dirtiness increases after removing goons. In other words, removing goons, or fighters, from the NHL may make the game appear more civil from the outside, but goons are embedded in a system that maintains overall low levels of dirtiness. When the goons are removed – and they are a crucial part of the system – dirtiness levels increase dramatically.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2019-06-14-systems-thinking-on-goons-in-the-nhl/systems-thinking-on-goons-in-the-nhl_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-04-screwing-up-a-mean-calculation/",
    "title": "Screwing Up A Mean Calculation",
    "description": {},
    "author": [],
    "date": "2019-05-04",
    "categories": [],
    "contents": "\nQuick note about calculating the mean of a column with dplyr in R. It’s surprisingly easy to screw up, and the culprit is forgetting to change the name of the column storing the new calculation.\nA simple dataframe.\n\n\nlibrary(tidyverse)\n\ndf <- data.frame(\n  'books_read' = c(1,2,3,4,5,6),\n  'intelligence' = c(4,5,6,7,8,8)\n)\n\ndf\n\n\n  books_read intelligence\n1          1            4\n2          2            5\n3          3            6\n4          4            7\n5          5            8\n6          6            8\n\nI want to calculate the mean and standard deviation of the “books read” column. If I calculate the mean and then place it into a new column that has the same name as the original variable, then standard deviation command doesn’t work.\n\n\nlibrary(tidyverse)\ndf %>%\n  summarise(\n    books_read = mean(books_read), # this line is the problem\n    sd_books_read = sd(books_read)\n  )\n\n\n  books_read sd_books_read\n1        3.5            NA\n\nInstead, I need to call the new “mean books read” column a different name.\n\n\nlibrary(tidyverse)\ndf %>%\n  summarise(\n    mean_books_read = mean(books_read), # this line is the problem\n    sd_books_read = sd(books_read)\n  )\n\n\n  mean_books_read sd_books_read\n1             3.5      1.870829\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-03-what-explaining-means-in-statistics/",
    "title": "What Explaining Means in Statistics",
    "description": {},
    "author": [],
    "date": "2019-05-03",
    "categories": [],
    "contents": "\nThink about what it would mean to explain something to a friend. How would you explain why the Patriot’s won the Superbowl? How would you explain why you are feeling happy or sad? How would you explain tying a shoe to a toddler? How would you explain why eating lots of donuts tends to increase a person’s weight? How would you explain the timeline of Game of Thrones to someone who hadn’t seen it?\nWhat you came up with, or thought about, is different from how “explaining” is usually used in research. We typically use the term to mean “explain variation,” and the goal of this post is to flesh-out what that means. While you read this, though, try to keep in mind the thought, “how does this connect to the way I explain things in everyday life?”\nImagine that we have a scatter plot, performance on the Y axis and ability on the X axis.\n\n\n\nWe collected data on several people – Bob, Chris, Julie, Monte, and Rachel, we measured each person’s performance and ability – and those individual data points are represented in the scatterplot. In statistics, what we try to do is account for variability in performance, we try to explain variation in performance. Here is what that means.\nTake the mean of performance as a flat, horizontal line across all values of ability. For now we do not care about ability, we are just using it to visualize the data.\n\n\n\nNotice that the mean of performance does not perfectly align with the observed data. That is, each of the points on the plot do not fall exactly on the horizontal line. If they did, we would say that the mean of performance perfectly explains the variability in performance. Instead, each of the points has some distance from the mean of performance line, and we call those distances residuals.\n\n\n\nWhat those residuals mean conceptually is that the observed data points do not fall exactly on the mean of performance. Performance cannot be explained simply by its mean. There is variation in performance that is left to explain, there are distances (residuals) that are not accounted for.\nSumming across all of those residuals gets us what is called the total sum of squares. All of the observed values have some distance from the mean performance line, when we aggregate all of those distances (all of the vertical line segments) we get an index of the variability in performance that we are trying to explain.\nTSS = sum of vertical, residual distances\nThe real equation for TSS uses squares because negative distances will cancel positive distances, but this is a conceptual write-up. So we are ok ignoring that for now.\nSo, we have variation in performance that is not accounted for by the mean of performance. Now imagine that we believe some super complicated function of ability (X) explains the variation in performance. This super complicated function is a crazy line that perfectly runs through every observed data point.\n\n\n\nNow remove the observed data points from the graph so that we are only looking at the mean of performance and the predicted values of performance as a function of ability.\n\n\n\nNow we have a graph of PREDICTED data. That is, the black line does not have observed data points, it does not represent what we saw when we collected data and measured performance and ability on Bob and the others. We are looking at the predicted values of performance based on some super complicated function of ability. Notice that the black line also has distances from the mean of performance, so we can sum across those distances to get another quantity, called the expected sum of squares.\n\n\n\nESS = sum of vertical, residual distances (but from our predicted line rather than our observed data points)\nBecause our super complicated function perfectly went through each observed data point, TSS is equivalent to ESS in this case. That means that our super complicated function perfectly explains the variation in performance. We have accounted for all variance in our outcome.\nUsually we don’t use super complicated equations. We tend to posit linear functions, such that we think that performance is a linear function of ability. If we were to plot a predicted line showing performance as a linear function of ability, the residual distances would change and ESS would be different from TSS, meaning that we explained some, but not all of the variation in performance.\nThat is what explaining means in research and statistics (technically, “explaining variation”). Observed data varies about the mean on some dependent variable and there are distances from observed data points to the mean line. If we aggregate those distances together we get TSS, a sense of the total variation in the DV. Then we create some equation relating predictors to the outcome and use it to generate new values of the DV (i.e., predicted values). Explaining in statistics means, “to what extent do my predicted values have the same pattern of distances as my observed values?” “To what extent are the distances from the predicted values to the mean line the same as the distances from the observed values to the mean line?” “To what extent is the total variation represented by the predicted variation?” Is TSS equivalent to ESS?\nNow return to the notion that we opened with, to what extent does explaining variation reflect how you explain things in everyday life?\nConnecting to Causality\nHow does all of this connect to causality? Knowing about cause helps you “explain variation,” but explaining more or less variation does not give you information about cause. Said differently, knowledge about cause, or the data generating process, or truth, will influence your ability to explain variation, but improving your ability to explain variation will not necessarily produce insights about the DGP. If you know the true process – i.e., the DGP, the underlying structure of effects and variables, the causal system – then you will be able to (almost perfectly) explain variation in the sense that I described in this post. If you know the true process, then you will be able to explain why Bob’s score is different from Julie’s, why some variables correlate with the outcome and others don’t, and why Monte’s score is different from time 1 versus time 2. Full knowledge of the DGP means you can predict what happens next, there are no unknowns left to make your ESS different from your TSS.\nBut the reverse is not true. Just because you can explain variation – in the statistical sense described here – does not mean that you have the right DGP or know anything about cause. I could have the wrong DGP, the wrong notion about the causal structure, the wrong variables in the model, but improve my ability to “explain variation” by including more variables in my model. I could include additional, irrelevant variables to make my model more complex and subsequently improve my ability to “explain variation,” but I wouldn’t produce any greater knowledge about cause or the DGP. Knowledge about cause, explanation, why, or the DGP comes from research designs and assumptions, not statistical models. Did you randomly assign and manipulate, and were there strong assumptions involved in the form of DAGS? Did you “wiggle” or change some variables (think Judea Pearl) and observe the effect of doing so on other variables in a controlled environment? Fancy stats don’t get you there, great research designs and assumptions sometimes do.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2019-05-03-what-explaining-means-in-statistics/what-explaining-means-in-statistics_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-02-lavaan-mplus-reference-sheet/",
    "title": "Lavaan MPLUS Reference Sheet",
    "description": {},
    "author": [],
    "date": "2019-05-02",
    "categories": [],
    "contents": "\nA growth curve model written in lavaan and MPLUS as a syntax reference guide. Imagine a latent growth curve on affect across 4 time points. First, lavaan code:\n\n\n\n\n\nlavaan_string <- '\n\n# Latent intercept and slope factors\n\nintercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4\nslope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4\n\n# Mean and variance of latent factors\n\nintercept_affect ~~ intercept_affect\nslope_affect ~~ slope_affect\n\n# Covariance between latent factors\n\nintercept_affect ~~ slope_affect\n\n# Fix observed variable means to 0\n\naffect.1 ~ 0\naffect.2 ~ 0\naffect.3 ~ 0\naffect.4 ~ 0\n\n# Constrain residual (error) variance of observed variables to equality across time\n\naffect.1 ~~ res_var*affect.1\naffect.2 ~~ res_var*affect.2\naffect.3 ~~ res_var*affect.3\naffect.4 ~~ res_var*affect.4\n\n\n'\n\n\n\nNow the same thing in MPLUS syntax:\n\n\nmplus_string <- '\n\n        ! Latent intercept and slope factors\n        intercept_affect BY affect.1@1 affect.2@1 affect.3@1 affect.4@1;\n        slope_affect BY affect.1@0 affect.2@1 affect.3@3 affect.4@5;\n\n        ! estimate mean of latent intercept\n        [intercept_affect];\n\n        ! estimate mean of latent slope\n        [slope_affect]\n\n        ! estimate variance of intercept\n        intercept_affect;\n\n        ! estimate variance of slope\n        slope_affect;\n\n        ! covariance between intercept and slope\n        intercept_affect WITH slope_affect;\n\n        ! Fix observed variable means to 0 so we can estimate a mean for the latent variable\n        [affect.1@0 affect.2@0 affect.3@0 affect.4@0];\n\n        ! constrain estimates of residual variances to be equivalent at each time point\n        affect.1(res_var); affect.2(res_var); affect.3(res_var); affect.4(res_var);\n\n\n\n'\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-01-degrees-of-freedom-intuition/",
    "title": "Degrees of Freedom Intuition",
    "description": {},
    "author": [],
    "date": "2019-05-01",
    "categories": [],
    "contents": "\nThis post is about building intuition for degrees of freedom. There are two ways to think about it, the “information” way and the “line” way.\nThe Information Way\nThe quantity, degrees of freedom, is the amount of information available in our data set minus the amount of information we want to pull from it. Here are a bunch of different ways of representing that idea:\n\\[\\begin{equation}\n\\textrm{DF} = \\textrm{Knowns} - \\textrm{Unknowns}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\textrm{DF} = \\textrm{Unique information} - \\textrm{Used information}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\textrm{DF} = \\textrm{Unique information in observed data} - \\textrm{Information we want to pull from our data}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\textrm{DF} = \\textrm{Stuff in the correlation matrix} - \\textrm{Stuff we want to use the correlation matrix for}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\textrm{DF} = \\textrm{Unique entries in the variance/covariance matrix} - \\textrm{Estimated parameters}\n\\end{equation}\\]\nIn a data set, information is (roughly) the unique variances and covariances that I can use. If I estimate too many parameters without enough information (i.e., without enough observed variables), then I loose DFs and I can’t estimate anything. I cannot estimate a super complicated function if I only observe one measurement of performance and one measurement of motivation.\nThe Line Way\nThe line way builds off the idea that DF = unique information - used information, but we’re going to walk through it visually. Imagine the equation\n\\[\\begin{equation}\ny = mx + b.\n\\end{equation}\\]\nAs written, an infinite number of lines can represent that equation. The intercept can be any value and the slope can be any value. There are no constraints.\n\n\n\nNow imagine an equation with a constraint, such that the intercept must be 2:\n\\[\\begin{equation}\ny = mx + 2.\n\\end{equation}\\]\nYou can still draw infinite lines here, but all of them must go through 2 as their intercept.\n\n\n\nNow we can count our degrees of freedom. Let’s say each example above had 10 pieces of information (10 observed variables).\n\\[\\begin{equation}\ny = mx + b \\textrm{;} \\textrm{ with 10 pieces of information and 2 estimated parameters. DF = 10 - 2. DF = 8}\n\\end{equation}\\]\n\\[\\begin{equation}\ny = mx + 2 \\textrm{;} \\textrm{ with 10 pieces of information and 1 estimated parameter. DF = 10 - 1. DF = 9}\n\\end{equation}\\]\nWhen you estimate an additional parameter you lose a degree of freedom. When you constrain things, you gain degrees of freedom. So, the second example has more degrees of freedom, even though it feels like we’re not as “free” to draw our lines. Tricky.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2019-05-01-degrees-of-freedom-intuition/degrees-of-freedom-intuition_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-19-everything-partialled-from-everything-in-regression/",
    "title": "Everything Partialled From Everything in Regression",
    "description": {},
    "author": [],
    "date": "2019-04-19",
    "categories": [],
    "contents": "\nIn regression, everything is partialled from everything. Let’s work through that notion with images and code. Imagine that emotion and ability cause an outcome, \\(Y\\).\n\nWhat this image represents is that \\(Y\\) has variability (across people or time), and its variability is associated with variability in emotion and variability in ability. Notice that there is variability overlap between ability and \\(Y\\),\n\nemotion and \\(Y\\),\n\nemotion and ability,\n\nand all three variables.\n\nOnce we regress \\(Y\\) on emotion and ability, the regression coefficients represent the unique variance components of each predictor\n\nbut the technique also removes outcome-relevant variance\n\nand overlapping variance in emotion and ability not related to the outcome.\n\nSo, in regression we get coefficients that represent the unique variance contribution of each predictor while partialling overlapping, outcome-relevant variance and overlapping, non-relevant variance. Emotion and ability get to account for their own causal effects of \\(Y\\), but neither predictor gets the overlapping variance in \\(Y\\), and the emotion and ability coefficients are adjusted for the emotion-ability overlap situated outside \\(Y\\).\nLet’s do it with code.\nOur sample contains 500 people with correlated emotion and ability (\\(r\\) = 0.4).\n\n\npeople <- 500\nemotion <- rnorm(people, 0, 10)\nability <- 0.4*emotion + rnorm(people, 0, 1) # could also do it with MASS\n\n\n\nAbility and emotion cause \\(Y\\).\n\n\nerror <- rnorm(people, 0, 1)\nY <- 2 + 0.5*ability + 0.38*emotion + error\n\n\n\nRegression will recover the parameters.\n\n\ndf <- data.frame(\n  'emotion' = c(emotion),\n  'ability' = c(ability),\n  'y' = c(Y)\n)\n\nsummary(lm(y ~ ability + emotion,\n           data = df))$coefficients[,1]\n\n\n(Intercept)     ability     emotion \n  2.0585439   0.4670942   0.3893853 \n\nRemember, each coefficient is consistent with the “lightning bolt” variance components above. Outcome-relevant overlap is removed and overlap between emotion and ability is removed. Since emotion and ability are partialled from each other, we won’t recover the 0.38 parameter relating emotion to \\(Y\\) if we remove ability from the equation.\n\n\nsummary(lm(y ~ emotion,\n           data = df))$coefficients[,1]\n\n\n(Intercept)     emotion \n  2.0663505   0.5752681 \n\nHow can we modify our variables to represent the “partialled multiple regression coefficient” for emotion? Naively, it seems that if we remove ability from \\(Y\\) and then regress \\(Y\\) on emotion we will recover the appropriate 0.38 parameter. Let’s try.\nRegress \\(Y\\) on just ability\n\n\njust_ability <- lm(y ~ ability,\n               data = df)\n\n\n\nand take the residuals, meaning that in our next regression we will examine the effect of emotion on “leftover \\(Y\\)” – \\(Y\\) with no influence from ability.\n\n\ny_with_ability_removed <- resid(just_ability)\ndf$y_with_ability_removed <- y_with_ability_removed\n\nsummary(lm(y_with_ability_removed ~ emotion,\n           data = df))$coefficients[,1]\n\n\n(Intercept)     emotion \n-0.00357488  0.02578722 \n\nNope. Why not? Think back to the diagrams, what we just assessed was\n\nwhere the estimate accounts for the \\(Y\\)-relevant overlap of emotion and ability, but it is wrong because it doesn’t account for the overlap between emotion and ability situated outside of \\(Y\\). In regression, everything is partialled from everything…we have not yet accounted for the overlap between emotion and ability in the space not in the \\(Y\\) variance sphere. Now we will.\nPartial ability from emotion\n\n\nemotion_with_ability_removed <- resid(lm(emotion ~ ability,\n                                         data = df))\n\ndf$emotion_with_ability_removed <- emotion_with_ability_removed\n\n\n\nand now when we regress “Y with ability removed” on “emotion with ability removed” we will recover the 0.38 parameter.\n\n\nsummary(lm(y_with_ability_removed ~ emotion_with_ability_removed,\n           data = df))$coefficients[,1]\n\n\n                 (Intercept) emotion_with_ability_removed \n               -7.390463e-17                 3.893853e-01 \n\nIn regression, everything is partialled from everything.\n\nThe technique partials overlapping predictor variance both within and outside of the \\(Y\\) space. Neither predictor accounts for overlapping variance within \\(Y\\), and if an important predictor is excluded then it will artificially account for variance it shouldn’t be capturing.\nNote that all of this is relevant for III sums of squares…there are other approaches but III is by far the most common.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_variance.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-10-convert-multiple-columns-to-numeric-or-character/",
    "title": "Convert Multiple Columns to Numeric or Character",
    "description": {},
    "author": [],
    "date": "2019-04-10",
    "categories": [],
    "contents": "\nQuick piece of code that turns all selected columns to numeric in R.\n\n\ndf[, c('col1', 'col2')] <- as.numeric(as.character(unlist(df[, c('col1', 'col2')])))\n\n\n\nMutating within tidyverse is always a good options as well.\n\n\ndf %>%\n  mutate_at(vars('column1', 'column2'), as.character)\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-29-simulating-a-moving-average-process/",
    "title": "Simulating a Moving Average Process",
    "description": {},
    "author": [],
    "date": "2019-01-29",
    "categories": [],
    "contents": "\nTwo ways to simulate a moving average process. A moving average is a linear combination of concurrent and historic noises:\n\\[\\begin{equation}\ny_t = z_t + z_{t-1} + z_{t-2}\n\\end{equation}\\]\nwhere \\(y_t\\) is the outcome variable that is influenced by noise at this moment (\\(z_t\\)) and noise from the last two time points. MA(q) processes can occur at any lag, I will use a two lag version here.\nThe first way to simulate this process is to generate all noise terms and then sample from that distribution throughout our recursive routine.\n\n\nset.seed(25)\n\ntime <- 200\nnoise <- rnorm(time)\nma_2 <- NULL\nfor(i in 3:time){\n  \n  ma_2[i] <- noise[i] + 0.7*noise[i-1] + 0.2*noise[i-2]\n  \n}\n\n\n\nThat simulation results in the following.\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\ndf1 <- data.frame(\n  'time' = c(1:time),\n  'y' = c(ma_2)\n)\n\nggplot(df1, aes(x = time, y = y)) + \n  geom_point() + \n  geom_line() \n\n\n\n\nThe second way to simulate it is to generate noise within the loop itself, store the noise, and then apply it to the outcome across time.\n\n\nset.seed(15)\n\nyt <- numeric(time)\nzs <- numeric(time)\n\nfor(i in 1:time){\n  \n  if(i == 1){\n    \n    zs[i] <- rnorm(1,0,1)\n    yt[i] <- zs[i]\n  \n  }else if(i == 2){\n    \n    zs[i] <- rnorm(1,0,1) \n    yt[i] <- zs[i] + 0.7*zs[i-1]\n    \n    }else{\n  \n    zs[i] <- rnorm(1,0,1)\n    yt[i] <- zs[i] + 0.7*zs[i-1] + 0.2*zs[i-2]\n  \n    }\n  \n}\n\n\n\nHere is the plot.\n\n\ndf2 <- data.frame(\n  'time' = c(1:time),\n  'y' = c(yt)\n)\n\nggplot(df2, aes(x = time, y = yt)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nThe second simulation style takes more code but I find it more intuitive. It is difficult for me to wrap my head around simulating all of the noise first and then applying it to the process as if the two are independent components – which is what the first simulation code mimics. To each their own.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2019-01-29-simulating-a-moving-average-process/simulating-a-moving-average-process_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-15-turning-unequal-dates-into-days/",
    "title": "Turning Unequal Dates into Days",
    "description": {},
    "author": [],
    "date": "2019-01-15",
    "categories": [],
    "contents": "\nLongitudinal data of a group or team often have missing days. For example, only Bob reports a stress score on January 3rd even though Joe and Sam are also part of the sample.\n\n   id       date stress\n1 bob 2019-01-01      4\n2 joe 2019-01-01      5\n3 sam 2019-01-01      6\n4 bob 2019-01-02      6\n5 joe 2019-01-02      5\n6 bob 2019-01-03      4\n7 bob 2019-01-04      5\n8 joe 2019-01-04      6\n9 sam 2019-01-04      7\n\nWe want to create an additional column called “day” and use integers rather than dates to make plotting easier/prettier. To do so, we need to create a new data frame of unique dates and unique days, and then we need to merge that new data fram with the original to align the new “day” integer values.\nTurn the dates into a character vector so that they are easier to work with.\n\n\ndf$date <- as.character(df$date)\n\n\n\nNow give each unique date a respective integer “day” value in a new data frame.\n\n\nuniq_dates <- unique(df$date)\n\nday_integers <- data.frame(\n  'date' = c(uniq_dates),\n  'day' = c(1:length(uniq_dates))\n)\n\nday_integers$date <- as.character(day_integers$date)\n\n\n\nFinally, merge the new day_integers data frame with the original so that we have easy numbers for plotting.\n\n\nplot_df <- left_join(df, day_integers)\n\nplot_df\n\n\n   id       date stress day\n1 bob 2019-01-01      4   1\n2 joe 2019-01-01      5   1\n3 sam 2019-01-01      6   1\n4 bob 2019-01-02      6   2\n5 joe 2019-01-02      5   2\n6 bob 2019-01-03      4   3\n7 bob 2019-01-04      5   4\n8 joe 2019-01-04      6   4\n9 sam 2019-01-04      7   4\n\nOne additional note. It can be instructive to see the inefficient way to get the same result using a for-loop. Here is un-evaluated code that is the for-loop equivalent to above.\n\n\n# take unique date\n# which rows match \n# plug in counter to those values\n# increase counter by 1\n\ntime_vec <- numeric(nrow(original_df))\nunique_dates <- unique(original_df$date)\n\ncounter <- 0\n\nfor(i in 1:length(unique_dates)){\n  \n  # take unique date\n  \n  datey <- unique_dates[i]\n  \n  # which rows match this date?\n  \n  use_rows <- which(original_df$date == datey)\n  \n  # increase counter\n  \n  counter <- counter + 1\n  \n  # plug in counter in time vec\n  \n  time_vec[use_rows] <- counter\n  \n}\n\noriginal_df$day <- time_vec\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-14-generating-time-in-a-data-frame/",
    "title": "Generating Time in a Data Frame",
    "description": {},
    "author": [],
    "date": "2019-01-14",
    "categories": [],
    "contents": "\n\n\n\nThere are two code variations I use to generate time indexes. If I need time cycles\n\n  id    score time\n1  a 16.37477    1\n2  b 23.50091    2\n3  c 17.49848    3\n4  a 11.83472    1\n5  b 23.48400    2\n6  c 23.82594    3\n7  a 19.00089    1\n8  b 14.25190    2\n9  c 27.68808    3\n\nthen I use a sequence command.\n\n\ntime <- seq(from = 1, to = 3, each = 3)\n\n\n\nIf I need time ordered\n\n  id    score time\n1  a 19.93433    1\n2  a 20.82730    1\n3  a 29.10959    1\n4  b 25.48347    2\n5  b 22.87176    2\n6  b 19.10510    2\n7  c 18.65874    3\n8  c 19.68696    3\n9  c 26.87015    3\n\nthen I use a replicate command.\n\n\ntime <- rep(c(1:3), each = 3)\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-13-only-store-successful-output-a-counter-placement-issue/",
    "title": "Only Store Successful Output: A Counter Placement Issue",
    "description": {},
    "author": [],
    "date": "2019-01-13",
    "categories": [],
    "contents": "\nSometimes I store every result in my initialized vector/matrix.\nHere is the data.\n\n  people    values day\n1   john  9.991020   1\n2  teddy  8.052606   1\n3  clare  9.213088   1\n4   john  8.819214   2\n5  teddy  8.364933   2\n6   john 10.670680   3\n7  teddy  9.233042   3\n8  clare 11.330505   3\n\nNow the code. I want to find the days where I have responses from John, Teddy, and Clare (as you can tell, I only have responses from all three of them on days 1 and 3).\n\n\nuse_days <- numeric(length(unique(df$days))) # initialized vector\ncounter <- 0\n\nselect_days <- c(1, 2, 3) \n\nfor(i in 1:length(select_days)){\n  counter <- counter + 1\n  \n  \n  # select the i-th day\n  \n  filter_data <- df %>%\n    filter(day == select_days[i])\n  \n  # are there three responses on this day?\n  \n  if(length(filter_data$day) == 3){ \n  use_days[counter] <- filter_data$day\n  }\n}\n\nuse_days\n\n\n[1]  1 NA  3\n\nThat code works, but what if I don’t want to store that NA during the second iteration? To only store successful output, put the counter in the “if statement.”\n\n\nuse_days <- numeric(length(unique(df$days))) # initialized vector\ncounter <- 0\n\nselect_days <- c(1, 2, 3) \n\nfor(i in 1:length(select_days)){\n  \n  # select the i-th day\n  \n  filter_data <- df %>%\n    filter(day == select_days[i])\n  \n  # are there three responses on this day?\n  \n  if(length(filter_data$day) == 3){ \n      counter <- counter + 1            # HERE IS THE CHANGE\n\n  use_days[counter] <- filter_data$day\n  }\n}\n\nuse_days\n\n\n[1] 1 3\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-11-row-labels-needed-to-spread/",
    "title": "Row Labels Needed to Spread",
    "description": {},
    "author": [],
    "date": "2019-01-11",
    "categories": [],
    "contents": "\nNo explanation for this set of notes, just a few reminders when spreading and gathering.\n\n  b_partial b_wo_partial se_partial se_wo_partial\n1         1            4          6             3\n2         2            5          7             2\n3         3            6          8             1\n\nWe want the columns to be “model,” “result,” and “value.”\nHere is my incorrect attempt.\n\n\ncd_try <- cd_try %>%\n  gather(b_partial, b_wo_partial, key = 'model', value = 'b1') \n\ncd_try\n\n\n  se_partial se_wo_partial        model b1\n1          6             3    b_partial  1\n2          7             2    b_partial  2\n3          8             1    b_partial  3\n4          6             3 b_wo_partial  4\n5          7             2 b_wo_partial  5\n6          8             1 b_wo_partial  6\n\n\n\ncd_try <- cd_try %>%\n  gather(se_partial, se_wo_partial, key = 'se_model', value = 'sd')\n\ncd_try # not evaluated because it won't work\n\n\n\nInstead, I need to gather everything in at the same time, split, and then spread.\n\n  b_partial b_wo_partial se_partial se_wo_partial\n1         1            4          6             3\n2         2            5          7             2\n3         3            6          8             1\n\nGather\n\n\ncd_try <- cd_try %>%\n  gather(b_partial, b_wo_partial, \n         se_partial, se_wo_partial,\n         key = 'result_model', value = 'value') # gather everything\n\ncd_try\n\n\n    result_model value\n1      b_partial     1\n2      b_partial     2\n3      b_partial     3\n4   b_wo_partial     4\n5   b_wo_partial     5\n6   b_wo_partial     6\n7     se_partial     6\n8     se_partial     7\n9     se_partial     8\n10 se_wo_partial     3\n11 se_wo_partial     2\n12 se_wo_partial     1\n\nSplit\n\n\ncd_try <- cd_try %>%\n  separate(result_model, into = c('result', 'model'), sep = \"_\")\n\ncd_try\n\n\n   result   model value\n1       b partial     1\n2       b partial     2\n3       b partial     3\n4       b      wo     4\n5       b      wo     5\n6       b      wo     6\n7      se partial     6\n8      se partial     7\n9      se partial     8\n10     se      wo     3\n11     se      wo     2\n12     se      wo     1\n\nSpread, BUT WHEN YOU SPREAD MAKE SURE TO INCLUDE ROW IDENTIFIERS.\n\n\ncd_try <- cd_try %>%\n  mutate(row_help = rep(1:6, 2))\n\ncd_try <- cd_try %>%\n  spread(result, value)\n\ncd_try\n\n\n    model row_help b se\n1 partial        1 1  6\n2 partial        2 2  7\n3 partial        3 3  8\n4      wo        4 4  3\n5      wo        5 5  2\n6      wo        6 6  1\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-10-reveal-hidden-nas-in-longitudinal-data/",
    "title": "Reveal Hidden NAs in Longitudinal Data",
    "description": {},
    "author": [],
    "date": "2019-01-10",
    "categories": [],
    "contents": "\nLongitudinal data sets often have hidden NAs when they are in long-form. For example, in the data set below Zoe is missing on days 2 and 4, but it isn’t obvious because there are no specific “NA’s” within the data.\n\n   time   id q1 q2\n1     1  Jac  4  3\n2     1 Jess  5  2\n3     1  Zoe  3  4\n4     2  Jac  6  1\n5     2 Jess  7  2\n6     3  Jac  5  3\n7     3 Jess  4  4\n8     3  Zoe  3  2\n9     4  Jac  4  3\n10    4 Jess  5  4\n\nUsually I recommend cleaning within the tidyverse package, but in this case I prefer reshape. Change the data frame to wide\n\n\nlibrary(reshape2)\nwide_cd <- reshape(cd, timevar = 'time', idvar = 'id', direction = 'wide')\n\n\n\nand then back to long to reveal the hidden NA’s.\n\n\ncd_reveal <- reshape(wide_cd, timevar = 'time', idvar = 'id', direction = 'long')\ncd_reveal\n\n\n         id time q1.1 q2.1\nJac.1   Jac    1    4    3\nJess.1 Jess    1    5    2\nZoe.1   Zoe    1    3    4\nJac.2   Jac    2    6    1\nJess.2 Jess    2    7    2\nZoe.2   Zoe    2   NA   NA\nJac.3   Jac    3    5    3\nJess.3 Jess    3    4    4\nZoe.3   Zoe    3    3    2\nJac.4   Jac    4    4    3\nJess.4 Jess    4    5    4\nZoe.4   Zoe    4   NA   NA\n\nIt is possible to do all of this within tidyverse, but it’s tricky because the spread command only applies to one column (the value parameter only takes one entry), so anytime your data frame contains multiple columns to spread over (almost always the case) then spread does not work well.\n\n\nlibrary(tidyverse)\ncd %>%\n spread(key = time, value = q1)\n\n\n    id q2  1  2  3  4\n1  Jac  1 NA  6 NA NA\n2  Jac  3  4 NA  5  4\n3 Jess  2  5  7 NA NA\n4 Jess  4 NA NA  4  5\n5  Zoe  2 NA NA  3 NA\n6  Zoe  4  3 NA NA NA\n\nNotice how it only used q1. The proper way to go from long to wide and then back to long to reveal the NA’s using tidyverse is either of the following:\n\n\ncd %>%\n  select(time, id, q1) %>%\n  spread(key = time, value = q1) %>%\n  gather(key = time, value = 'q1', '1','2','3','4') # string code needed\n\n\n     id time q1\n1   Jac    1  4\n2  Jess    1  5\n3   Zoe    1  3\n4   Jac    2  6\n5  Jess    2  7\n6   Zoe    2 NA\n7   Jac    3  5\n8  Jess    3  4\n9   Zoe    3  3\n10  Jac    4  4\n11 Jess    4  5\n12  Zoe    4 NA\n\n\n\ntime_string <- as.character(unique(cd$time))\n\ncd %>%\n  select(time, id, q1) %>%\n  spread(key = time, value = q1) %>%\n  gather(key = time, value = 'q1', time_string) # string code not needed due to pre-allocation\n\n\n     id time q1\n1   Jac    1  4\n2  Jess    1  5\n3   Zoe    1  3\n4   Jac    2  6\n5  Jess    2  7\n6   Zoe    2 NA\n7   Jac    3  5\n8  Jess    3  4\n9   Zoe    3  3\n10  Jac    4  4\n11 Jess    4  5\n12  Zoe    4 NA\n\nAgain, I prefer reshape because the spread commands in tidyverse are not easy to read.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-09-ggsave/",
    "title": "GGsave",
    "description": {},
    "author": [],
    "date": "2019-01-09",
    "categories": [],
    "contents": "\nQuick note on ggsave in ggplot2.\n\n\ng1 <- ggplot(df1, aes(x = time, y = y)) + \n  geom_point(data = df1, aes(color = color), size = 6) + \n  geom_line() + \n  theme_minimal() +\n  theme(panel.grid = element_blank()) + \n  theme(text = element_blank()) + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  theme(legend.position = \"none\")\n\n\nggsave('one.pdf', g1, width = 6, height = 3)\n\n\np <- ggplot(df, aes(x = Time, y = Cohesion, color = ID)) + \n  geom_point() + \n  geom_line() + \n  facet_wrap(~Team, nrow = 1) + \n  theme_minimal() +\n  theme(panel.grid = element_blank()) + \n  theme(text = element_text(size = 12)) + \n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) + \n  scale_x_discrete(limits = c(1,3,5,7)) + \n  labs(title = \"Simulation 1\")\n\n\nggsave(\"test.2.pdf\",p, width=6, height=2.8)\n\n\n\nBo\\(^2\\)m = )\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-08-the-premature-covariate/",
    "title": "The Premature Covariate",
    "description": {},
    "author": [],
    "date": "2019-01-08",
    "categories": [],
    "contents": "\nA replication of Patricia Cohen’s wonderful, “problem of the premature covariate” (chapter 2 in Collins & Horn, 1991). Here is a simple version of the problem. Imagine that we want to know the influence of a life event, like meeting a friend, on happiness. We conduct a study where we measure people’s happiness at time one, wait two weeks, and then measure their happiness again along with whether or not they met a friend since we last observed them. To assess the relationship between meeting a friend and happiness, we regress post happiness on both pre happiness and whether or not they met a friend. That is, our regression takes the form:\n\n\nHappy\\(_{post}\\) ~ Happy\\(_{pre}\\) + Met_Friend.\n\n\nWhat Dr. Cohen draws attention to is that the coefficient relating “met friend” to happiness will be biased if (a) there is some non-zero probability of reverse causality and (b) we do not measure “met friend” exactly when it occurs. Remember, in our mock study we assessed both happiness and whether or not someone met a friend during our post-measure. Is that exactly when different people across the sample actually met a new friend? Perhaps, but most of our sample either did or did not meet a friend at some unknown time within the past two weeks; Dr. Cohen’s point is that this unknown is an issue to linger on.\nSimulation Explanation\nIf Dr. Cohen’s issue is worthwhile, then, under a system where “met friend” truly does not influence happiness, we should be able to make “met friend” appear to influence happiness based on the points she raises. That is, when “met friend” does not influence happiness we should be able to make it appear so if we create a system where (1) happiness instead influences the probability of meeting a friend (reverse causality) and (2) we do not measure “met friend” exactly when it occurs. Below, we generate data where “met friend” does not influence happiness but the coefficient relating “met friend” to happiness will still be significant (and large) because of the issues raised.\nHere are the steps to the simulation:\nStart with a random value for happiness (distributed normally across 600 people) at time one.\nHappiness at \\(t+1\\) is its previous value plus one of the following, all with equal probability: +0.25, -0.25, or 0.\nAt each time point, concurrent happiness influences the probability of meeting a friend. When happiness is low people are unlikely to meet a friend, whereas when happiness is high people are more likely to meet a friend. Meeting a friend is coded as 0 or 1 for each time point (i.e., no or yes).\nContinue for 25 time points.\nAssess the relationships between post happiness, pre happiness, and “met friend.” Pre happiness is always time one, whereas we will explore different post assesssments (e.g., post happiness is time 25 vs. post happiness is time 20). “Met friend” will always be whether the individual met a friend within 5 time points of the post happiness assessment. So, if we analyze post happiness as time 20, then “met friend” is whether the individual met a friend during times 15 through 20.\nNotice that the simulation captures the notions raised above: “met friend” does not influence happiness, instead the reverse happens. And after making a decision about the timing of our pre and post assessment we lose information about when “met friend” actually happened. We know whether it happened but not when; we also don’t retain information on the differences in timing across our sample.\nMeeting a Friend or Not\nThe most difficult aspect of the simulation is specifying step 3: “met friend” is some function of concurrent happiness. Dr. Cohen’s original explanation is, “the probability of \\(X\\) for each unit of time was determined by a Markov process, with probability increasing as a function of the level of contemporaneous \\(Y\\). Probabilities used increased from 0 for those with current \\(Y\\) less than -1.00 to 0.25 for those with current scores of 1.5 or greater” (she uses different variables for x and y in her discussion). What does that mean? How do we specify a Markov process where the probability of “met friend” is between 0 and 0.25 with respect to happiness cutoffs like -1.00 and 1.5? I don’t know either. But we can make it easier by recognizing that, at its core, the idea is simply, “meeting a friend is more likely when people are happier,” which we can represent with a simple linear equation like \\(y = mx + b\\). All we need to do is to find the slope and y-intercept, then we’ll have an equation where we can plug in “happiness” and get “probability of meeting a friend.” Here is how.\nRemember that we can find the slope and y-intercept of a line if we know the location of two of its points. Here, we know that the probability of “met friend” needs to be between 0 and 0.25, and the happiness cutoffs need to be -1.00 and 1.5. If I want to relate happiness to “met friend,” then, I can put happiness on the x-axis and “met friend” on the y-axis and recognize that by combining these cutoffs I get the end-points of a line: (1.5, 0.25) is one point and (-1, 1.5) is the other. Computing rise-over-run and then solving for the intercept gives me the following:\n\n\nProbability of meeting a friend = 0.1*Happy + 0.01\n\n\nNow we have a way to compute the probability of meeting a friend based on happiness. It is not as precise as the Markov process but it will work just fine. (Note: I actuallly use the points (1.4999, 2.4999) and (-0.999, 1.4999) to calculate the slope and intercept in the simulation because I will also use if-statements for the cutoffs)\nSimulate One Person\nIt’s always helpful to make sure we can get a simulation to work on one person. In the simulation below, \\(y\\) is happiness and \\(x\\) is “met friend.”\n\n\ntime <- 25\ny <- numeric(time)\nx <- numeric(time)\n\ncount <- 0\n\nfor(i in 1:time){\n  count <- count + 1\n  \n  if(i == 1){\n    \n    y[count] <- rnorm(1, mean = 0.5, sd = 0.5)\n    x[count] <- 0\n    \n  }else{\n    \n    \n    # y up or down with autoregression\n    \n    updownsame <- sample(c('up', 'down', 'same'), 1)\n    \n    if(updownsame == 'up'){\n      \n      y[count] <- y[count - 1] + 0.25\n      \n    }else if(updownsame == 'down'){\n      \n      y[count] <- y[count - 1] - 0.25\n      \n    }else{\n      \n      y[count] <- y[count - 1]\n      \n    }\n    \n    # x is a function of y\n    \n    if(y[count] <= -1.00){\n      \n      x_prob <- 0\n      \n    }else if(y[count] >= 1.5){\n      \n      x_prob <- 0.25\n      \n    }else{\n      \n      x_prob <- 0.10004*y[count] + 0.09994\n      \n      \n    }\n    \n    x[count] <- rbinom(1, 1, x_prob)\n    \n  }\n  \n}\n\n\n\nFull Sample\nThat script worked, so now let’s update the code slightly and run it across 600 people.\n\n\npeople <- 600\ntime <- 25\ndf <- matrix(, nrow = people*time, ncol = 4)\n\ncount <- 0\n\nfor(j in 1:people){\n  \n  \n\nfor(i in 1:time){\n  count <- count + 1\n  \n  if(i == 1){\n    \n    df[count, 1] <- j\n    df[count, 2] <- i\n    df[count, 3] <- rnorm(1, mean = 0.5, sd = 0.5)\n    df[count, 4] <- 0\n    \n  }else{\n    \n    df[count, 1] <- j\n    df[count, 2] <- i\n    \n    # y up or down with autoregression\n    \n    updownsame <- sample(c('up', 'down', 'same'), 1)\n    \n    if(updownsame == 'up'){\n      \n      df[count, 3] <- df[count - 1, 3] + 0.25\n      \n    }else if(updownsame == 'down'){\n      \n      df[count, 3] <- df[count - 1, 3] - 0.25\n      \n    }else{\n      \n      df[count, 3] <- df[count - 1, 3]\n      \n    }\n    \n    # x is a function of y\n    \n    if(df[count, 3] <= -1.00){\n      \n      x_prob <- 0\n      \n    }else if(df[count, 3] >= 1.5){\n      \n      x_prob <- 0.25\n      \n    }else{\n      \n      x_prob <- 0.10004*df[count, 3] + 0.09994\n      \n    }\n    \n    df[count, 4] <- rbinom(1, 1, x_prob)\n    \n  }\n  \n}\n\n  \n  \n}\n\n\ndf <- data.frame(df)\nnames(df) <- c('id', 'time', 'happy', 'met_friend')\nlibrary(tidyverse)\n\n\n\nResults\nRemember, we generated data where “met friend” did not influence happiness. Now we are going to assess the coefficient relating “met friend” to happiness to see if it differs from zero. First, let’s say our post-assessment happened at time 10.\nTrim down our data set to just that time frame.\n\n\nhappy10_sample <- df %>%\n  filter(time < 11)\n\n\n\nHow many friends did each person meet between times 5 and 10?\n\n\nfriend_count <- happy10_sample %>%\n  filter(time > 4) %>%\n  group_by(id) %>%\n  summarise(\n    friend_count = sum(met_friend)\n  )\n\n\n\nNow change that to:\n0 = did not happen\n1 = happened at least once (meaning sum of friend_count is not equal to 0)\n\n\nfriend_count <- friend_count %>%\n  mutate(friend_event = case_when(\n    friend_count == 0 ~ 0,\n    friend_count != 0 ~ 1\n  ))\n\n\n\nMerge that count back into the happy10 data set and prepare the data for regression.\n\n\n# Merge back into y10 df\n\nhappy10_sample <- left_join(happy10_sample, friend_count)\n\n# Filter down to what's needed for regression\n\nhappy10_filter <- happy10_sample %>%\n  select(id, time, happy, friend_event) %>%\n  filter(time == 1 | time == 10)\n\nlibrary(reshape2)\n\nhappy10_wide <- reshape(happy10_filter, idvar = 'id', timevar = 'time', direction = 'wide')\n\n# The x columns are synonymous, so I can remove one \n\nhappy10_wide <- happy10_wide[, c('id', 'happy.10', 'happy.1', 'friend_event.1')]\nnames(happy10_wide) <- c('id', 'happy_post', 'happy_pre', 'met_friend')\n\n\n\nNow regress post happy on pre happy and whether or not they met a friend between times 5 and 10.\n\n\nsummary(lm(happy_post ~ happy_pre + met_friend,\n           data = happy10_wide))$coefficients\n\n\n               Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.08305241 0.04376560 -1.897664 5.822247e-02\nhappy_pre    0.86224682 0.05237323 16.463503 1.730324e-50\nmet_friend   0.20006802 0.05412227  3.696593 2.386306e-04\n\nThe coefficient relating “met friend” to happiness is about 0.3 and it is significant (remember there was no influence from “met friend” to happiness).\nWhat about when we change the post assessment to time point 15?\nFirst create a function out of all the “tidying” steps above:\n\n\ndf_create <- function(time1){\n  library(reshape2)\n  library(tidyverse)\n  time2 <- time1 - 5\n  \n  y_sample <- df %>%\n    filter(time <= time1)\n  \n  friend_count <- y_sample %>%\n    filter(time >= time2) %>%\n    group_by(id) %>%\n    summarise(\n      friend_count = sum(met_friend)\n    )\n  \n  friend_count <- friend_count %>%\n    mutate(friend_event = case_when(\n      friend_count == 0 ~ 0,\n      friend_count != 0 ~ 1\n    ))\n  \n  y_sample <- left_join(y_sample, friend_count)\n  y_filter <- y_sample %>%\n    select(id, time, happy, friend_event) %>%\n    filter(time == 1 | time == time1)\n  \n  y_wide <- reshape(y_filter, idvar = 'id', timevar = 'time', direction = 'wide')\n  \n  yname <- paste('happy.', time1, sep = '')\n  \n  y_wide <- y_wide[, c('id', yname, 'happy.1', 'friend_event.1')]\n  names(y_wide) <- c('id', 'happy_post', 'happy_pre', 'met_friend')\n  \n  return(y_wide)\n\n}\n\n\n\nHere are the results:\n\n\nhappy15_wide <- df_create(15)\n\nsummary(lm(happy_post ~ happy_pre + met_friend,\n           data = happy15_wide))$coefficients\n\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.1990938 0.05192122 -3.834536 1.391674e-04\nhappy_pre    0.7839255 0.06106283 12.838014 1.737539e-33\nmet_friend   0.4890845 0.06212003  7.873218 1.636154e-14\n\nWhat about when we select time 25 as our post assessment?\n\n\nhappy25_wide <- df_create(25)\n\nsummary(lm(happy_post ~ happy_pre + met_friend,\n           data = happy25_wide))$coefficients\n\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.3211208 0.06592975 -4.870651 1.426504e-06\nhappy_pre    0.6821083 0.07913204  8.619876 6.016853e-17\nmet_friend   0.8063170 0.07986960 10.095418 3.125506e-22\n\nNotice how large the coefficient relating “met friend” to happiness is here: close to 0.9 – remember, there truly is no effect relating “met friend” to happiness.\nConclusion\nIf there is some probability of reverse causality and we don’t measure the event exactly when it occurs then the estimate relating that event to our outcome will be biased. If many “event opportunities” occur between our pre and post measure then our estimate will be extremely biased.\nBo\\(^2\\)m = )\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-06-column-names-as-parameters-with-ggplot2/",
    "title": "Column Names As Parameters with GGplot2",
    "description": {},
    "author": [],
    "date": "2019-01-06",
    "categories": [],
    "contents": "\nAnother example of using column names as parameters with quo, this time within ggplot2. A snippet of the data:\n\n  day   id stress performance\n1   1 Josh      8           8\n2   2 Josh      9          12\n3   3 Josh      4          17\n4   4 Josh      8           6\n5   5 Josh      8          15\n6   6 Josh      8          16\n\nLet’s say we want to plot each person’s stress over time: three time-series trajectories.\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nggplot(df, aes(x = day, y = stress, color = id)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nGreat, but imagine having a data set with 300 different DVs. Instead of re-calling ggplot each time we can create a function where the column (DV) is the paramter.\n\n\nplot_it <- function(col_name){\n  \n  g <- ggplot(df, aes(x = day, y = !!col_name, color = id)) + \n  geom_point() + \n  geom_line()\n  \n  return(g)\n  \n}\n\n\n\nNote the !! before the parameter. Now, to plot the new graph we use quo within the function call.\n\n\nplot_it(quo(performance))\n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2019-01-06-column-names-as-parameters-with-ggplot2/column-names-as-parameters-with-ggplot2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-05-mutating-scale-items-with-na/",
    "title": "Mutating Scale Items with NA",
    "description": {},
    "author": [],
    "date": "2019-01-05",
    "categories": [],
    "contents": "\nCreating item totals with a data set containing NAs is surprisingly difficult. Here is the data.\n\n\nlibrary(tidyverse)\n\ncd <- data.frame(\n  \"q1\" = c(1,2,NA),\n  \"q2\" = c(2,2,2),\n  'q3' = c(NA, NA,2),\n  'id' = c('201', '202', '203')\n)\n\ncd\n\n\n  q1 q2 q3  id\n1  1  2 NA 201\n2  2  2 NA 202\n3 NA  2  2 203\n\nMutating directly over columns with NA does not work.\n\n\ncd %>%\n  mutate(cohesion = \n           q1 + q2 + q3)\n\n\n  q1 q2 q3  id cohesion\n1  1  2 NA 201       NA\n2  2  2 NA 202       NA\n3 NA  2  2 203       NA\n\nFiltering removes the data we are interested in.\n\n\ncd %>%\n  filter(!is.na(q1) == T && !is.na(q2) == T && !is.na(q3) == T)\n\n\n[1] q1 q2 q3 id\n<0 rows> (or 0-length row.names)\n\nWe cannot use rowMeans in combination with mutate because the two are not compatible. The code below is not evaluated, but if you run it it does not work.\n\n\ncd %>%\n  mutate(cohesion =\n           rowMeans(q1, q2, q3, na.rm = T))\n\n\n\nUsing the rowwise command within a pipe gets us close\n\n\ncd %>%\n  rowwise() %>%\n  mutate(mean = mean(q1, q2, q3, na.rm = T))\n\n\n# A tibble: 3 × 5\n# Rowwise: \n     q1    q2    q3 id     mean\n  <dbl> <dbl> <dbl> <chr> <dbl>\n1     1     2    NA 201       1\n2     2     2    NA 202       2\n3    NA     2     2 203     NaN\n\nbut the mean value is not calculated correctly. We need to include c() to vectorize the items.\n\n\ncd %>%\n  rowwise() %>%\n  mutate(mean = mean(c(q1, q2, q3), na.rm = T))\n\n\n# A tibble: 3 × 5\n# Rowwise: \n     q1    q2    q3 id     mean\n  <dbl> <dbl> <dbl> <chr> <dbl>\n1     1     2    NA 201     1.5\n2     2     2    NA 202     2  \n3    NA     2     2 203     2  \n\nFinally the right answer. Use rowwise in combination with a vectorized mutate.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-04-frequentist-confidence-intervals/",
    "title": "Frequentist Confidence Intervals",
    "description": {},
    "author": [],
    "date": "2019-01-04",
    "categories": [],
    "contents": "\nPurpose\nImagine that you are interested in the relationship between stress and performance. To assess it, you observe 600 people at work and measure their stress via a self-report (e.g., “I feel stressed”) and their performance via objective performance scores for the day (e.g., number of sales). You regress performance on stress and find that the estimated coefficient relating to two is 0.45. You then build a 95% confidence interval using the standard error that the analysis spit out and find that the CI is 0.45 +- 0.1.\nWhat does that confidence interval actually mean? The purpose of this exercise is to build intuition behind frequentist CIs.\nSteps\nGenerate the population\nSample the population. On that sample…\n2a) Regress performance on stress\n2b) Calculate a CI\n2c) Does the CI contain the population parameter?\nRe-sample and repeat\n1) Generate the population\nOur population will contain 100,000 people\n\n\npop_number <- 100000\n\n\n\nwith stress scores distributed about zero. The scale here doesn’t matter – we care about the relationship between stress and performance and less about (in this example) the distributions of stress and performance themselves.\n\n\npopulation_stress <- rnorm(pop_number, 0, 5)\n\n\n\nThe true relationship between stress and performance will be 0.45. Let’s set that parameter\n\n\nstress_performance_coefficient <- 0.45\n\n\n\nand then generate performance.\n\n\npopulation_performance <- stress_performance_coefficient*population_stress + rnorm(pop_number, 0, 1)\n\n\n\nNow plug everything into a data set. Remember, this is the population.\n\n\ndf <- data.frame(\n  'person' = c(1:pop_number),\n  'stress' = c(population_stress),\n  'performance' = c(population_performance)\n)\n\n\n\nWhat is the paramter relating stress to performance? 0.45, keep that in mind. Time to sample the population as if we conducted a study and run our regression.\n2) Sample the population\nRandomly select 600 people from our population. That is, pretend we ran a study on 600 subjects.\n\n\nsample_size <- 600\nrandom_numbers <- sample(c(1:pop_number), sample_size)\n\nsample_df <- df %>%\n  filter(person %in% random_numbers)\n\n\n\n2a) Regress Performance on Stress\nUse the lm command for regression in R.\n\n\nsummary(lm(performance ~ stress,\n           data = sample_df))\n\n\n\nCall:\nlm(formula = performance ~ stress, data = sample_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2166 -0.7325 -0.0102  0.7308  3.1016 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0003186  0.0427267  -0.007    0.994    \nstress       0.4482278  0.0087063  51.483   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.044 on 598 degrees of freedom\nMultiple R-squared:  0.8159,    Adjusted R-squared:  0.8156 \nF-statistic:  2651 on 1 and 598 DF,  p-value: < 2.2e-16\n\n2b) Compute the CI\nSave the output of the regression in an object so we can pull out the specific coefficients that we are interested in.\n\n\noutput <- summary(lm(performance ~ stress,\n                  data = sample_df))\n\n\n\nPull out the coefficient relating stress to performance\n\n\nslope_coefficient <- output$coefficients[2,1]\n\n\n\nand use it along with the SEs to calculate the confidence interval.\n\n\nse_upper <- slope_coefficient + 1.96*output$coefficients[2,2]\nse_lower <- slope_coefficient - 1.96*output$coefficients[2,2]\n\n\n\n2c) Does the CI contain the population parameter?\nRemember that the parameter is 0.45.\n\n\ncontain_parameter <- NULL\n\nif(se_lower <= stress_performance_coefficient && se_upper >= stress_performance_coefficient){\n  contain_parameter <- 'yes'\n}else{\n  contain_parameter <- 'no'\n}\n\ncontain_parameter\n\n\n[1] \"yes\"\n\nWhat did we do? We sampled the population, ran a regression to relate stress to performance, and then calculated a CI on the slope term. The interpretation of a CI, however, is across infinite samples. Now we need to run through the sample, regress, and calculate CI procedure again and again and again – Monte Carlo.\n3) Re sample and repeat\nI created a function that samples the population, runs a regression, calculates the CI, and then saves whether or not the interval contained 0.45 (‘yes’ or ‘no’). You can view that code in the raw rmarkdown file. For now, just know that the function is called sample_regress_calc_ci.\n\n\n\nWe are going to re-run step 2 from above 900 times\n\n\nsims <- 900\n\n\n\nand store the ‘yes’ or ‘no’ result in a vector.\n\n\nall_ci_contains <- numeric(sims)\n\n\n\nHere is the full Monte Carlo code.\n\n\nsims <- 900\nall_ci_contains <- numeric(sims)\n\nfor(i in 1:sims){\n  \n  result <- sample_regress_calc_ci()\n  all_ci_contains[i] <- result\n  \n}\n\n\n\nInterpretation\nHow many computed intervals contain the population parameter?\n\n\nsum(all_ci_contains == 'yes') / sims\n\n\n[1] 0.9511111\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-09-06-more-on-column-names-as-parameters/",
    "title": "More on Column Names as Parameters",
    "description": {},
    "author": [],
    "date": "2018-09-06",
    "categories": [],
    "contents": "\n\n\n\nUse quo or enquo when you want to include column names as parameters in a function. For example, a function like the following would not work:\n\n\nbad_function <- function(data, col_name){\n  \n  newdf <- data %>%\n    mutate('adjusted_column' = col_name + 1)\n  \n  return(newdf)\n  \n}\n\nbad_function(df, column_i_care_about)\n\n\n\nbecause column_i_care_about isn’t specified in a form that mutate can work with.\nExamples\nThe data are contained in df1.\n\n\ndf1 <- data.frame(\n  a = c(1,2,NA),\n  b = c(NA,3,4)\n)\n\ndf1\n\n\n   a  b\n1  1 NA\n2  2  3\n3 NA  4\n\nThe function: take the column specified by the parameter and add one to every value. Then return the new data frame.\n\n\nadder <- function(col_use){\n  \n  newdf <- df1 %>%\n    mutate('adder' = \n             (!!col_use) + 1)  # correct form here using !!\n    \n  return(newdf)\n  \n}\n\nadder(quo(a))                 # correct form here using quo\n\n\n   a  b adder\n1  1 NA     2\n2  2  3     3\n3 NA  4    NA\n\nA more complicated function by incorporating is.na.\n\n\nna_tagger <- function(col_use){\n  \n  newdf <- df1 %>%\n    mutate('na_tag' = \n             ifelse(is.na((!!col_use)) == T, 1, 0))\n  \n  return(newdf)\n}\n\nna_tagger(quo(a))\n\n\n   a  b na_tag\n1  1 NA      0\n2  2  3      0\n3 NA  4      1\n\nIn the examples above I used quo interactively. You get the same result by instead using enquo within the function.\n\n\nadder2 <- function(col_use){\n  \n  col_use <- enquo(col_use)\n  \n  newdf <- df1 %>%\n    mutate('adder' = \n             (!!col_use) + 1)\n  \n  return(newdf)\n}\n\nadder2(a)\n\n\n   a  b adder\n1  1 NA     2\n2  2  3     3\n3 NA  4    NA\n\nOne More Note\nSometimes I also need to specify the data set and column within a dplyr command and then use the parameter to select a specific row. The following format seems to work well: data[['col_name']][row]. Here is a function that is inefficient but demonstrates the point well:\n\n\nselector2 <- function(x, y){\n  \n  new <- df1 %>%\n    filter(robby == df1[['robby']][x]) %>%\n    filter(ruddy == df1[['ruddy']][y])\n  \n  return(new)\n}\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-12-monte-carlo-approximation/",
    "title": "Monte Carlo Approximation",
    "description": {},
    "author": [],
    "date": "2018-08-12",
    "categories": [],
    "contents": "\nMonte Carlo helps us understand processes that we can describe but don’t yet have analytic solutions for. Here are two examples: the birthday problem and the tasting tea problem.\nBirthday Problem\nIf you are standing in a room with 25 other people, what is the probability that at least two people share the same birthday? This question has a mathematical solution, but if we don’t know it we can use Monte Carlo to help.\nSelect 25 people with random birthdays\n\n\ngroup_birthdays <- sample(1:365, 15, replace = TRUE)\n\n\n\nand then check whether two of them share a birthday.\n\n\nshared_birthday <- length(group_birthdays[duplicated(group_birthdays)])\n\n# Returns 1 if yes and 0 if no\n\n\n\nNow place everything into a loop and evaluate 5000 times for the final Monte Carlo:\n\n\ngroup_size <- 15\niterations <- 5000\nshared_birthdays_counter <- 0\n\nfor(i in 1:iterations){\n  \n  \n  group_birthdays <- sample(1:365, 15, replace = TRUE)\n  \n  shared_birthday <- length(group_birthdays[duplicated(group_birthdays)])\n  \n  if(shared_birthday == 1){\n    \n    shared_birthdays_counter <- shared_birthdays_counter + 1\n  }\n  \n  \n  \n}\n\n\n\nThe probability of a shared birthday among a group of 15 is…\n\n\nshared_birthdays_counter / iterations\n\n\n[1] 0.2188\n\nThe probability of a shared birthday as we increase group size…\n\n\nsizes <- 2:25\nprob_store <- numeric(length(sizes))\n\nfor(j in 1:24){\n  \n  \n\n\ngroup_size <- j\niterations <- 5000\nshared_birthdays_counter <- 0\n\nfor(i in 1:iterations){\n  \n  \n  group_birthdays <- sample(1:365, group_size, replace = TRUE)\n  \n  shared_birthday <- length(group_birthdays[duplicated(group_birthdays)])\n  \n  if(shared_birthday == 1){\n    \n    shared_birthdays_counter <- shared_birthdays_counter + 1\n  }\n  \n  \n  \n}\n\nprob_store[j] <- shared_birthdays_counter / iterations\n\n}\n\ndf <- data.frame(\n  'group_size' = c(2:25),\n  'probability' = c(prob_store)\n)\n\nlibrary(ggplot2)\n\nplot1 <- ggplot(df, aes(x = group_size, y = probability)) + \n  geom_bar(stat = 'identity', color = 'orange')\n\nplot1\n\n\n\n\nThe equation to solve the birthday problem is\n\\[\\begin{equation}\nn! / (n - k)!\n\\end{equation}\\]\nwhere \\(n\\) is the number of possible birthdays and \\(k\\) is group size. The beauty of Monte Carlo is that we didn’t need the above equation to learn about our shared birthday process.\nTasting Tea\nImagine that I make one cup of tea with milk and then ask you the following: did I pour the tea or milk first? I repeat this for eight cups of tea. What is the probability that you guess correctly for 3 of the cups? For all 8 cups?\nFirst, we generate truth. For each cup, ‘M’ means I poured milk first and ‘T’ means I poured tea first.\n\n\npossible_pours <- c(rep('M', 4), rep('T', 4))\ntrue_pours <- sample(possible_pours, size = 8)\n\n# The true first pours\n\ntrue_pours\n\n\n[1] \"M\" \"M\" \"M\" \"M\" \"T\" \"T\" \"T\" \"T\"\n\nThen you make a guess for each cup.\n\n\nguess <- c('M', 'T', 'T', 'M', 'T', 'T', 'M', 'M')\n\n\n\nIn this case, you guessed that I poured milk first for cup 1 and tea first for cup 2. How many of your guesses are correct?\n\n\ncorrect <- sum(true_pours == guess)\n\ncorrect\n\n\n[1] 4\n\nNow we can put all of that into a Monte Carlo loop.\n\n\niterations <- 5000\ncorrect_store <- numeric(iterations)\n\nfor(i in 1:iterations){\n  \n  possible_pours <- c(rep('M', 4), rep('T', 4))\n  true_pours <- sample(possible_pours, size = 8)\n  \n  guess <- c('M', 'T', 'T', 'M', 'T', 'T', 'M', 'M')\n  \n  correct <- sum(true_pours == guess)\n  \n  correct_store[i] <- correct\n\n  \n}\n\n\n\nWhat is the probability of you guessing correctly for 2 cups…6?\n\n\nprop.table(table(correct_store))\n\n\ncorrect_store\n     0      2      4      6      8 \n0.0142 0.2228 0.5164 0.2326 0.0140 \n\nJust like the birthday problem, there are equations that govern this “tea problem.” We don’t know what they are, but we can still learn about the process by using Monte Carlo approximation.\nThese examples can be found with greater discussion in Quantitative Social Science by Kosuke Imai.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-08-12-monte-carlo-approximation/monte-carlo-approximation_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-04-longitudinal-plotting/",
    "title": "Longitudinal Plotting",
    "description": {},
    "author": [],
    "date": "2018-07-04",
    "categories": [],
    "contents": "\nA few random notes about plotting, describing, and thinking about trajectories.\nPlotting Trajectories\nImagine we record “affect” (\\(Y\\)) for five people over 20 time points. ggplot2 produces poor longitudinal trajectories if you only specify time and affect as variables:\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nplot1 <- ggplot(df1, aes(x = time, y = affect)) + \n  geom_point() + \n  geom_line()\n\nplot1\n\n\n\n\nInstead, specify “id” either as the grouping variable:\n\n\nplot2 <- ggplot(df1, aes(x = time, y = affect, group = id)) + \n  geom_point() + \n  geom_line()\n\nplot2\n\n\n\n\nor a color.\n\n\nplot3 <- ggplot(df1, aes(x = time, y = affect, color = id)) + \n  geom_point() + \n  geom_line()\n\nplot3\n\n\n\n\nIf you have a data set with too many trajectories\n\n\n\nthen select a random sample to keep dark\n\n\ndf2_sample_ids <- sample(df2$id, 5)\ndf2_sample <- df2 %>%\n  filter(id %in% df2_sample_ids)\n\n\n\nand change the color of the background trajectories to a lighter color.\n\n\nplot5 <- ggplot(df2, aes(x = time, y = affect, group = id)) + \n  geom_point(color = 'gray85') + \n  geom_line(color = 'gray85') + \n  \n  \n  # HERE COMES ADDITIONAL CHANGES\n  \n  geom_point(data = df2_sample, aes(x = time, y = affect, group = id)) + \n  geom_line(data = df2_sample, aes(x = time, y = affect, group = id))\n\nplot5\n\n\n\n\nNotice that I had to evoke two additional geom commands and source my new data sample.\nTrajectory Descriptions\nEquilibrium\n\n\n\nPanel A: Increasing equilibrium level with constant variance.\nPanel B: Decreasing equilibrium level with constant variance.\nPanel C: Decreasing equilibrium level with increasing variance.\nLatent Growth Intercepts and Slopes\n\n\n\nPanel A: Between person differences in intercept but no differences in slope.\nPanel B: Between person differences in slope but no differences in intercept.\nPanel C: Between person differences in intercepts and slopes.\nBetween and Within Person Variance\n\n\n\nPanel A: Between person differences in level (intercept in LGC literature) but no between person differences in variability.\nPanel B: No between person differences in level (intercept) or variability, but the amount of variability in these trajectories is greater than Panel A. Panel C: No between person differences in level (intercept) but there are between person differences in variability.\nMain Effects and Interactions (Cross Sectional vs. Over Time)\nImagine we re-test the main and interaction effects from a cross-sectional study several times. If the results are stable across time, what would they look like?\nMain Effect\nGroup A (difficult, specific goals) higher performance than group B (vague goals).\n\n\n\n\n\n\nInteraction\nFor males: Group A (difficult, specific goals) higher performance than group B (vague goals). For females: Group B (vague goals) higher performance than group B (difficult, specific goals).\n\n\n\nInteraction and Main Effect\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-07-04-longitudinal-plotting/longitudinal-plotting_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-02-column-names-as-parameters/",
    "title": "Column Names As Parameters",
    "description": {},
    "author": [],
    "date": "2018-06-02",
    "categories": [],
    "contents": "\nI always forget how to use column names as function parameters, so here is an example.\nFunction with no column name parameters\nFunction:\nSelect columns\nReplace the Jimmy and James ‘v_1’ values with 99\n\n\nlibrary(tidyverse)\n\ndish <- data.frame(\n  'person' = c('jimmy', 'james', 'johnny'),\n  'v_1' = c(rnorm(3, 0, 1)),\n  'v_2' = c(rnorm(3, 10, 5)),\n  'v_3' = c(rnorm(3, 50, 10)),\n  'v_4' = c(rnorm(3, 25, 15))\n)\n\nmini <- dish %>%\n  select(person, v_1, v_2)\n\nmini[mini$person == 'jimmy', 2] <- 99\nmini[mini$person == 'james', 2] <- 99\n\n\n\nThe original data:\n\n  person        v_1       v_2      v_3      v_4\n1  jimmy  0.4543694 14.593479 62.66191 10.95676\n2  james -0.4837649 10.126899 48.31108 23.01771\n3 johnny -0.9562436  6.407953 49.09710 26.37863\n\nWhat we changed it to:\n\n  person        v_1       v_2\n1  jimmy 99.0000000 14.593479\n2  james 99.0000000 10.126899\n3 johnny -0.9562436  6.407953\n\nHere is the function equivalent:\n\n\nimpute_99 <- function(data){\n  \n  \n  new_data <- data %>%\n    select(person, v_1, v_2)\n  \n  new_data[new_data$person == 'jimmy', 2] <- 99\n  new_data[new_data$person == 'james', 2] <- 99\n  \n  return(new_data)\n  \n  \n}\n\n\n\nOur result:\n\n\nadjusted_data <- impute_99(dish)\nadjusted_data\n\n\n  person        v_1       v_2\n1  jimmy 99.0000000 14.593479\n2  james 99.0000000 10.126899\n3 johnny -0.9562436  6.407953\n\nFunction with column names as parameters\nNow, what if we want to use specific column names as parameters in our function? We could change the function to:\n\n\nimpute_99_column_specific <- function(data, column1, column2){\n  \n  new_data <- data %>%\n    select(person, column1, column2)\n  \n  new_data[new_data$person == 'jimmy', 2] <- 99 # column1 change\n  new_data[new_data$person == 'james', 2] <- 99 # column2 change\n  \n  return(new_data)\n  \n}\n\n\n\nwhere ‘column1’ and ‘column2’ can be replaced by specific names. Here is where I usually get confused, the following code does not work:\n\n\ncool_data <- impute_99_column_specific(dish, v_1, v_2)\n\n\n\nFortunately the correction is simple, just put quotes around the column names:\n\n\ncool_data <- impute_99_column_specific(dish, 'v_1', 'v_2')\ncool_data\n\n\n  person        v_1       v_2\n1  jimmy 99.0000000 14.593479\n2  james 99.0000000 10.126899\n3 johnny -0.9562436  6.407953\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-05-spline-modeling/",
    "title": "Spline Modeling",
    "description": {},
    "author": [],
    "date": "2018-05-05",
    "categories": [],
    "contents": "\nA few spline models (also known as piecewise models). As in previous posts, ‘affect’ is the name given to values of \\(y\\) throughout.\n1) Growth and Even More Growth\nA model that captures a process that increases initially and then increases at an even greater rate once it reaches time point 5. The data generating process:\n\\[\\begin{equation}\ny_{it} = \n  \\begin{cases}\n  4 + 0.3t + error_{t}, & \\text{if time < 5}\\\\\n  8 + 0.9t + error_{t}, & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\\]\nThe data generating code and plot\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggplot2)\nlibrary(MASS)\n\nN <- 400\ntime <- 10\n\nintercept_1 <- 4\nintercept_2 <- 8\n\ngrowth1 <- 0.3\ngrowth2 <- 0.9\n\n\ndf_matrix <- matrix(, ncol = 3, nrow = N*time)\n\n\ncount <- 0\n\nfor(i in 1:N){\n  \n  unob_het_y <- rnorm(1,0,1)\n  \n  \n  for(j in 1:time){\n    \n    count <- count + 1\n    \n    if(j < 5){\n    df_matrix[count, 1] <- i\n    df_matrix[count, 2] <- j\n    df_matrix[count, 3] <- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1)\n    \n    }else{\n      \n      df_matrix[count, 1] <- i\n      df_matrix[count, 2] <- j\n      df_matrix[count, 3] <- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1)\n      \n      \n    }\n  }\n  \n}\n\ndf <- data.frame(df_matrix)\n\nnames(df) <- c('id', 'time', 'affect')\n\ndf1 <- df %>%\n  filter(time < 5)\n\ndf2 <- df %>%\n  filter(time >= 5)\n\ndf_sum1 <- df1 %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\ndf_sum2 <- df2 %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\nggplot() + \n  geom_point(data = df1, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df1, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_point(data = df2, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df2, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df_sum1, aes(x = time, y = affect)) + \n  geom_line(data = df_sum2, aes(x = time, y = affect))\n\n\n\n\nEstimating the parameters using SEM:\n\n\nlibrary(lavaan)\n\ndf_wide <- reshape(df, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\nspline_string <- [1687 chars quoted with ''']\n\nspline_model <- growth(spline_string, data = df_wide)\nsummary(spline_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 83 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                40.920\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.816\n\nModel Test Baseline Model:\n\n  Test statistic                              1854.037\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.005\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6174.770\n  Loglikelihood unrestricted model (H1)      -6154.310\n                                                      \n  Akaike (AIC)                               12379.539\n  Bayesian (BIC)                             12439.411\n  Sample-size adjusted Bayesian (BIC)        12391.815\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.021\n  P-value RMSEA <= 0.05                          1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  level1_affect =~                                    \n    affect.1          1.000                           \n    affect.2          1.000                           \n    affect.3          1.000                           \n    affect.4          1.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         0.000                           \n  level2_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          1.000                           \n    affect.6          1.000                           \n    affect.7          1.000                           \n    affect.8          1.000                           \n    affect.9          1.000                           \n    affect.10         1.000                           \n  slope1_affect =~                                    \n    affect.1          1.000                           \n    affect.2          2.000                           \n    affect.3          3.000                           \n    affect.4          4.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         0.000                           \n  slope2_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          5.000                           \n    affect.6          6.000                           \n    affect.7          7.000                           \n    affect.8          8.000                           \n    affect.9          9.000                           \n    affect.10        10.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  level1_affect ~~                                    \n    level2_affect     0.941    0.183    5.147    0.000\n    slope1_affect    -0.093    0.052   -1.783    0.075\n    slope2_affect     0.006    0.021    0.292    0.770\n  level2_affect ~~                                    \n    slope1_affect     0.041    0.052    0.802    0.423\n    slope2_affect    -0.028    0.037   -0.748    0.454\n  slope1_affect ~~                                    \n    slope2_affect    -0.004    0.006   -0.581    0.561\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n   .affect.7          0.000                           \n   .affect.8          0.000                           \n   .affect.9          0.000                           \n   .affect.10         0.000                           \n    level1_affect     3.929    0.083   47.407    0.000\n    level2_affect     7.849    0.107   73.630    0.000\n    slope1_affect     0.294    0.024   12.166    0.000\n    slope2_affect     0.909    0.013   72.612    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    lvl1_ff           1.234    0.199    6.198    0.000\n    lvl2_ff           1.135    0.336    3.375    0.001\n    slp1_ff           0.032    0.018    1.839    0.066\n    slp2_ff           0.005    0.005    1.068    0.285\n   .affct.1 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.2 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.3 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.4 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.5 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.6 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.7 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.8 (rs_v)    1.009    0.029   34.641    0.000\n   .affct.9 (rs_v)    1.009    0.029   34.641    0.000\n   .affc.10 (rs_v)    1.009    0.029   34.641    0.000\n\nThe structure of the basis coefficients is the important piece that allows us to capture the change in slope:\n\n\n'\n# latent slope for first half basis coefficients\n\nslope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10\n\n# latent slope for second half basis coefficients\n\nslope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10\n\n'\n\n\n\n2) Growth and Negative Growth\nA model that captures a process that goes up and then goes down. The data generating process:\n\\[\\begin{equation}\ny_{it} = \n  \\begin{cases}\n  4 + 0.5t + error_{t}, & \\text{if time < 5}\\\\\n  4 - 0.5t + error_{t}, & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\\]\nThe data generating code and plot\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggplot2)\nlibrary(MASS)\n\nN <- 400\ntime <- 10\n\nintercept_1 <- 4\nintercept_2 <- 4\n\ngrowth1 <- 0.8\ngrowth2 <- -0.8\n\ndf_matrix_b <- matrix(, ncol = 3, nrow = N*time)\n\n\ncount <- 0\n\nfor(i in 1:N){\n  \n  unob_het_y <- rnorm(1,0,1)\n  \n  \n  for(j in 1:time){\n    \n    count <- count + 1\n    \n    if(j < 5){\n      df_matrix_b[count, 1] <- i\n      df_matrix_b[count, 2] <- j\n      df_matrix_b[count, 3] <- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1)\n      \n    }else{\n      \n      df_matrix_b[count, 1] <- i\n      df_matrix_b[count, 2] <- j\n      df_matrix_b[count, 3] <- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1)\n      \n      \n    }\n  }\n  \n}\n\ndf_b <- data.frame(df_matrix_b)\n\nnames(df_b) <- c('id', 'time', 'affect')\n\ndf1_b <- df_b %>%\n  filter(time < 5)\n\ndf2_b <- df_b %>%\n  filter(time >= 5)\n\ndf_sum1_b <- df1_b %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\ndf_sum2_b <- df2_b %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\nggplot() + \n  geom_point(data = df1_b, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df1_b, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_point(data = df2_b, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df2_b, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df_sum1_b, aes(x = time, y = affect)) + \n  geom_line(data = df_sum2_b, aes(x = time, y = affect))\n\n\n\n\nEstimating the parameters using SEM:\n\n\nlibrary(lavaan)\n\n\ndf_wide_b <- reshape(df_b, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\nspline_string_b <- [1687 chars quoted with ''']\n\nspline_model_b <- growth(spline_string_b, data = df_wide_b)\nsummary(spline_model_b, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 79 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                60.348\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.150\n\nModel Test Baseline Model:\n\n  Test statistic                              1931.249\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.995\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6219.675\n  Loglikelihood unrestricted model (H1)      -6189.501\n                                                      \n  Akaike (AIC)                               12469.349\n  Bayesian (BIC)                             12529.221\n  Sample-size adjusted Bayesian (BIC)        12481.625\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.023\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.041\n  P-value RMSEA <= 0.05                          0.995\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.059\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  level1_affect =~                                    \n    affect.1          1.000                           \n    affect.2          1.000                           \n    affect.3          1.000                           \n    affect.4          1.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         0.000                           \n  level2_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          1.000                           \n    affect.6          1.000                           \n    affect.7          1.000                           \n    affect.8          1.000                           \n    affect.9          1.000                           \n    affect.10         1.000                           \n  slope1_affect =~                                    \n    affect.1          1.000                           \n    affect.2          2.000                           \n    affect.3          3.000                           \n    affect.4          4.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         0.000                           \n  slope2_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          5.000                           \n    affect.6          6.000                           \n    affect.7          7.000                           \n    affect.8          8.000                           \n    affect.9          9.000                           \n    affect.10        10.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  level1_affect ~~                                    \n    level2_affect     1.110    0.180    6.151    0.000\n    slope1_affect    -0.029    0.049   -0.594    0.553\n    slope2_affect     0.016    0.020    0.788    0.431\n  level2_affect ~~                                    \n    slope1_affect    -0.041    0.046   -0.893    0.372\n    slope2_affect     0.036    0.034    1.037    0.300\n  slope1_affect ~~                                    \n    slope2_affect    -0.003    0.005   -0.576    0.565\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n   .affect.7          0.000                           \n   .affect.8          0.000                           \n   .affect.9          0.000                           \n   .affect.10         0.000                           \n    level1_affect     3.886    0.084   46.497    0.000\n    level2_affect     3.740    0.103   36.398    0.000\n    slope1_affect     0.813    0.023   36.105    0.000\n    slope2_affect    -0.778    0.012  -65.684    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    lvl1_ff           1.241    0.203    6.124    0.000\n    lvl2_ff           0.721    0.315    2.289    0.022\n    slp1_ff          -0.004    0.016   -0.290    0.772\n    slp2_ff          -0.003    0.004   -0.709    0.478\n   .affct.1 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.2 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.3 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.4 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.5 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.6 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.7 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.8 (rs_v)    1.035    0.030   34.641    0.000\n   .affct.9 (rs_v)    1.035    0.030   34.641    0.000\n   .affc.10 (rs_v)    1.035    0.030   34.641    0.000\n\nNotice that the string syntax is the exact same because the process changes at the same point in time, it does not matter if the process changes to ‘more positive’ or ‘more negative.’\n3) Negative Growth, Growth, and Negative Growth\nNow a process that goes down, goes up, and then goes back down. The data generating process:\n\\[\\begin{equation}\ny_{it} = \n  \\begin{cases}\n  4 - 0.5t + error_{t}, & \\text{if time < 5}\\\\\n  4 + 0.5t + error_{t}, & \\text{if 5 < time < 10}\\\\\n  4 - 0.5t + error_{t}, & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\\]\nThe data generating code and plot\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(ggplot2)\nlibrary(MASS)\n\nN <- 400\ntime <- 15\n\nintercept_1 <- 4\nintercept_2 <- 4\nintercept_3 <- 4\n\ngrowth1 <- -0.5\ngrowth2 <- 0.5\ngrowth3 <- -0.5\n\n\ndf_matrix_c <- matrix(, ncol = 3, nrow = N*time)\n\n\ncount <- 0\n\nfor(i in 1:N){\n  \n  unob_het_y <- rnorm(1,0,1)\n  \n  \n  for(j in 1:time){\n    \n    count <- count + 1\n    \n    if(j < 5){\n      df_matrix_c[count, 1] <- i\n      df_matrix_c[count, 2] <- j\n      df_matrix_c[count, 3] <- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1)\n      \n    }else if(j >= 5 && j < 10){\n      \n      df_matrix_c[count, 1] <- i\n      df_matrix_c[count, 2] <- j\n      df_matrix_c[count, 3] <- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1)\n      \n      \n    }else{\n      \n      df_matrix_c[count, 1] <- i\n      df_matrix_c[count, 2] <- j\n      df_matrix_c[count, 3] <- intercept_3 + growth3*j + unob_het_y + rnorm(1,0,1)\n      \n    }\n  }\n  \n}\n\ndf_c <- data.frame(df_matrix_c)\n\nnames(df_c) <- c('id', 'time', 'affect')\n\ndf1_c <- df_c %>%\n  filter(time < 5)\n\ndf2_c <- df_c %>%\n  filter(time >= 5 & time < 10)\n\ndf3_c <- df_c %>%\n  filter(time >= 10)\n\ndf_sum1_c <- df1_c %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\ndf_sum2_c <- df2_c %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\ndf_sum3_c <- df3_c %>%\n  group_by(time) %>%\n  summarise(\n    affect = mean(affect)\n  )\n\nggplot() + \n  geom_point(data = df1_c, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df1_c, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_point(data = df2_c, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df2_c, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df_sum1_c, aes(x = time, y = affect)) + \n  geom_line(data = df_sum2_c, aes(x = time, y = affect)) + \n  geom_point(data = df3_c, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df3_c, aes(x = time, y = affect, group = id), color = 'gray85') + \n  geom_line(data = df_sum3_c, aes(x = time, y = affect))\n\n\n\n\nNow estimate the parameters using SEM:\n\n\nlibrary(lavaan)\n\ndf_wide_c <- reshape(df_c, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\nspline_string_c <- [2843 chars quoted with ''']\n\nspline_model_c <- growth(spline_string_c, data = df_wide_c)\nsummary(spline_model_c, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 167 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        42\n  Number of equality constraints                     9\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               120.754\n  Degrees of freedom                               102\n  P-value (Chi-square)                           0.099\n\nModel Test Baseline Model:\n\n  Test statistic                              3055.629\n  Degrees of freedom                               105\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9115.240\n  Loglikelihood unrestricted model (H1)      -9054.863\n                                                      \n  Akaike (AIC)                               18296.479\n  Bayesian (BIC)                             18428.198\n  Sample-size adjusted Bayesian (BIC)        18323.486\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.021\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.035\n  P-value RMSEA <= 0.05                          1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  level1_affect =~                                    \n    affect.1          1.000                           \n    affect.2          1.000                           \n    affect.3          1.000                           \n    affect.4          1.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         0.000                           \n    affect.11         0.000                           \n    affect.12         0.000                           \n    affect.13         0.000                           \n    affect.14         0.000                           \n    affect.15         0.000                           \n  level2_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          1.000                           \n    affect.6          1.000                           \n    affect.7          1.000                           \n    affect.8          1.000                           \n    affect.9          1.000                           \n    affect.10         0.000                           \n    affect.11         0.000                           \n    affect.12         0.000                           \n    affect.13         0.000                           \n    affect.14         0.000                           \n    affect.15         0.000                           \n  level3_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         1.000                           \n    affect.11         1.000                           \n    affect.12         1.000                           \n    affect.13         1.000                           \n    affect.14         1.000                           \n    affect.15         1.000                           \n  slope1_affect =~                                    \n    affect.1          1.000                           \n    affect.2          2.000                           \n    affect.3          3.000                           \n    affect.4          4.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10         0.000                           \n    affect.11         0.000                           \n    affect.12         0.000                           \n    affect.13         0.000                           \n    affect.14         0.000                           \n    affect.15         0.000                           \n  slope2_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          5.000                           \n    affect.6          6.000                           \n    affect.7          7.000                           \n    affect.8          8.000                           \n    affect.9          9.000                           \n    affect.10         0.000                           \n    affect.11         0.000                           \n    affect.12         0.000                           \n    affect.13         0.000                           \n    affect.14         0.000                           \n    affect.15         0.000                           \n  slope3_affect =~                                    \n    affect.1          0.000                           \n    affect.2          0.000                           \n    affect.3          0.000                           \n    affect.4          0.000                           \n    affect.5          0.000                           \n    affect.6          0.000                           \n    affect.7          0.000                           \n    affect.8          0.000                           \n    affect.9          0.000                           \n    affect.10        10.000                           \n    affect.11        11.000                           \n    affect.12        12.000                           \n    affect.13        13.000                           \n    affect.14        14.000                           \n    affect.15        15.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  level1_affect ~~                                    \n    level2_affect     1.113    0.209    5.311    0.000\n    level3_affect     1.354    0.280    4.844    0.000\n    slope1_affect    -0.046    0.051   -0.902    0.367\n    slope2_affect    -0.004    0.025   -0.169    0.866\n    slope3_affect    -0.023    0.020   -1.142    0.254\n  level2_affect ~~                                    \n    level3_affect     0.729    0.399    1.828    0.067\n    slope1_affect    -0.062    0.058   -1.074    0.283\n    slope2_affect     0.076    0.055    1.391    0.164\n    slope3_affect     0.021    0.030    0.725    0.468\n  level3_affect ~~                                    \n    slope1_affect    -0.074    0.077   -0.963    0.336\n    slope2_affect     0.048    0.050    0.960    0.337\n    slope3_affect    -0.028    0.060   -0.462    0.644\n  slope1_affect ~~                                    \n    slope2_affect     0.005    0.007    0.626    0.531\n    slope3_affect     0.003    0.006    0.511    0.609\n  slope2_affect ~~                                    \n    slope3_affect    -0.004    0.004   -1.151    0.250\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n   .affect.7          0.000                           \n   .affect.8          0.000                           \n   .affect.9          0.000                           \n   .affect.10         0.000                           \n   .affect.11         0.000                           \n   .affect.12         0.000                           \n   .affect.13         0.000                           \n   .affect.14         0.000                           \n   .affect.15         0.000                           \n    level1_affect     3.876    0.083   46.673    0.000\n    level2_affect     3.993    0.122   32.835    0.000\n    level3_affect     3.964    0.163   24.272    0.000\n    slope1_affect    -0.491    0.024  -20.791    0.000\n    slope2_affect     0.496    0.015   32.376    0.000\n    slope3_affect    -0.501    0.012  -41.367    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    lvl1_ff           1.167    0.201    5.816    0.000\n    lvl2_ff           0.505    0.448    1.127    0.260\n    lvl3_ff           1.470    0.815    1.804    0.071\n    slp1_ff           0.011    0.017    0.666    0.505\n    slp2_ff          -0.012    0.007   -1.658    0.097\n    slp3_ff           0.001    0.005    0.253    0.801\n   .affct.1 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.2 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.3 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.4 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.5 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.6 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.7 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.8 (rs_v)    1.061    0.032   33.639    0.000\n   .affct.9 (rs_v)    1.061    0.032   33.639    0.000\n   .affc.10 (rs_v)    1.061    0.032   33.639    0.000\n   .affc.11           0.958    0.076   12.593    0.000\n   .affc.12           0.939    0.072   12.961    0.000\n   .affc.13           0.896    0.069   12.940    0.000\n   .affc.14           0.964    0.076   12.657    0.000\n   .affc.15           1.001    0.087   11.487    0.000\n\nAgain, the basis coefficients are the important piece here:\n\n\n'\n\n\n# latent slope for first third basis coefficients\n\nslope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + \n                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \n                 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \n                 0*affect.13 + 0*affect.14 + 0*affect.15\n\n# latent slope for second third basis coefficients\n\nslope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + \n                 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + \n                 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \n                 0*affect.13 + 0*affect.14 + 0*affect.15\n\n# latent slope for final third basis coefficients\n\nslope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 +\n                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \n                 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + \n                 13*affect.13 + 14*affect.14 + 15*affect.15\n\n\n\n'\n\n\n[1] \"\\n\\n\\n# latent slope for first third basis coefficients\\n\\nslope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + \\n                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \\n                 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \\n                 0*affect.13 + 0*affect.14 + 0*affect.15\\n\\n# latent slope for second third basis coefficients\\n\\nslope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + \\n                 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + \\n                 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \\n                 0*affect.13 + 0*affect.14 + 0*affect.15\\n\\n# latent slope for final third basis coefficients\\n\\nslope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 +\\n                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \\n                 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + \\n                 13*affect.13 + 14*affect.14 + 15*affect.15\\n\\n\\n\\n\"\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-05-05-spline-modeling/spline-modeling_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-15-latent-growth-curves/",
    "title": "Latent Growth Curves",
    "description": {},
    "author": [],
    "date": "2018-04-15",
    "categories": [],
    "contents": "\nLatent Growth Curves\nI will progress through three models: linear, quadratic growth, and latent basis. In every example I use a sample of 400, 6 time points, and ‘affect’ as the variable of interest.\nDon’t forget that multiplying by time\n\\(0.6t\\)\nis different from describing over time\n\\(0.6_t\\).\n1) Linear\nThe data generating process:\n\\[\\begin{equation}\ny_{it} = 4 - 0.6t + e_{t}\n\\end{equation}\\]\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(MASS)\n\nN <- 400\ntime <- 6\n\nintercept <- 4\nlinear_growth <- -0.6\n\ndf_matrix <- matrix(, nrow = N*time, ncol = 3)\n\ncount <- 0\n\nfor(i in 1:400){\n  \n  unob_het_affect <- rnorm(1,0,3)\n\n  \n  for(j in 1:6){\n    \n    count <- count + 1\n    \n    if(j == 1){\n      \n      df_matrix[count, 1] <- i\n      df_matrix[count, 2] <- j\n      df_matrix[count, 3] <- intercept + unob_het_affect + rnorm(1,0,1)\n    }else{\n      \n      \n      df_matrix[count, 1] <- i\n      df_matrix[count, 2] <- j\n      df_matrix[count, 3] <- intercept + linear_growth*j + unob_het_affect + rnorm(1,0,1)\n      \n    }\n    \n    \n    \n  }\n  \n  \n}\n\ndf <- data.frame(df_matrix)\nnames(df) <- c('id', 'time', 'affect')\n\nrandom_ids <- sample(df$id, 5)\n\nrandom_df <- df %>%\n  filter(id %in% random_ids)\n  \n\nggplot(df, aes(x = time, y = affect, group = id)) + \n  geom_point(color = 'gray85') + \n  geom_line(color = 'gray85') + \n  geom_point(data = random_df, aes(x = time, y = affect, group = id), color = 'blue') + \n  geom_line(data = random_df, aes(x = time, y = affect, group = id), color = 'blue') \n\n\n\n\nEstimating the model:\nFormatting the data:\n\n\ndf_wide <- reshape(df, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\n\nFirst, an intercept only (no change) model:\n\n\nlibrary(lavaan)\n\nno_change_string <- '\n\n# Latent intercept factor\n\nintercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6\n\n# Mean and variance of latent intercept factor\n\nintercept_affect ~~ intercept_affect\n\n# Fix observed variable means to 0\n\naffect.1 ~ 0\naffect.2 ~ 0\naffect.3 ~ 0\naffect.4 ~ 0\naffect.5 ~ 0\naffect.6 ~ 0\n\n# Constrain residual (error) variance of observed variables to equality across time\n\naffect.1 ~~ res_var*affect.1\naffect.2 ~~ res_var*affect.2\naffect.3 ~~ res_var*affect.3\naffect.4 ~~ res_var*affect.4\naffect.5 ~~ res_var*affect.5\naffect.6 ~~ res_var*affect.6\n\n\n'\n\nno_change_model <- growth(no_change_string, data = df_wide)\nsummary(no_change_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                              2073.804\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4042.863\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.491\n  Tucker-Lewis Index (TLI)                       0.682\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5203.688\n  Loglikelihood unrestricted model (H1)      -4166.786\n                                                      \n  Akaike (AIC)                               10413.376\n  Bayesian (BIC)                             10425.350\n  Sample-size adjusted Bayesian (BIC)        10415.831\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.462\n  90 Percent confidence interval - lower         0.445\n  90 Percent confidence interval - upper         0.479\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.194\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect =~                                    \n    affect.1             1.000                           \n    affect.2             1.000                           \n    affect.3             1.000                           \n    affect.4             1.000                           \n    affect.5             1.000                           \n    affect.6             1.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n    intercept_ffct    2.067    0.153   13.509    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    intrcp_           8.914    0.662   13.460    0.000\n   .affct.1 (rs_v)    2.698    0.085   31.623    0.000\n   .affct.2 (rs_v)    2.698    0.085   31.623    0.000\n   .affct.3 (rs_v)    2.698    0.085   31.623    0.000\n   .affct.4 (rs_v)    2.698    0.085   31.623    0.000\n   .affct.5 (rs_v)    2.698    0.085   31.623    0.000\n   .affct.6 (rs_v)    2.698    0.085   31.623    0.000\n\nNow, a linear growth model centered at time point 1. The intercept factor estimate, therefore, is the estimated average affect at time 1.\n\n\nlibrary(lavaan)\n\nlinear_change_string <- '\n\n# Latent intercept and slope factors\n\nintercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6\nslope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4 + 4*affect.5 + 5*affect.6\n\n# Mean and variance of latent factors\n\nintercept_affect ~~ intercept_affect\nslope_affect ~~ slope_affect\n\n# Covariance between latent factors\n\nintercept_affect ~~ slope_affect\n\n# Fix observed variable means to 0\n\naffect.1 ~ 0\naffect.2 ~ 0\naffect.3 ~ 0\naffect.4 ~ 0\naffect.5 ~ 0\naffect.6 ~ 0\n\n# Constrain residual (error) variance of observed variables to equality across time\n\naffect.1 ~~ res_var*affect.1\naffect.2 ~~ res_var*affect.2\naffect.3 ~~ res_var*affect.3\naffect.4 ~~ res_var*affect.4\naffect.5 ~~ res_var*affect.5\naffect.6 ~~ res_var*affect.6\n\n\n'\n\nlinear_change_model <- growth(linear_change_string, data = df_wide)\nsummary(linear_change_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 48 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                95.218\n  Degrees of freedom                                21\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4042.863\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.982\n  Tucker-Lewis Index (TLI)                       0.987\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4214.395\n  Loglikelihood unrestricted model (H1)      -4166.786\n                                                      \n  Akaike (AIC)                                8440.790\n  Bayesian (BIC)                              8464.738\n  Sample-size adjusted Bayesian (BIC)         8445.700\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.094\n  90 Percent confidence interval - lower         0.075\n  90 Percent confidence interval - upper         0.114\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.034\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect =~                                    \n    affect.1             1.000                           \n    affect.2             1.000                           \n    affect.3             1.000                           \n    affect.4             1.000                           \n    affect.5             1.000                           \n    affect.6             1.000                           \n  slope_affect =~                                        \n    affect.1             0.000                           \n    affect.2             1.000                           \n    affect.3             2.000                           \n    affect.4             3.000                           \n    affect.5             4.000                           \n    affect.6             5.000                           \n\nCovariances:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect ~~                                    \n    slope_affect         0.049    0.036    1.368    0.171\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n    intercept_ffct    3.806    0.154   24.653    0.000\n    slope_affect     -0.696    0.011  -61.197    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    intrcp_           8.992    0.674   13.336    0.000\n    slp_ffc          -0.007    0.004   -1.712    0.087\n   .affct.1 (rs_v)    1.030    0.036   28.284    0.000\n   .affct.2 (rs_v)    1.030    0.036   28.284    0.000\n   .affct.3 (rs_v)    1.030    0.036   28.284    0.000\n   .affct.4 (rs_v)    1.030    0.036   28.284    0.000\n   .affct.5 (rs_v)    1.030    0.036   28.284    0.000\n   .affct.6 (rs_v)    1.030    0.036   28.284    0.000\n\ninspect(linear_change_model, 'cov.lv')\n\n\n                 intrc_ slp_ff\nintercept_affect  8.992       \nslope_affect      0.049 -0.007\n\nThis model does an adequate job recovering the intercept and slope parameters.\nIf I wanted to center the model at time point 3 the latent intercept term would be interpreted as the estimated average affect at time 3 and the syntax would change to:\n\n\n'\nslope_affect =~ -2*affect.1 + -1*affect.2 + 0*affect.3 + 1*affect.4 + 2*affect.5 + 3*affect.6\n\n'\n\n\n\n2) Quadratic\nThe data generating process:\n\\[\\begin{equation}\ny_{it} = 4 + 0.2t + 0.7t^2 + e_{t}\n\\end{equation}\\]\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(MASS)\n\nN <- 400\ntime <- 6\n\n\n\nintercept_mu <- 4\nlinear_growth2 <- 0.2\nquad_growth <- 0.7\n\ndf_matrix2 <- matrix(, nrow = N*time, ncol = 3)\n\ncount <- 0\n\nfor(i in 1:400){\n  \n  unob_het_affect <- rnorm(1,0,3)\n\n  \n  for(j in 1:6){\n    \n    count <- count + 1\n    \n    if(j == 1){\n      \n      df_matrix2[count, 1] <- i\n      df_matrix2[count, 2] <- j\n      df_matrix2[count, 3] <- intercept + rnorm(1,0,1) + rnorm(1,0,1)\n    }else{\n      \n      \n      df_matrix2[count, 1] <- i\n      df_matrix2[count, 2] <- j\n      df_matrix2[count, 3] <- intercept + linear_growth2*j + quad_growth*(j^2) + unob_het_affect + rnorm(1,0,1)\n      \n    }\n    \n    \n    \n  }\n  \n  \n}\n\ndf2 <- data.frame(df_matrix2)\nnames(df2) <- c('id', 'time', 'affect')\n\nrandom_ids2 <- sample(df2$id, 5)\n\nrandom_df2 <- df2 %>%\n  filter(id %in% random_ids2)\n  \n\nggplot(df2, aes(x = time, y = affect, group = id)) + \n  geom_point(color = 'gray85') + \n  geom_line(color = 'gray85') + \n  geom_point(data = random_df2, aes(x = time, y = affect, group = id), color = 'blue') + \n  geom_line(data = random_df2, aes(x = time, y = affect, group = id), color = 'blue') + \n  theme_wsj()\n\n\n\n\nEstimating the model:\nQuadratic growth model:\n\n\ndf_wide2 <- reshape(df2, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\nlibrary(lavaan)\n\nquad_change_string <- [1012 chars quoted with ''']\n\nquad_change_model <- growth(quad_change_string, data = df_wide2)\nsummary(quad_change_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 82 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               609.350\n  Degrees of freedom                                17\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3203.346\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.814\n  Tucker-Lewis Index (TLI)                       0.836\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4603.247\n  Loglikelihood unrestricted model (H1)      -4298.572\n                                                      \n  Akaike (AIC)                                9226.494\n  Bayesian (BIC)                              9266.409\n  Sample-size adjusted Bayesian (BIC)         9234.678\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.295\n  90 Percent confidence interval - lower         0.275\n  90 Percent confidence interval - upper         0.315\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.223\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                       Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect =~                                     \n    affect.1              1.000                           \n    affect.2              1.000                           \n    affect.3              1.000                           \n    affect.4              1.000                           \n    affect.5              1.000                           \n    affect.6              1.000                           \n  slope_affect =~                                         \n    affect.1              0.000                           \n    affect.2              1.000                           \n    affect.3              2.000                           \n    affect.4              3.000                           \n    affect.5              4.000                           \n    affect.6              5.000                           \n  quad_slope_affect =~                                    \n    affect.1              0.000                           \n    affect.2              1.000                           \n    affect.3              4.000                           \n    affect.4              9.000                           \n    affect.5             16.000                           \n    affect.6             25.000                           \n\nCovariances:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect ~~                                    \n    slope_affect         0.897    0.148    6.066    0.000\n    quad_slop_ffct      -0.136    0.024   -5.748    0.000\n  slope_affect ~~                                        \n    quad_slop_ffct      -0.461    0.049   -9.468    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n    intercept_ffct    4.226    0.069   60.965    0.000\n    slope_affect      2.163    0.103   20.974    0.000\n    quad_slop_ffct    0.610    0.017   36.907    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    intrcp_           0.626    0.146    4.288    0.000\n    slp_ffc           3.107    0.304   10.205    0.000\n    qd_slp_           0.067    0.008    8.463    0.000\n   .affct.1 (rs_v)    1.579    0.064   24.495    0.000\n   .affct.2 (rs_v)    1.579    0.064   24.495    0.000\n   .affct.3 (rs_v)    1.579    0.064   24.495    0.000\n   .affct.4 (rs_v)    1.579    0.064   24.495    0.000\n   .affct.5 (rs_v)    1.579    0.064   24.495    0.000\n   .affct.6 (rs_v)    1.579    0.064   24.495    0.000\n\nThis model recovers the intercept and quadratic parameters but not the linear growth parameter.\n3) Latent Basis\nThis model allows us to see where a majority of the change occurs in the process. For example, does more change occur between time points 2 and 3 or 5 and 6? In this model we are not trying to recover the parameters, but describe the change process in detail.\nData generating process:\nTime 1 - Time 3: \\[\\begin{equation}\ny_{it} = 4 + 0.2t + e_{t}\n\\end{equation}\\]\nTime 4 - Time 6: \\[\\begin{equation}\ny_{it} = 4 + 0.8t + e_{t}\n\\end{equation}\\]\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(MASS)\n\nN <- 400\ntime <- 6\n\n\nintercept_mu <- 4\ngrowth_1 <- 0.2\ngrowth_2 <- 0.8\n\n\ndf_matrix3 <- matrix(, nrow = N*time, ncol = 3)\n\ncount <- 0\n\nfor(i in 1:400){\n  \n  unob_het_affect <- rnorm(1,0,3)\n  \n  \n  for(j in 1:6){\n    \n    count <- count + 1\n    \n    if(j < 4){\n      \n      df_matrix3[count, 1] <- i\n      df_matrix3[count, 2] <- j\n      df_matrix3[count, 3] <- intercept + growth_1*j + unob_het_affect + rnorm(1,0,1)\n      \n    }else{\n      \n      \n      df_matrix3[count, 1] <- i\n      df_matrix3[count, 2] <- j\n      df_matrix3[count, 3] <- intercept + growth_2*j + unob_het_affect + rnorm(1,0,1)\n      \n    }\n    \n    \n    \n  }\n  \n  \n}\n\ndf3 <- data.frame(df_matrix3)\nnames(df3) <- c('id', 'time', 'affect')\n\nrandom_ids3 <- sample(df3$id, 5)\n\nrandom_df3 <- df3 %>%\n  filter(id %in% random_ids3)\n  \n\nggplot(df3, aes(x = time, y = affect, group = id)) + \n  geom_point(color = 'gray85') + \n  geom_line(color = 'gray85') + \n  geom_point(data = random_df3, aes(x = time, y = affect, group = id), color = 'blue') + \n  geom_line(data = random_df3, aes(x = time, y = affect, group = id), color = 'blue')\n\n\n\n\nEstimating the model:\nLatent basis:\nSimilar to a linear growth model but we freely estimate the intermediate basis coefficients. Remember to constrain the first basis coefficient to zero and the last to 1.\n\n\ndf_wide3 <- reshape(df3, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\nlibrary(lavaan)\n\nlb_string <- '\n\n# Latent intercept and slope terms with intermediate time points freely estimated\n\nintercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6\nslope_affect =~ 0*affect.1 + bc1*affect.2 + bc2*affect.3 + bc3*affect.4 + bc4*affect.5 + 1*affect.6\n\n# Mean and variance of latent factors\n\nintercept_affect ~~ intercept_affect\nslope_affect ~~ slope_affect\n\n# Covariance between latent factors\n\nintercept_affect ~~ slope_affect\n\n# Fix observed variable means to 0\n\naffect.1 ~ 0\naffect.2 ~ 0\naffect.3 ~ 0\naffect.4 ~ 0\naffect.5 ~ 0\naffect.6 ~ 0\n\n# Constrain residual (error) variance of observed variables to equality across time\n\naffect.1 ~~ res_var*affect.1\naffect.2 ~~ res_var*affect.2\naffect.3 ~~ res_var*affect.3\naffect.4 ~~ res_var*affect.4\naffect.5 ~~ res_var*affect.5\naffect.6 ~~ res_var*affect.6\n\n\n'\n\nlb_model <- growth(lb_string, data = df_wide3)\nsummary(lb_model, fit.measures = T)\n\n\nlavaan 0.6-9 ended normally after 64 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                17.043\n  Degrees of freedom                                17\n  P-value (Chi-square)                           0.451\n\nModel Test Baseline Model:\n\n  Test statistic                              4190.601\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4202.053\n  Loglikelihood unrestricted model (H1)      -4193.532\n                                                      \n  Akaike (AIC)                                8424.107\n  Bayesian (BIC)                              8464.021\n  Sample-size adjusted Bayesian (BIC)         8432.291\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.003\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.045\n  P-value RMSEA <= 0.05                          0.973\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.018\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect =~                                    \n    affect.1             1.000                           \n    affect.2             1.000                           \n    affect.3             1.000                           \n    affect.4             1.000                           \n    affect.5             1.000                           \n    affect.6             1.000                           \n  slope_affect =~                                        \n    affect.1             0.000                           \n    affect.2 (bc1)       0.056    0.015    3.799    0.000\n    affect.3 (bc2)       0.095    0.015    6.550    0.000\n    affect.4 (bc3)       0.666    0.013   49.749    0.000\n    affect.5 (bc4)       0.827    0.014   58.819    0.000\n    affect.6             1.000                           \n\nCovariances:\n                      Estimate  Std.Err  z-value  P(>|z|)\n  intercept_affect ~~                                    \n    slope_affect        -0.086    0.158   -0.546    0.585\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .affect.1          0.000                           \n   .affect.2          0.000                           \n   .affect.3          0.000                           \n   .affect.4          0.000                           \n   .affect.5          0.000                           \n   .affect.6          0.000                           \n    intercept_ffct    3.921    0.167   23.461    0.000\n    slope_affect      4.647    0.069   67.762    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    intrcp_          10.178    0.746   13.651    0.000\n    slp_ffc          -0.111    0.074   -1.499    0.134\n   .affct.1 (rs_v)    0.996    0.035   28.284    0.000\n   .affct.2 (rs_v)    0.996    0.035   28.284    0.000\n   .affct.3 (rs_v)    0.996    0.035   28.284    0.000\n   .affct.4 (rs_v)    0.996    0.035   28.284    0.000\n   .affct.5 (rs_v)    0.996    0.035   28.284    0.000\n   .affct.6 (rs_v)    0.996    0.035   28.284    0.000\n\nbc1 represents the percentage of change for the average individual between time 1 and 2. bc2 represents the percentage change betwen time 1 and 3, bc4 is the percentage change between time 1 and 5, etc.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-04-15-latent-growth-curves/latent-growth-curves_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-03-30-social-trait-development-computational-model/",
    "title": "Social Trait Development Computational Model",
    "description": {},
    "author": [],
    "date": "2018-03-30",
    "categories": [],
    "contents": "\nI built the following simple computational model for an individual differences class in the Spring of 2018 to demonstrate how to incorporate explantory elements for trait development into a computational framework. This model assumes that an individual’s trait development depends on 1) the environment and 2) interactions with others inside and outside of the individual’s social group. Moreover, the model assumes traits are somewhat stable and exhibit self-similarity across time. The main properties I am trying to capture, therefore, include:\nThe development of a stable trait through interactions with…\na social group\nrandom others\nthe environment\n\nThese properties do not represent what I think of as “true” aspects of trait development (although I think they are important). I use them, instead, to show the translation from verbal concepts to code representations.\nHere is the pseudocode for the model:\nBuild agent\nRandom initial trait value\nPeer group holder (initially 0)\n\nBuild global population of people with the trait (normally distributed)\nTime 1\nPeople in peer group?\nIf yes:\nWhat is their average trait level?\nUse that level to filter who the agent interacts with from the global population\n\nIf no:\nMove to next step\n\n\nSelect person from the global population to interact with\nUniform (-1, 1) = quality of the interaction\nIf it goes well, agent’s trait is influenced by this person\ni.e., If uniform > 0\n\nIf it does not go well, agent keeps own trait\ni.e., If uniform < 0\n\n\n\nEnvironment\nRandom number that influences trait\n\n\nUpdate trait and peer holder\nIf the interaction went well, the new person joins the agent’s social group\n\nIterate\nThe Incomplete Model\nFirst I present the model without a loop in very simple code. We begin with a distribution of the trait in the population.\n\n\nglobal_population <- data.frame(\n  \"People\" = c(1:1000),\n  \"SDO\" = c(rnorm(1000, 100, 10))\n)\n\n\n\nThen I create the agent. I used SDO as my example in class, so that will be the “trait” here. The agent is given an initial value of the trait.\n\n\nagent <- list(\n  SDO = 0,\n  Peeps = NULL\n)\n\n\ninitial_sdo_value <- rnorm(1, 100, 10)\n\nagent[[1]][1] <- initial_sdo_value\n\nagent\n\n\n$SDO\n[1] 106.3572\n\n$Peeps\nNULL\n\nIf the agent has a social group (‘peeps’), then we would take the mean of their trait levels to inform who the agent interacts with from the global population.\n\n\n# if peeps > 0, take the average of their trait level\n\nnum_peeps <- length(agent$Peeps)\n\ntrait_of_peeps <- mean(agent$Peeps)\n\n# use average to bias how I sample the population\n# use filter (+ or - 25 from average)\n\n\n\nBecause this is the first time point, however, the agent does not have a social group. Now we select a person from the global population for our agent to interact with. If our agent had a social group, the social group’s average trait would inform who we select, but again in this case the interaction is random.\n\n\nother <- sample(global_population$SDO, 1)\n\n\n\nThe interaction is good or bad…\n\n\ninteraction_quality <- runif(1, min = -1, max = 1)\n\n\n\nIf the interaction is good, our agent’s trait is influenced by this new individual.\n\n\n# quality good? interaction_quality > 0\n\nnew_sdo <- agent$SDO + (other - agent$SDO)*interaction_quality\n\n# quality bad? interaction quality < 0\n\nnew_sdo <- agent$SDO\n\n\n\nThen we throw in some environmental disturbance for fun\n\n\n# Environment\n\nenvironment_sdo <- sample(c(-20:20), 1)\n\nnew_sdo <- new_sdo + environment_sdo\n\n\n\nand conclude by updating the agent\n\n\n# Update agent\n\nagent$SDO <- c(agent$SDO, new_sdo)\n\n# If the interaction went well, this person goes into friend group. If not, leave them out\n\nagent$Peeps <- c(agent$Peeps, other)\n\nagent\n\n\n$SDO\n[1] 106.3572 108.3572\n\n$Peeps\n[1] 105.9289\n\nThe Full Model\nHere is the full model and a plot of the agent’s trait over time.\n\n\n# - -----------------------------------------------------------------------\n\n\n# - -----------------------------------------------------------------------\n\n\n# - -----------------------------------------------------------------------\n\n\n# - -----------------------------------------------------------------------\n\n\n# - -----------------------------------------------------------------------\n\n\n# - -----------------------------------------------------------------------\n\nlibrary(tidyverse)\n\n# Generate over time\n\ntime_points <- 400\n\n\nglobal_population <- data.frame(\n  \"People\" = c(1:1000),\n  \"SDO\" = c(rnorm(1000, 100, 10))\n)\n\n\nagent <- list(\n  SDO = rep(0,time_points),\n  Peeps = rep(0,time_points)\n)\n\n\ninitial_sdo_value <- rnorm(1, 100, 10)\n\nagent[[1]][1] <- initial_sdo_value\n\nother <- sample(global_population$SDO, 1)\n\nagent[[2]][1] <- other\n\ncount <- 0\n\nfor(i in 2:time_points){\n  \n    count <- count + 1\n    \n    \n    # sample global population and interact with them\n    # filter based on peeps average\n    \n    # need to change this to only use values that are not zero\n    \n    use_non_zero_values <- agent$Peeps[agent$Peeps > 0]\n    \n    use_vals <- mean(use_non_zero_values)\n    filter_top <- use_vals + 20\n    filter_lower <- use_vals - 20\n\n    new_df <- global_population %>%\n      filter(SDO < filter_top & SDO > filter_lower)\n    \n    other <- sample(new_df$SDO, 1)\n    \n    interaction_quality <- runif(1, min = -1, max = 1)\n    \n    # quality good or bad?\n    if(interaction_quality > 0){\n      new_sdo <- agent$SDO[i - 1] + (other - agent$SDO[i - 1])*interaction_quality\n    }else{\n      new_sdo <- agent$SDO[i - 1]\n    }\n    \n    \n    # Environment\n    \n    environment_sdo <- sample(c(-20:20), 1)\n    \n    new_sdo <- new_sdo + environment_sdo\n    \n    \n    # Update agent\n    \n    \n    agent$SDO[i] <- new_sdo\n    \n    if(interaction_quality > 0){\n      agent$Peeps[i] <- other\n    }else{\n      agent$Peeps <- agent$Peeps\n    }\n    \n    \n    \n    \n}\n\n\nlibrary(ggplot2)\nplot_agent <- data.frame(\n  'Agent_SDO' = c(agent$SDO),\n  \"Peeps_SDO\" = c(agent$Peeps),\n  \"Time\" = c(1:time_points)\n)\n\n\nnew_data <- plot_agent %>%\n  filter(Peeps_SDO > 0) %>%\n  gather(Agent_SDO, Peeps_SDO, key = 'variable', value = 'SDO')\n\n\nggplot(new_data, aes(x = Time, y = SDO)) + \n  geom_point() + \n  geom_line(color = 'blue') + \n  facet_wrap(~variable) + \n  ylab(\"Level\")\n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-03-30-social-trait-development-computational-model/social-trait-development-computational-model_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-02-21-credential-update-after-password-change/",
    "title": "Credential Update After Password Change",
    "description": {},
    "author": [],
    "date": "2018-02-21",
    "categories": [],
    "contents": "\nQuick note on updating your credentials after you change your password on GitHub.\nSteps\nsearch for keychain access in spotlight\ndelete the old github credentials\nclone and adjust a repo\ntrigger the credential helper\npush changes to repo\nSearch for “keychain access” in spotlight.\n\nEnter “github” into the search bar. Delete all github password entries.\n\nClone a repo and make some changes.\n\ncd Desktop\n\ngit clone https://github.com/Cdishop/website.git\n\ncd website\n\necho \"some change\" >> README.md\n\nActivate keychain.\n\ngit credential-osxkeychain\n\ngit config --global credential.helper osxkeychain\n\nTrigger a username/password entry by pushing to remote.\n\ngit add .\n\ngit commit -m \"commit for keychain change\"\n\ngit push -u origin master\n\n[enter credentials]\n\ngit push\n\nRoute 2 - No Keychain\nYou may not see a saved password in your keychain. If that’s the case, then you can allow your push to fail and then reset your credentials in the terminal.\nClone, adjust, and then push a repository. After pushing, you will see a “fatal” error.\nEnter\n\ngit config --global user.email \"Your email\"\n\ngit config --global user.name \"Your Name\"\n\nThen push again. You will be asked for a username and password. All subsequent repository pushes should then be automatic.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-02-18-restart-clear-environment/",
    "title": "Restart & Clear Environment",
    "description": {},
    "author": [],
    "date": "2018-02-18",
    "categories": [],
    "contents": "\n\n\n# restart session\n.rs.restartR()\n\n# clear environment\nremove(list = ls())\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-02-16-numerical-integration-and-optimization/",
    "title": "Numerical Integration and Optimization",
    "description": {},
    "author": [],
    "date": "2018-02-16",
    "categories": [],
    "contents": "\nIntegration\nTrapezoid Rule\nTo find the area under a curve we can generate a sequence of trapezoids that follow the rules of the curve (i.e., the data generating function for the curve) along the \\(x\\)-axis and then add all of the trapezoids together. To create a trapezoid we use the following equation:\nlet \\(w\\) equal the width of the trapezoid (along the \\(x\\)-axis), then\nArea = (\\(w/2\\) * \\(f(x_i)\\)) + \\(f(x_i+1)\\)\n\nfor a single trapezoid. That procedure then iterates across our entire \\(x\\)-axis and adds all of the components together.\nHere is an example function: \\(f(x) = 8 + cos(x^3)\\) and we will evaluate it over the interval [1, 10]. First, a plot of the curve itself.\n\n\nx <- seq(from = 1, to = 10, by = 1)\n\nf_x <- 8 + cos(x^3)\n\n\n\n# Plot\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nex_plot <- data.frame(\n  \"x\" = c(x),\n  \"y\" = c(f_x)\n)\n\ng_plot <- ggplot(ex_plot, aes(x = x, y = y)) + \n  geom_point() + \n  geom_smooth(se = F, span = 0.2) + \n  scale_x_continuous(breaks = c(1:10)) + \n  theme_wsj()\n\ng_plot\n\n\n\n\nThe trapezoid algorithm:\n\n\n# Parameters = the function, x-axis beginning, x-axis end, the number of trapezoids to create\n\ntrapezoid_rule <- function(fx, start, end, num_traps){\n  \n  # The width of each trapezoid\n  \n  w <- (end - start) / num_traps\n  \n  # the x-axis to evaluate our function along\n  \n  x_axis <- seq(from = start, to = end, by = w)\n  \n  # the y axis: apply the function (fx) to each value of our x-axis\n  \n  y_axis <- sapply(x_axis, fx)\n  \n  # The trapezoid rule: find the area of each trapezoid and then add them together\n  \n  trap_total <- w * ( (y_axis[1] / 2) + sum(y_axis[2:num_traps]) + (y_axis[num_traps + 1] / 2) )\n  \n  return(trap_total)\n  \n}\n\n\n\nNow we can evaluate our function (\\(f(x) = 8 + cos(x^3)\\)) with our trapezoid algorithm to find the area under its curve.\nUsing only 3 trapezoids:\n\n\neval_function <- function(x){\n  \n  8 + cos(x^3)\n  \n}\n\ntrapezoid_rule(eval_function, 1, 10, 3)\n\n\n[1] 72.29808\n\nUsing 10 trapezoids:\n\n\ntrapezoid_rule(eval_function, 1, 10, 10)\n\n\n[1] 72.84693\n\nUsing 50000 trapezoids:\n\n\ntrapezoid_rule(eval_function, 1, 10, 50000)\n\n\n[1] 71.84439\n\nOptimization\nThe Golden-Section Method\nNewton’s methods are great for finding local maxima or minima, but they also require knowing the derivative of whatever function we are evaluating. The goldent section method does not, and works in the following way:\nDefine three points along the x-axis: left (\\(l\\)), right (\\(r\\)), and middle (\\(m\\))\nChoose one of the following sections along the \\(x\\)-axis according to which is larger:\nmiddle to right (section “right”)\nmiddle to left (section “left”)\n\nChoose a point on the \\(x\\)-axis within section “right” according to the ‘golden rule’ (for our purposes the specifics of the golden rule are not important)\nApply our function to \\(y\\) and \\(m\\)\nIf \\(f(y)\\) > \\(f(m)\\), then \\(l\\) becomes \\(m\\) and \\(m\\) becomes \\(y\\)\nElse \\(r\\) becomes \\(y\\)\n\n\nChoose a point on the \\(x\\)-axis within section “left” according to the ‘golden rule’ (for our purposes the specifics of the golden rule are not important)\nApply our function to \\(y\\) and \\(m\\)\nIf \\(f(y)\\) > \\(f(m)\\), then \\(r\\) becomes \\(m\\) and \\(m\\) becomes \\(y\\)\nElse \\(l\\) becomes \\(y\\)\n\n\nContinue until the size of the “right” or “left” window diminishes to some a priori set tolerance value\nNote that this method assumes that the\nNow in code:\nOur example function: \\(f(x) = sin(x * 3)\\)\n\n\nx_2 <- seq(from = -5, to = 5, by = 1)\n\nf_x_2 <- -0.5 * (x_2^2) + 4\n\n\n# Plot\n\nlibrary(ggplot2)\n\nex_plot_2 <- data.frame(\n  \"x\" = c(x_2),\n  \"y\" = c(f_x_2)\n)\n\ng_plot_2 <- ggplot(ex_plot_2, aes(x = x, y = y)) + \n  geom_point() + \n  geom_smooth(se = F)\n\ng_plot_2\n\n\n\n\nThe golden section algorithm:\n\n\ngolden_section <- function(fx, x.l, x.r, x.m, tolerance){\n  \n  # The golden ratio rule to help select 'y' when needed\n  \n  grule <- 1 + (1 * sqrt(5)) / 2\n  \n  # Apply the function at each of our starting locations (left, right, middle)\n  \n  # left\n  \n  f.l <- fx(x.l)\n  \n  # right\n  \n  f.r <- fx(x.r)\n  \n  # middle\n  \n  f.m <- fx(x.m)\n  \n  # continue to iterate until we pass our tolderance level for how big the \"right\" \"left\" window should be\n  \n  while (( x.r - x.l) > tolerance){\n    \n    \n    # if the right window is larger than the left window, then operate on the right window side\n    \n    if ( (x.r - x.m) > (x.m - x.l) ){\n      \n      # select a point, y, according to the golden ratio rule\n      \n      y <- x.m + (x.r - x.m) / grule\n      \n      # apply the function to our selected y point\n      \n      f.y <- fx(y)\n      \n      # if the function at point y is higher than the function at the mid point\n      \n      if(f.y >= f.m){\n        \n        # reassign our points according to the algorithm steps outlined above\n        \n        # in this case, within the right window y was higher than the middle. So 'left' needs to become our new middle, and 'middle' needs to become y\n        \n        x.l <- x.m\n        f.l <- f.m\n        \n        x.m <- y\n        f.m <- f.y\n      } else {\n        \n        # if the function at y was lower than the function at the mid point\n        \n        # shift 'right' to our y point\n        \n        x.r <- y\n        f.r <- f.y\n        \n      }\n      \n      \n      \n      \n      \n    } else{\n      \n      # if the right window is not larger than the left window, select the left window to operate on\n      \n      \n      # choose a point, y, within the left window according to the golden ratio\n      \n      y <- x.m - (x.m - x.l) / grule\n      \n      # apply our function to that point\n      \n      f.y <- fx(y)\n      \n      \n      # if the function at y is greater than the function at the mid point (within the left window)\n      \n      if(f.y >= f.m){\n        \n        # reassign values according to the golden section method discussed above\n        \n        # in this case, within the left window our selected point is higher than the mid point (which is to the right of the selected y point)\n        # so our \"mid\" point needs to become our \"right\" point and y needs to become \"left\"\n        \n        x.r <- x.m\n        f.r <- f.m\n        \n        x.m <- y\n        f.m <- f.y\n        \n        \n        \n      }else{\n        \n        # if the y point is lower than the function at the mid point\n        \n        # now our y needs to become \"left\"\n        \n        x.l <- y\n        f.l <- f.y\n      }\n      \n      \n    }\n    \n    \n    \n  }\n  \n  # return the mid point\n  \n  return(x.m)\n  \n}\n\n\n\nTo summarize, the algorithm splits the \\(x\\)-axis into windows (left, middle, right) and then evaluates the function across those windows. The dimensions of the windows change over time depending on whether the function at \\(y\\) is higher or lower than a specific window dimension.\nThese examples are described in more detail in Jones, Maillardet, and Robinson, Introduction to Scientific Programming and Simulation Using R\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-02-16-numerical-integration-and-optimization/numerical-integration-and-optimization_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-17-horizontal-y-axis-ggplot2/",
    "title": "Horizontal Y Axis GGplot2",
    "description": {},
    "author": [],
    "date": "2018-01-17",
    "categories": [],
    "contents": "\nI always forget how to make the y-axis horizontal in ggplot2. Here’s a note.\n\n\ntheme(axis.title.y = element_text(angle = 0)) \n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-11-random-walks/",
    "title": "Random Walks",
    "description": {},
    "author": [],
    "date": "2018-01-11",
    "categories": [],
    "contents": "\nSome random walk fun. I use 400 steps in each example.\nOne-Dimensional Random Walk\nA random walk using a recursive equation.\n\n\n# Empty vector to store the walk\n\nrw_1 <- numeric(400)\n\n# Initial value\n\nrw_1[1] <- 7\n\n# The Random Walk equation in a for-loop\n\nfor(i in 2:400){\n  \n  rw_1[i] <- 1*rw_1[i - 1] + rnorm(1,0,2)\n  \n}\n\nplot(rw_1)\n\n\n\n\nA random walk using R’s “cumsum” command. Here, I will generate a vector of randomly selected 1’s and -1’s. “Cumsum” then compiles those values.\n\n\n# A vector of 1's and -1's\n\nrw_2 <- sample(c(1, -1), 400, replace = T)\n\nrw_2 <- cumsum(rw_2)\n\nplot(rw_2)\n\n\n\n\nTwo-Dimensional Random Walk\nNow for the real fun. Here, the walk can move forward (1) or backward (-1) along either dimension 1 or 2. So, if the walk moves forward (1) in dimension 1, dimension 2 receives a value of 0 for that step. If the walk moves backward (-1) in dimension 2, dimension 1 receives a 0 for that step.\n\n\n# A matrix to store our walk\n\n# Column 1 is dimension 1, column 2 is dimension 2\n\nrw_3 <- matrix(0, ncol = 2, nrow = 400)\n\nindex <- cbind(\n      1:400, sample(c(1, 2),\n      400,\n      replace = T)\n)\n\n\n\nThe “index” merits some explaining. The walk will randomly choose to move in dimension 1 (column 1 in “rw_3”) or 2 (column 2 in “rw_3”). This index establishes a way of assigning which choice the walk makes. Here is what “index” looks like:\n\n\nhead(index)\n\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    1\n[3,]    3    1\n[4,]    4    2\n[5,]    5    1\n[6,]    6    1\n\nThe first column values tell the random walk which step its on (i.e., which row in “rw_3”), and the second column values tell the random walk which dimension it will step through (i.e., which column in “rw_3”).\nSo the “index” represents a random selection of dimension 1 or 2 at each step. Now I can apply that random choice to the random choice of stepping forward or backward (1 or -1).\n\n\n# At each step, select a dimension (specified by the index; column 1 or 2 of rw_3)\n\n# Then randomly select forward or backward\n\nrw_3[index] <- sample(c(-1, 1), \n                      400, \n                      replace = T)\n\n\n\n# Now sum each column (dimension) just like our 1-dimensional walks\n\nrw_3[,1] <- cumsum(rw_3[,1])\nrw_3[,2] <- cumsum(rw_3[,2])\n\n\n\nHere is a visualization of the walk:\n\n\nlibrary(plotly)\n\nrw_3 <- data.frame(rw_3)\nrw_3$step <- c(1:400)\n\nnames(rw_3)[1:2] <- c(\"Dim_1\", \"Dim_2\")\n\nplot_ly(rw_3, x = ~step, y = ~Dim_1, z = ~Dim_2, type = 'scatter3d', mode = 'lines',\n        line = list(color = '#1f77b4', width = 1))\n\n\n\n{\"x\":{\"visdat\":{\"11be0110078a0\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"11be0110078a0\",\"attrs\":{\"11be0110078a0\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"lines\",\"line\":{\"color\":\"#1f77b4\",\"width\":1},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"step\"},\"yaxis\":{\"title\":\"Dim_1\"},\"zaxis\":{\"title\":\"Dim_2\"}},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400],\"y\":[-1,-2,-3,-3,-2,-1,-2,-2,-2,-3,-4,-5,-4,-3,-4,-3,-3,-3,-3,-2,-2,-2,-2,-2,-3,-4,-4,-3,-3,-3,-2,-2,-1,-2,-1,-1,-1,-1,0,0,0,-1,-1,-1,0,-1,-2,-2,-2,-1,-1,-1,0,1,1,2,1,1,2,3,3,2,3,2,1,0,-1,-2,-2,-2,-2,-2,-2,-2,-3,-4,-5,-4,-5,-5,-6,-7,-8,-7,-6,-5,-6,-5,-5,-5,-5,-4,-4,-4,-5,-4,-4,-5,-4,-4,-3,-4,-3,-4,-3,-3,-3,-2,-3,-3,-3,-3,-4,-4,-3,-4,-5,-5,-5,-5,-5,-4,-4,-5,-5,-6,-7,-7,-8,-9,-8,-9,-9,-8,-7,-6,-7,-8,-8,-8,-8,-9,-9,-10,-10,-10,-10,-10,-10,-11,-11,-11,-10,-9,-10,-10,-9,-9,-8,-8,-9,-9,-9,-9,-9,-9,-9,-9,-8,-8,-7,-7,-8,-8,-8,-9,-8,-8,-9,-9,-10,-10,-9,-9,-8,-8,-8,-9,-8,-9,-9,-10,-11,-11,-11,-11,-11,-10,-10,-10,-10,-10,-9,-9,-10,-11,-11,-12,-12,-11,-11,-12,-11,-11,-12,-11,-12,-12,-12,-12,-13,-12,-13,-13,-13,-12,-12,-12,-11,-10,-11,-11,-12,-12,-11,-11,-12,-12,-12,-11,-11,-11,-11,-11,-11,-12,-12,-12,-12,-11,-12,-13,-13,-13,-13,-12,-12,-12,-13,-14,-14,-15,-15,-15,-16,-16,-16,-17,-17,-16,-17,-16,-16,-16,-15,-15,-16,-16,-16,-15,-15,-16,-16,-16,-16,-17,-17,-18,-19,-18,-18,-17,-16,-17,-18,-18,-18,-18,-19,-18,-17,-18,-17,-17,-17,-17,-17,-17,-17,-17,-17,-17,-17,-17,-17,-17,-16,-16,-17,-17,-16,-15,-14,-14,-13,-12,-12,-11,-10,-11,-11,-11,-11,-12,-12,-13,-13,-12,-11,-11,-11,-11,-11,-11,-12,-12,-13,-14,-14,-14,-13,-13,-13,-12,-13,-13,-13,-14,-14,-15,-14,-14,-13,-12,-13,-13,-12,-11,-11,-11,-12,-12,-12,-11,-10,-10,-10,-9,-9,-9,-9,-10,-10,-10,-10,-10,-9,-9,-9,-10,-9,-9,-8,-8,-7,-8,-9,-9,-9,-10],\"z\":[0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,2,1,0,0,-1,0,-1,-2,-2,-2,-3,-3,-4,-5,-5,-6,-6,-6,-6,-7,-8,-7,-7,-8,-9,-9,-8,-9,-9,-9,-9,-8,-9,-9,-10,-11,-11,-11,-10,-10,-10,-11,-11,-11,-10,-10,-10,-10,-10,-10,-10,-10,-11,-12,-11,-10,-9,-10,-10,-10,-10,-10,-10,-11,-11,-11,-11,-11,-11,-11,-11,-11,-12,-11,-12,-12,-13,-14,-14,-14,-15,-15,-15,-14,-14,-14,-14,-14,-14,-15,-14,-14,-14,-13,-14,-15,-15,-16,-16,-16,-16,-15,-14,-13,-12,-12,-13,-13,-12,-12,-12,-13,-13,-13,-13,-13,-14,-14,-14,-14,-14,-14,-15,-16,-17,-17,-16,-16,-15,-16,-15,-16,-15,-15,-16,-15,-15,-15,-15,-16,-16,-17,-17,-18,-18,-17,-16,-17,-16,-17,-18,-17,-17,-16,-16,-17,-17,-16,-15,-15,-15,-16,-16,-17,-17,-16,-16,-15,-15,-16,-15,-15,-15,-15,-16,-16,-16,-17,-18,-19,-18,-18,-19,-20,-21,-20,-20,-19,-19,-19,-18,-18,-17,-17,-18,-18,-18,-17,-17,-17,-17,-16,-17,-18,-18,-18,-18,-19,-18,-18,-17,-16,-16,-16,-16,-17,-17,-16,-16,-17,-17,-16,-17,-17,-18,-19,-18,-17,-16,-16,-17,-18,-19,-19,-19,-19,-20,-19,-18,-18,-19,-20,-20,-20,-21,-21,-20,-19,-19,-18,-17,-17,-18,-18,-18,-18,-17,-18,-18,-19,-19,-20,-21,-21,-22,-22,-23,-24,-25,-25,-24,-24,-24,-24,-23,-23,-23,-23,-23,-22,-21,-20,-20,-20,-20,-20,-20,-21,-22,-21,-22,-23,-24,-25,-26,-27,-28,-29,-28,-27,-27,-26,-26,-27,-27,-27,-27,-28,-28,-28,-29,-29,-29,-29,-28,-27,-28,-28,-29,-29,-30,-30,-30,-31,-32,-31,-30,-31,-31,-30,-30,-30,-29,-28,-28,-29,-28,-28,-28,-27,-28,-28,-27,-27,-27,-28,-28,-28,-28,-27,-27,-27,-26,-25,-25,-24,-23,-23,-23,-22,-21,-21,-22,-21,-20,-20,-19,-18,-19,-20,-20,-19,-18,-18,-18,-19,-19,-18,-18,-18,-18,-17,-16,-16],\"mode\":\"lines\",\"line\":{\"color\":\"#1f77b4\",\"width\":1},\"type\":\"scatter3d\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2018-01-11-random-walks/random-walks_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-03-combining-csv-files/",
    "title": "Combining CSV Files",
    "description": {},
    "author": [],
    "date": "2018-01-03",
    "categories": [],
    "contents": "\nA couple quick pieces of code to assist any time I need to work with many CSV files.\nInto List\nThis first code chunk loads all of the CSV files in a folder, makes each into data frame, and stores each separately in a list.\n\n\nsetwd(\"enter path\")\n\n# A character vector of every file name\n\nfiles <- Sys.glob(\"*.csv\")\n\n# A list of all CSV files in the respective folder as data.frames\n\nmyfiles <- lapply(files, FUN = read.csv)\n\n# To load any single data set...\n\ndata_set1 <- myfiles[[1]]\n\n\n\nInto Single Data Frame\nThe code above stores each file into a list as a separate data frame. If I want to combine every CSV file into the same data frame I can do the following:\n\n\nsetwd(\"enter path\")\n\n# A character vector of every file name\n\nfiles <- list.files(pattern = \"*.csv\")\n\n# Now the full command\n\ndata_set <- do.call(cbind, \n                    lapply(files, \n                           function(x) read.csv(x, stringsAsFactors = FALSE)))\n\n\n\nThe code shown uses “cbind” so every variable within every CSV file will receive its own column in my “data_set.” If every CSV file has the same variable names replace “cbind” with “rbind.”\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-02-formatting-qualtrics-responses/",
    "title": "Formatting Qualtrics Responses",
    "description": {},
    "author": [],
    "date": "2018-01-02",
    "categories": [],
    "contents": "\nHere is a quick piece of code to create numeric response scores when data are read in as strings (e.g., “Strongly Agree, Agree, Neutral”).\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(plyr)\n\ndf <- read.csv(\"path\")\n\nlabels_to_values1 <- function(x){\n  \n  mapvalues(x, from = c(\"Strongly Agree\", \n                        \"Agree\", \n                        \"Slightly Agree\", \n                        \"Slightly Disagree\", \n                        \"Disagree\", \n                        \"Strongly Disagree\"),\n                        to = c(6,5,4,3,2,1))\n  \n}\n\nrecode_df <- df %>%\n  select(column_to_modify1, column_to_modify2, column_to_modify2, etc) %>%\n  apply(2, FUN = labels_to_values1) %>%\n  data.frame()\n\n\n\nNote that R will throw you warnings if all of the response options are not used, but the code will still work.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-first-differencing-by-group/",
    "title": "First Differencing By Group",
    "description": {},
    "author": [],
    "date": "2017-12-23",
    "categories": [],
    "contents": "\nA bit of practice taking the first difference when the data is not consistent with a typical time-series structure.\nThe first set of data.\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\ndff <- tibble(\n  'id' = c('a', 'a', 'b', 'b', 'c', 'c'),\n  'survey' = c(1, 2, 1, 2, 1, 2),\n  'score' = c(4, 4, 2, 4, 5, 2),\n  'team' = c('a', 'a', 'a', 'a', 'a', 'a')\n)\ndff %>% kable() %>% kable_styling()\n\n\n\nid\n\n\nsurvey\n\n\nscore\n\n\nteam\n\n\na\n\n\n1\n\n\n4\n\n\na\n\n\na\n\n\n2\n\n\n4\n\n\na\n\n\nb\n\n\n1\n\n\n2\n\n\na\n\n\nb\n\n\n2\n\n\n4\n\n\na\n\n\nc\n\n\n1\n\n\n5\n\n\na\n\n\nc\n\n\n2\n\n\n2\n\n\na\n\n\nThe goal is to subtract scores on the first survey from scores on the second survey. E.g., what are the change scores across the surveys for each participant?\n\n\ndff %>% \n  group_by(id) %>% \n  mutate(diffscore = score - lag(score))\n\n\n# A tibble: 6 × 5\n# Groups:   id [3]\n  id    survey score team  diffscore\n  <chr>  <dbl> <dbl> <chr>     <dbl>\n1 a          1     4 a            NA\n2 a          2     4 a             0\n3 b          1     2 a            NA\n4 b          2     4 a             2\n5 c          1     5 a            NA\n6 c          2     2 a            -3\n\nThe second set of data.\n\n\nscore <- c(10,30,14,20,6)\ngroup <- c(rep(1001,2),rep(1005,3))\ndf <- data.frame(score,group)\n\ndf %>% kable() %>% kable_styling()\n\n\n\nscore\n\n\ngroup\n\n\n10\n\n\n1001\n\n\n30\n\n\n1001\n\n\n14\n\n\n1005\n\n\n20\n\n\n1005\n\n\n6\n\n\n1005\n\n\nGroup 10001 has two scores whereas group 1005 has 3. I want the change from one score to another for each group.\n\n\ndf %>%\n  group_by(group) %>%\n  mutate(first_diff = score - lag(score))\n\n\n# A tibble: 5 × 3\n# Groups:   group [2]\n  score group first_diff\n  <dbl> <dbl>      <dbl>\n1    10  1001         NA\n2    30  1001         20\n3    14  1005         NA\n4    20  1005          6\n5     6  1005        -14\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-why-detecting-interactions-is-easier-in-the-lab/",
    "title": "Why Detecting Interactions is Easier in the Lab",
    "description": {},
    "author": [],
    "date": "2017-11-15",
    "categories": [],
    "contents": "\nA fun simulation by McClelland and Judd (1993) in Psychological Bulletin that demonstrates why detecting interactions outside the lab (i.e., in field studies) is difficult. In experiments, scores on the independent variables are located at the extremes of their respective distributions because we manipulate conditions. The distribution of scores across all of the independent variables in field studies, conversely, is typically assumed to be normal. By creating “extreme groups” in experiments, therefore, it becomes easier to detect interactions.\nImagine running an experiment where we randomly assign participants to one of two groups on an independent variable, goal difficulty. In one group the goal is challening, in the other group the goal is easy to accomplish. We are then interested in which group performs better on a task. After randomly assigning to groups, the distribution of scores on “goal difficulty” would be as follows:\n\n\n\n\n\n\nwhere 50 people are assigned to each condition. In this case, the distribution of scores is aligned at the extremes (i.e., -1, or the hard goal, and 1, or the easy goal) because we manipulated that variable. In field studies, where we cannot manipulate goal difficulty, the distribution of scores would be as follows:\n\n\n\nwhere scores about the independent variable (goal difficulty) are dispersed because we did not manipulate. The same distributional differences occur across other independent variables that we include in our design, and they are the reason behind fewer interaction detections in field studies.\nThe cool part is that this happens even when the data generating mechanisms are exactly the same. The mechanism that causes \\(y\\), in both the experiments and field studies in this simulation, will be:\n\\[\\begin{equation}\ny_{i} = b_0{i} + b_1{x_i} + b_2{z_i} + b_3{zx_i} + e_{i}\n\\end{equation}\\]\nwhere \\(y_{i}\\) is the value of the outcome (i.e., performance) for the \\(i^\\text{th}\\) person, \\(x_i\\) is the value of one independent variable for the \\(i^\\text{th}\\) person (i.e., goal difficulty), \\(z_i\\) is the value of another independent variable for the \\(i^\\text{th}\\) person (e.g., whatever variable you please), \\(zx_i\\) represents the combination of values on \\(x\\) and \\(z\\) for the \\(i^\\text{th}\\) person (i.e., the interaction term), \\(e_i\\) is a normally distributed error term for the \\(i^\\text{th}\\) person, and \\(b_0\\), \\(b_1\\), and \\(b_2\\) represent the regression intercept and coefficients relating the predictors to the outcome.\nAgain, the data generating equation, the thing that causes \\(y\\), is the same for both field studies and experiments. We are going to find differences, however, simply because the distribution on the independent variables are different.\nThe values for \\(b_0\\), \\(b_1\\), and \\(b_2\\) will be, respectively, 0, 0.20, 0.10, and 1.0 (see McClelland & Judd, 1993). In other words, our interaction coefficient is gigantic.\nEach simulation will use the equation just presented to generate data across 100 individuals in the field and 100 individuals in the lab. The only difference between the two groups will be their initial distribution on \\(x\\) and \\(z\\). For the lab group, their scores will be randomly assigned to -1 or 1, and in the field group scores will be randomly dispersed (normally) between -1 and 1. After generating the data I then estimate the coefficients using multiple regression and save the significance value in a vector. The process then interates 1000 times.\nThe Experiment Data\nThe distribution of X:\n\n\n\nThe distribution of Z:\n\n\n\nThe distribution of Y after using the equation above to generate scores on Y:\n\n\ny_values <- b_0 + 0.20*x_values + 0.10*z_values + \n            1.00*x_values*z_values + rnorm(100,0,4)\n\nhist(y_values)\n\n\n\n\nNow estimate the parameters using regression:\n\n\nexp_data <- data.frame(\"X\" = c(x_values),\n                       \"Z\" = c(z_values),\n                       \"Y\" = c(y_values))\n\nexp_model <- lm(Y ~ X + Z + X:Z, data = exp_data)\nsummary(exp_model)\n\n\n\nCall:\nlm(formula = Y ~ X + Z + X:Z, data = exp_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5221 -2.5763  0.3173  2.9709  8.2962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -0.14473    0.38409  -0.377  0.70715   \nX           -0.06575    0.38409  -0.171  0.86444   \nZ           -0.24140    0.38409  -0.628  0.53117   \nX:Z          1.06693    0.38409   2.778  0.00658 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.777 on 96 degrees of freedom\nMultiple R-squared:  0.08142,   Adjusted R-squared:  0.05272 \nF-statistic: 2.836 on 3 and 96 DF,  p-value: 0.04215\n\nThe Field Study Data\nThe distribution of X:\n\n\n\nThe distribution of Z:\n\n\n\nThe distribution of Y after using the equation above to generate scores on Y:\n\n\nf_y_values <- b_0 + 0.20*f_x_values + 0.10*f_z_values + \n              1.00*f_x_values*f_z_values + rnorm(100,0,4)\n\nhist(f_y_values)\n\n\n\n\nNow estimate the parameters using regression:\n\n\nfield_data <- data.frame(\"FX\" = c(f_x_values),\n                           \"FZ\" = c(f_z_values),\n                           \"FY\" = c(f_y_values))\n\nfield_model <- lm(FY ~ FX + FZ + FX:FZ, data = field_data)\nsummary(field_model)\n\n\n\nCall:\nlm(formula = FY ~ FX + FZ + FX:FZ, data = field_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.394 -2.967  0.134  2.868 11.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.97308    0.40804   2.385   0.0191 *\nFX          -0.04196    0.82524  -0.051   0.9596  \nFZ          -1.36550    1.04609  -1.305   0.1949  \nFX:FZ        1.34535    2.19279   0.614   0.5410  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.057 on 96 degrees of freedom\nMultiple R-squared:  0.02166,   Adjusted R-squared:  -0.008912 \nF-statistic: 0.7085 on 3 and 96 DF,  p-value: 0.5492\n\nPutting Everything Into Monte Carlo\nReplicate the process above 1000 times and save the p-value each time\n\n\nsims <- 1000\nexp_results <- numeric(1000)\nfield_results <- numeric(1000)\n\n\nX_coefficient <- 0.20\nZ_coefficient <- 0.10\n\nXZ_coefficient <- 1.00\nMu <- 0\n\nxy_data <- c(-1,1)\n\nlibrary(MASS)\n\nfor(i in 1:sims){\n  \n  # Experiment Data\n  \n  # X\n  x_values <- sample(xy_data, 100, replace = T)\n  \n  # Z\n  z_values <- sample(xy_data, 100, replace = T)\n  \n  # Y\n  y_values <- Mu + X_coefficient * x_values + Z_coefficient * z_values + \n              XZ_coefficient * x_values * z_values + rnorm(100,0,4)\n  \n  exp_data <- data.frame(\"X\" = c(x_values),\n                         \"Z\" = c(z_values),\n                         \"Y\" = c(y_values))\n  \n  \n  # Field Data\n  \n  # X\n  f_x_values <- rnorm(100, 0, 0.5)\n  \n  # Z\n  f_z_values <- rnorm(100, 0, 0.5)\n  \n  # Y\n  f_y_values <- Mu + X_coefficient * f_x_values + Z_coefficient * f_z_values + \n                XZ_coefficient * f_x_values * f_z_values + rnorm(100,0,4)\n \n  \n  field_data <- data.frame(\"FX\" = c(f_x_values),\n                           \"FZ\" = c(f_z_values),\n                           \"FY\" = c(f_y_values))\n  \n  \n  # Modeling\n  \n  \n  exp_model <- lm(Y ~ X + Z + X:Z, data = exp_data)\n  exp_results[i] <- summary(exp_model)$coefficients[4,4]\n  \n  field_model <- lm(FY ~ FX + FZ + FX:FZ, data = field_data)\n  field_results[i] <- summary(field_model)$coefficients[4,4]\n  \n}\n\n\n\nThe Results\nWhat proportion of experiments find significant interaction effects?\n\n\nsum(exp_results < 0.05) / 1000\n\n\n[1] 0.669\n\nWhat proportion of field studies find significant interaction effects?\n\n\nsum(field_results < 0.05) / 1000\n\n\n[1] 0.086\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-11-07-why-detecting-interactions-is-easier-in-the-lab/why-detecting-interactions-is-easier-in-the-lab_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-workforce-dynamics/",
    "title": "Workforce Dynamics",
    "description": {},
    "author": [],
    "date": "2017-08-22",
    "categories": [],
    "contents": "\nWe can model the states of a system by applying a transition matrix to values represented in an initial distribution and repeating it until we reach an equilibrium.\nSuppose we want to model how job roles in a given company change over time. Let us assume the following:\nThere are three (hierarchical) positions in the company:\nAnalyst\nProject Coordinator\nManager\n\n30 new workers enter the company each year, and they all begin as analysts\nThe probability of moving from …\nan analyst to a project coordinator is 75%\na project coordinator to a manager is 8%\n\nThe probability of staying in a position is 25%\nThe initial distribution of people in each role (analyst, PC, manager) is: c(45, 15, 6)\nThe Initial States:\n\n\ninitial <- c(45, 15, 6)\n\n\n\nThe Transition Matrix:\nConsistent with the assumptions described above…\n\n\ntransition <- matrix(c(   0.25, 0.00, 30,\n                          0.75, 0.25, 0.00,\n                          0.00, 0.08, 0.25  ), 3, 3, byrow = T)\n\n\n\nThe Company Roles Over 50 Years:\n\n\ndf <- matrix(, nrow = 50, ncol = 3)\n\ncount <- 0\n\nfor(i in 1:50){\n  count <- count + 1\n  \n  if(i == 1){\n    \n    df[count,] = initial\n  \n  }\n  else{\n    \n    df[count,] = transition%^%i %*% initial\n  }\n  \n}\n\n\n\nIf job-movement in a company aligned with our initial assumptions, we would expect the distribution of jobs to follow this pattern across time:\nSome data tidying first…\n\n\ndf <- data.frame(df)\nnames(df) <- c(\"Analyst\", \"Project_Coordinator\", \"Manager\")\ndf$Time <- rep(1:nrow(df))\n\ndata_f <- df %>%\n  gather(Analyst, Project_Coordinator, Manager, key = \"Position\", value = \"Num_People\")\n\ntotal_value <- data_f %>%\n  group_by(Time) %>%\n  summarise(\n    total = sum(Num_People)\n  )\n\ndata_f <- left_join(data_f, total_value)\n\n\ndata_f <- data_f %>%\n  mutate(Proportion = Num_People / total)\n\n\n\nThe proportion of people in each position:\n\n\nlibrary(ggthemes)\n\nggplot(data_f, aes(x = Time, y = Proportion, color = Position)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nThe amount of people in the company overall:\n\n\nggplot(data_f, aes(x = Time, y = Num_People, color = Position)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nAs you can tell, this is unrealistic =)\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-11-07-workforce-dynamics/workforce-dynamics_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-counting-degrees-of-freedom/",
    "title": "Counting Degrees of Freedom",
    "description": {},
    "author": [],
    "date": "2017-07-13",
    "categories": [],
    "contents": "\nThis post contains a bunch of examples where I practice counting dfs. In each example, I generate the data, estimate the parameters using SEM, count the dfs, and then compare my count to what the model spits back. To count dfs, I need to know the number of knowns and unknowns in my system:\n\\[\\begin{equation}\n\\textrm{DFs} = \\textrm{knowns - unknowns}\n\\end{equation}\\]\nTo count the number of knowns, I need to know the number of observed variables, p:\n\\[\\begin{equation}\n\\textrm{knowns} = p*(p+1) / 2\n\\end{equation}\\]\nTo count the number of unknowns, I count the number of parameters that my model estimates. Now for the examples.\nExample 1 - Trust and availability cause helping\nDGP\n\n\npeople <- 400\ntrust <- rnorm(people, 40, 2)\navailability <- rnorm(people, 20, 5)\nerror <- rnorm(people, 0, 2)\n\nhelping <- 3 + 0.2*trust + 0.7*availability + error\n\n\n\nSEM\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\n\ndf <- data.frame(\n  'id' = c(1:people),\n  'trust' = c(trust),\n  'availability' = c(availability),\n  'helping' = c(helping)\n)\n\nex1_string <- '\n\nhelping ~ b1*trust + b2*availability\n\n'\n\nex1_model <- sem(ex1_string, data = df)\n\n\n\nCount dfs\nKnowns (count the observed variables)\n\n\n# p*(p + 1) / 2\n\n3*(3+1) / 2\n\n\n[1] 6\n\nUnknowns (count the estimated parameters)\n1 for b1\n1 for b2\n1 for the variance of trust\n1 for the variance of availability\n1 for the covariance of trust and availability\n1 for the prediction error on helping\ntotal = 6\n6 - 6 = 0\n\n\nshow(ex1_model)\n\n\nlavaan 0.6-9 ended normally after 14 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         3\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nNow if I restrict the covariance of trust and availability to be zero I should have 1 df\n\n\nex1_string_restrict <- '\n\nhelping ~ b1*trust + b2*availability\ntrust ~~ 0*availability\n\n'\n\nex1_model_restrict <- sem(ex1_string_restrict, data = df)\nshow(ex1_model_restrict) # yup\n\n\nlavaan 0.6-9 ended normally after 15 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 0.281\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.596\n\nExample 2 - Common factor underlying 6 observed items\nDGP\n\n\ncommon_factor <- rnorm(people, 30, 2)\nerror_cf <- rnorm(people, 0, 2)\nitem1 <- 0.35*common_factor + error_cf\nitem2 <- 0.22*common_factor + error_cf\nitem3 <- 0.18*common_factor + error_cf\nitem4 <- 0.24*common_factor + error_cf\nitem5 <- 0.31*common_factor + error_cf\nitem6 <- 0.44*common_factor + error_cf\n\n# nope, that approach is wrong. If I do above then my errors are not independent\n# prediction errors (in this case measurement) should be independent\n\nitem1 <- 0.35*common_factor + rnorm(people, 0, 2)\nitem2 <- 0.22*common_factor + rnorm(people, 0, 2)\nitem3 <- 0.18*common_factor + rnorm(people, 0, 2)\nitem4 <- 0.24*common_factor + rnorm(people, 0, 2)\nitem5 <- 0.31*common_factor + rnorm(people, 0, 2)\nitem6 <- 0.44*common_factor + rnorm(people, 0, 2)\n\ndf_cf <- data.frame(\n  'id' = c(1:people),\n  'item1' = c(item1),\n  'item2' = c(item2),\n  'item3' = c(item3),\n  'item4' = c(item4),\n  'item5' = c(item5),\n  'item6' = c(item6)\n)\n\n\n\nSEM\n\n\nex2_string <- '\n\ncom_factor =~ 1*item1 + fl2*item2 + fl3*item3 + fl4*item4 + fl5*item5 + fl6*item6\n'\n\nex2_model <- sem(ex2_string, data = df_cf)\n\n\n\nCount dfs\nknowns (count the observed variables)\n\n\n# p*(p + 1) / 2\n\n6*(6 + 1) / 2\n\n\n[1] 21\n\nunknowns (count the estimated parameters)\n6 factor loadings, but I constrained the first one to be 1 (I have to to estimate the latent variable), so 5 parameters\n5 measurement errors for the 5 factor loadings\n1 variance for the latent exogenous variable\n1 mean for the latent exogenous variable\ntotal = 12\n21 - 12 = 9\n\n\nshow(ex2_model)\n\n\nlavaan 0.6-9 ended normally after 69 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                15.528\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.077\n\nExample 3 - Two latent variables predict one observed outcome\nCognitive ability (latent variable 1) and assertiveness (latent variable 2) predict productivity. Cognitive ability and assertiveness are both captured with 2 manifest items/variables.\nDGP\n\n\n# cog ability (latent exogenous variable 1)\ncog_ability <- rnorm(people, 100, 15)\nca_item1 <- 0.78*cog_ability + rnorm(people, 0, 1)\nca_item2 <- 0.11*cog_ability + rnorm(people, 0, 1)\n\n# assertiveness (latent exogenous variable 2)\nassertive <- rnorm(people, 30, 8)\nass_item1 <- 0.81*assertive + rnorm(people, 0, 1)\nass_item2 <- 0.34*assertive + rnorm(people, 0, 1)\n\n# productivity (observed outcome)\n\nproductivity <- 0.55*cog_ability + 0.82*assertive + rnorm(people, 0, 5)\n\n# data\n\ndf_3 <- data.frame(\n  'id' = c(1:people),\n  'ca_item1' = c(ca_item1),\n  'ca_item2' = c(ca_item2),\n  'ass_item1' = c(ass_item1),\n  'ass_item2' = c(ass_item2),\n  'productivity' = c(productivity)\n  \n  )\n\n\n\nSEM\n\n\nex3_string <- '\n\ncog_ability =~ 1*ca_item1 + fl2*ca_item2\nassertiveness =~ 1*ass_item1 + fla*ass_item2\n\ncog_ability ~~ cog_ability\nassertiveness ~~ assertiveness\ncog_ability ~~ assertiveness\n\nproductivity ~ b1*cog_ability + b2*assertiveness\n\n'\n\nex3_model <- sem(ex3_string, data = df_3)\n\n\n\nCount dfs\nknowns (count the observed variables)\n\n\n# p*(p+1) / 2\n\n5*(5+1) / 2\n\n\n[1] 15\n\nunknowns (count the estimated parameters)\n4 factor loadings but I constrained 2 of them, so 2 factor loadings\n2 measurement errors (4 items, but constrained 2 of them)\n1 variance on cog ability\n1 mean on cog ability\n1 variance on assertiveness\n1 mean on assertiveness\n1 covariance among cog ability and assertiveness\nb1\nb2\n2 prediction errors\ntotal = 13\n15 - 13 = 2\n\n\nshow(ex3_model)\n\n\nlavaan 0.6-9 ended normally after 170 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 2.776\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.428\n\nNope. I’m one off, where did I go wrong?\nAh, there is only 1 prediction error because productivity is being predicted. I counted 2 prediction errors because I gave one to both b1 and b2. So, the unknowns should be…\n4 factor loadings but I constrained 2 of them, so 2 factor loadings\n2 measurement errors (4 items, but constrained 2 of them)\n1 variance on cog ability\n1 mean on cog ability\n1 variance on assertiveness\n1 mean on assertiveness\n1 covariance among cog ability and assertiveness\nb1\nb2\n1 prediction error\ntotal = 12\n15 - 12 = 3\n\n\nshow(ex3_model)\n\n\nlavaan 0.6-9 ended normally after 170 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 2.776\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.428\n\nExample 4 - a causes b, which causes c, which causes d\nDGP\n\n\na <- rnorm(people, 300, 3)\nb <- 0.67*a + rnorm(people, 0, 1)\nc <- 0.99*b + rnorm(people, 0, 10)\nd <- 4 + 4*c + rnorm(people, 0, 4)\n\ndf_chain <- data.frame(\n  'id' = c(1:people),\n  'a' = c(a),\n  'b' = c(b),\n  'c' = c(c),\n  'd' = c(d)\n)\n\n\n\nSEM\n\n\nex4_string <- '\n\nb ~ b1*a\nc ~ b2*b\nd ~ b3*c\n\na ~~ a\n\n'\n\nex4_model <- sem(ex4_string, data = df_chain)\n\n\n\nCount dfs\nknowns (count the observed variables)\n\n\n# p*(p+1) / 2\n4*(4+1) / 2\n\n\n[1] 10\n\nunknowns (count the estimated parameters)\nb1\nb2\nb3\n3 prediction errors\n1 variance for the lone exogenous variable (a)\ntotal = 7\n10 - 7 = 3\n\n\nshow(ex4_model)\n\n\nlavaan 0.6-9 ended normally after 41 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 3.046\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.385\n\nExample 5 - Observed affect over 7 time points\nDGP\n\n\ntime <- 7\naffect_store <- matrix(, ncol = 3, nrow = time*people)\ncount <- 0\nfor(i in 1:people){\n  \n  unob_het <- rnorm(1, 0, 3)\n  \n  for(j in 1:time){\n    count <- count + 1\n    \n    if(j == 1){\n      affect_store[count, 1] <- i\n      affect_store[count, 2] <- j\n      affect_store[count, 3] <- unob_het + 50 + rnorm(1, 0, 1)\n    }else{\n      affect_store[count, 1] <- i\n      affect_store[count, 2] <- j\n      affect_store[count, 3] <- 0.8*affect_store[count - 1, 3] + unob_het + rnorm(1, 0, 1)\n      \n    }\n  }\n  \n}\ndf5 <- data.frame(affect_store)\nnames(df5) <- c('id', 'time', 'affect')\nlibrary(reshape2)\ndf5_wide <- reshape(df5, idvar = 'id', timevar = 'time', direction = 'wide')\n\n\n\nSEM\n\n\nex5_string <- '\n\nunob_het =~ 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7\n\naffect.2 ~ ar*affect.1\naffect.3 ~ ar*affect.2\naffect.4 ~ ar*affect.3\naffect.5 ~ ar*affect.4\naffect.6 ~ ar*affect.5\naffect.7 ~ ar*affect.6\n\naffect.1 ~~ affect.1\nunob_het ~~ unob_het\naffect.1 ~~ unob_het\n\n'\n\nex5_model <- sem(ex5_string, data = df5_wide)\n\n\n\nCount dfs\nknowns (count the observed variables)\n\n\n# p*(p+1) / 2\n7*(7+1) / 2\n\n\n[1] 28\n\nunknowns (count the estimated parameters)\nar is 1 estimated parameter\n1 variance of unobserved heterogeneity\n1 variance of affect.1\n1 covariance among affect.1 and unobserved heterogeneity\n6 prediction errors\ntotal = 10\n28 - 10 = 18\n\n\nshow(ex5_model)\n\n\nlavaan 0.6-9 ended normally after 120 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n  Number of equality constraints                     5\n                                                      \n  Number of observations                           400\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                29.227\n  Degrees of freedom                                18\n  P-value (Chi-square)                           0.046\n\nWhy didn’t I estimate a mean for unobserved heterogeneity here? In all of the other examples I estimated the variance (1 parameter) and the mean (1 parameter) of the latent exogenous variable. In this case, unobserved heterogeneity is the latent exogenous variable but I only estimated its variance. That’s because in this model we don’t really care about the mean of unobserved heterogeneity, it’s just a latent variable that we incorporate to account for stable individual differences. In other words, when I estimate latent cog ability and assertiveness as IVs to predict an outcome, I care about their means. Here, unobserved heterogeneity is just an additional factor to account for, not a variable whose mean I really care to know. That said, if I wanted to estimate the mean of unobserved heterogeneity (which would result in one additional estimated parameter and one fewer df) then I would incorporate the following into the model string.\n\n\n'\n\nunob_het ~ 1 # lavaan code for estimating the mean of a latent variable\n\n'\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-tidyverse-randoms/",
    "title": "Tidyverse Randoms",
    "description": {},
    "author": [],
    "date": "2017-06-24",
    "categories": [],
    "contents": "\nSome tidyverse commands I came across and hadn’t seen before. Thought it would be useful to store them here.\nReplace & Recode\nReplace missing values with the median.\n\n\ndf <- df %>%\n  mutate(stress = replace(stress,\n                          is.na(stress),\n                          median(stress, na.rm = T)))\n\n\n\nChange a variable’s label.\n\n\ndf <- df %>%\n  mutate(group = replace(group, group == \"A\", \"Group-A\"))\n\n\n\nRecode is a simple version of case_when.\n\n\ndf %>%\n  mutate(color = recode(color,\n                        \"g\" = \"green\",\n                        \"b\" = \"blue\",\n                        \"y\" = \"y\",\n                        .default = \"other\"))\n\n\n\nAn Alternative To Quosure\n\n\ncalc <- function(data, group_var) {\n  data %>%\n    group_by({{ group_var }}) %>%\n    summarize(mean = mean(stress))\n}\n\n\n\n\n\ncalc_m_sd <- function(data, mean_var, sd_var) {\n  data %>%\n    summarize(\n      \"mean_{{mean_var}}\" := mean({{ mean_var }}),\n      \"sd_{{sd_var}}\" := mean({{ sd_var }})\n    )\n}\n\n\n\nUsing .data in a for-loop\n\n\nfor (variable in names(df)) {\n  df %>% count(.data[[variable]]) %>% print()\n}\n\n\n\nSelect a column if it’s row values have x\n\n\ndf %>%\n  select_if(is.numeric) %>%\n  select_if(~mean(., na.rm=TRUE) > 10)\n\n\ndf %>% \n  select_all(any_vars(str_detect(., pattern = \"Mu\")))\n\n\n\nIf with “is” At with “vars\n\n\nmutate_if(is.numeric)\n\nmutate_at(vars(contains(\"Q\")))\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-convert-text-file/",
    "title": "Convert Text File",
    "description": {},
    "author": [],
    "date": "2017-04-09",
    "categories": [],
    "contents": "\nA quick piece of code that reads a text file, changes something, saves a new text file, and iterates that process for every text file in that folder.\n\nsetwd(\"path to the text files\")\nlibrary(readr)\n\nall_files = Sys.glob(\"*.txt\")\n\n\n\nfor(i in 1:length(all_files)){\n  \n  data = all_files[i]\n  mystring = read_file(paste(data))\n  \n  new_data = gsub(\"old piece of text\", \"new piece of text\", mystring)\n  \n  write_file(new_data, path = paste(\"something\", code, \".txt\", sep = \"\")\n  \n  \n}\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-facet-wrap-across-multiple-pages/",
    "title": "Facet Wrap Across Multiple Pages",
    "description": {},
    "author": [],
    "date": "2017-04-07",
    "categories": [],
    "contents": "\nGreat discussion of extending a facet wrap across several pages.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-the-binomial-effect-size-display/",
    "title": "The Binomial Effect Size Display",
    "description": {},
    "author": [],
    "date": "2017-01-01",
    "categories": [],
    "contents": "\nEffect sizes provide information about the magnitude of an effect. Unfortunately, they can be difficult to interpret or appear “small” to anyone unfamiliar with the typical effect sizes in a given research field. Rosenthal and Rubin (1992) provide an intuitive effect size, called the Binomial Effect Size Display, that captures the change in success rate due to a treatment.\nThe calculation is simple:\nTreamtment BESD = 0.50 + (r / 2)\nControl BESD = 0.50 - (r / 2)\nwhere r is the correlation coefficient between treatment and survival (however defined). Many mathematical discussions exist, below is a simulation of one specific example by Randolph and Edmondson (2005). Please keep in mind the BESD is not without its critics (e.g., Thompson 1998).\nThe Example\nAziothymidine (AZT) is used to treat AIDS, and the correlation between AZT use and survival is 0.23. Using the equations above, we can calculate the BESD for the treatment and control groups.\n\n\n# Survival\n\nAZT_survive <- 0.50 + (0.23 / 2)\nPlacebo_survive <- 0.50 - (0.23 / 2)\n\n\n\nSo the survival percentages for each group are:\n\n\nAZT_survive\n\n\n[1] 0.615\n\nPlacebo_survive\n\n\n[1] 0.385\n\nNow we can simulate that process to see if our results match.\nThe Simulation\nPreliminary set up:\n\n\nk <- 1000\npercent_treatment_survive <- numeric(k)\npercent_control_survive <- numeric(k)\n\n# The correlation between AZT and survival is 0.23\n\nSigma <- matrix(c(1.0, 0.23,\n                    0.23, 1.0), 2, 2, byrow = T)\n\n\n\nRunning the process:\n\n\nfor(i in 1:k){\n  \n  # Draws from a binomial distribution with 0.50 base rate\n  \n  # The correlation between both vectors is 0.23\n  \n  # The first vector is treatment vs control assignment.\n  # 1 = treatment ; 0 = control\n  \n  # The second vector is survive vs. not survive\n  # 1 = survive ; 0 = not survive\n  \n  x <- rmvbin(5000, margprob = c(0.5, 0.5), bincorr = Sigma)\n  x <- as.data.frame(x)\n  \n  # \"Survive\" is when column 2 is equal to 1\n  \n  total_survive <- x %>%\n        filter(V2 == 1)\n  \n  # The amount of people in each group that survived\n  \n  treatment_survive <- sum(total_survive$V1 == 1) / nrow(total_survive)\n  \n  control_survive <- sum(total_survive$V1 == 0) / nrow(total_survive)\n  \n  # Save the results from each iteration\n  \n  percent_treatment_survive[i] <- treatment_survive\n  percent_control_survive[i] <- control_survive\n  \n}\n\n\n\nComparison\nOur original calculations were as follows:\n\n\nAZT_survive\n\n\n[1] 0.615\n\nPlacebo_survive\n\n\n[1] 0.385\n\nand here are the simulation results:\n\n\nmean(percent_treatment_survive)\n\n\n[1] 0.6159293\n\nmean(percent_control_survive)\n\n\n[1] 0.3840707\n\nKeep in mind the BESD assumes a 50/50 base rate of success (however defined) with no treatment.\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-art-with-monte-carlo/",
    "title": "Art With Monte Carlo",
    "description": {},
    "author": [],
    "date": "2016-12-24",
    "categories": [],
    "contents": "\nI like to think of Monte Carlo as a counting method. If a condition is satisfied we make a note (e.g., 1), and if the condition is not satisfied we make a different note (e.g., 0). We then iterate and evaluate the pattern of 1’s and 0’s to learn about our process. Art can be described in a similar way: if a condition is satisfied we use a color, and if a condition is not satisfied we use a different color. After many iterations, we have an image.\nHere is a simulation that “draws” a process, inspired by Caleb Madrigal (link here).\nThe Data Generating Process\n\n\nf <- function(x){\n  2*sin(4*x) + 2*sin(5*x) + 12\n}\n\n\n\nSome Initial Values\n\n\nx <- seq(0, 10, length.out  = 1000)\n\n\n\nUsing the DGP to generate values of Y\n\n\ny <- f(x)\n\nplot(x, y)\n\n\n\n\nThis is the process we want to “draw”\nNow for the Monte Carlo\nWe are going to evaluate 10,000 points within our process space (10 x 16).\n\n\nnum_points <- 10000\nrect_width <- 10\nrect_height <- 16\n\npoints <- matrix(, ncol = 2, nrow = num_points)\n\n\n\nColumn 1 of our points matrix represents the width of our process space while column 2 represents its height. First we fill the matrix with random values within our process space:\n\n\nfor(i in 1:num_points){\n  points[i,1] = runif(1, 0, rect_width)\n  points[i,2] = runif(1, 0, rect_height)\n}\n\n\n\nNow we iterate across all of those points and evaluate them with respect to our process. Think of the “width” as X values and the “height” as Y values. Given a value of X, is our random value of Y less than it would be if we created a Y value by using our function (f(x))? If so, mark it in the “points_under” vector. If not, mark it in the “points_over” vector.\n\n\npoints_under = matrix(, ncol = 2, nrow = num_points)\npoints_above = matrix(, ncol = 2, nrow = num_points)\n\nfor(i in 1:num_points){\n  if(points[i,2] < f(points[i,1])){\n    points_under[i,1] <- points[i,1]\n    points_under[i,2] <- points[i,2]\n  }\n  else{\n    points_above[i,1] <- points[i,1]\n    points_above[i,2] <- points[i,2]\n  }\n}\n\n\n\nPut the results into new vectors without NA’s. Some NA’s come up because our data generating process is crazy.\n\n\npoints_under_x <-  points_under[!is.na(points_under[,1]),1]\npoints_under_y <-  points_under[!is.na(points_under[,2]),2]\n\npoints_over_x <- points_above[!is.na(points_above[,1]),1]\npoints_over_y <- points_above[!is.na(points_above[,2]),2]\n\n\n\nNow we have an image…\n\n\nplot(points_under_y ~ points_under_x, pch = 20, cex = 0.3)\n\n\n\n\nBo\\(^2\\)m =)\n\n\n\n",
    "preview": "posts/2021-11-07-art-with-monte-carlo/art-with-monte-carlo_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-08-14T18:01:43-04:00",
    "input_file": {}
  }
]
